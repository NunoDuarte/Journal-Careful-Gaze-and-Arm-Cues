
@inproceedings{fligge_minimum_2012,
	address = {Rome, Italy},
	title = {Minimum jerk for human catching movements in {3D}},
	isbn = {978-1-4577-1200-5 978-1-4577-1199-2 978-1-4577-1198-5},
	url = {http://ieeexplore.ieee.org/document/6290265/},
	doi = {10.1109/BioRob.2012.6290265},
	abstract = {To investigate fast human reaching movements in 3D, we asked 11 right-handed persons to catch a tennis ball while we tracked the movements of their arms. To ensure consistent trajectories of the ball, we used a catapult to throw the ball from three different positions. Tangential velocity proﬁles of the hand were in general bell-shaped and hand movements in 3D coincided with well known results for 2D point-to-point movements such as minimum jerk theory or the 2/3rd power law. Furthermore, two phases, consisting of fast reaching and slower ﬁne movements at the end of hand placement could clearly be seen. The aim of this study was to ﬁnd a way to generate human-like (catching) trajectories for a humanoid robot.},
	language = {en},
	urldate = {2021-07-15},
	booktitle = {2012 4th {IEEE} {RAS} \& {EMBS} {International} {Conference} on {Biomedical} {Robotics} and {Biomechatronics} ({BioRob})},
	publisher = {IEEE},
	author = {Fligge, Nadine and McIntyre, Joseph and van der Smagt, Patrick},
	month = jun,
	year = {2012},
	pages = {581--586},
}

@article{kato_where_2019,
	title = {The where of handovers by humans: {Effect} of partner characteristics, distance and visual feedback},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {The where of handovers by humans},
	url = {https://dx.plos.org/10.1371/journal.pone.0217129},
	doi = {10.1371/journal.pone.0217129},
	language = {en},
	number = {6},
	urldate = {2021-07-15},
	journal = {PLOS ONE},
	author = {Kato, Saki and Yamanobe, Natsuki and Venture, Gentiane and Yoshida, Eiichi and Ganesh, Gowrishankar},
	editor = {Li, Zhan},
	month = jun,
	year = {2019},
	pages = {e0217129},
}

@article{hansen_humanhuman_2017,
	title = {Human–{Human} {Handover} {Tasks} and {How} {Distance} and {Object} {Mass} {Matter}},
	volume = {124},
	issn = {0031-5125, 1558-688X},
	url = {http://journals.sagepub.com/doi/10.1177/0031512516682668},
	doi = {10.1177/0031512516682668},
	abstract = {We investigated the coordination between two individuals during object handovers. Ten participants (eight males, two females; 26.0 Æ 5.0 years, 72.7 Æ 13.5 kg, 1.73 Æ 0.8 m) arranged in pairs (a giver and a receiver), passed an object from the giver to the receiver at a self-selected speed. A motion capture system quantified the giver and the receiver’s motion simultaneously. Three interpersonal distances and three object masses were chosen to study the handover. We hypothesized that (a) the handover occurs at half of the interpersonal distance between the giver and receiver and (b) the handover height depends on the objects’ mass. Taken together, our results show that the handover strongly depends on the interpersonal distance between the giver and receiver, while object mass related only to handover duration.},
	language = {en},
	number = {1},
	urldate = {2021-07-15},
	journal = {Perceptual and Motor Skills},
	author = {Hansen, Clint and Arambel, Paula and Ben Mansour, Khalil and Perdereau, Véronique and Marin, Frédéric},
	month = feb,
	year = {2017},
	pages = {182--199},
}

@article{xompero_corsmal_2020,
	title = {Corsmal containers manipulation},
	url = {http://corsmal.eecs.qmul.ac.uk/containersmanip.html},
	doi = {https://doi.org/10.17636/101CORSMAL1},
	author = {Xompero, Alessio and Sanchez-Matilla, Ricardo and {Andrea Cavallaro} and Mazzon, Ricardo},
	year = {2020},
}

@inproceedings{starke_force_2019,
	address = {Toronto, ON, Canada},
	title = {On {Force} {Synergies} in {Human} {Grasping} {Behavior}},
	isbn = {978-1-5386-7630-1},
	url = {https://ieeexplore.ieee.org/document/9035047/},
	doi = {10.1109/Humanoids43949.2019.9035047},
	abstract = {The human hand is a versatile and complex system with dexterous manipulation capabilities. For the transfer of human grasping capabilities to humanoid robotic and prosthetic hands, an understanding of the dynamic characteristics of grasp motions is fundamental. Although the analysis of grasp synergies, especially for kinematic hand postures, is a very active ﬁeld of research, the description and transfer of grasp forces is still a challenging task. In this work, we introduce a novel representation of grasp synergies in the force space, socalled force synergies, which describe forces applied at contact locations in a low dimensional space and are inspired by the correlations between grasp forces in ﬁngers and palm. To evaluate this novel representation, we conduct a human grasping study with eight subjects performing handover and tool use tasks on 14 objects with varying content and weight using 16 different grasp types. We capture contact forces at 18 locations within the hand together with the joint angle values of a data glove with 22 degrees of freedom. We identify correlations between contact forces and derive force synergies using dimensionality reduction techniques, which allow to represent grasp forces applied during grasping with only eight parameters.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {2019 {IEEE}-{RAS} 19th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	publisher = {IEEE},
	author = {Starke, Julia and Chatzilygeroudis, Konstantinos and Billard, Aude and Asfour, Tamim},
	month = oct,
	year = {2019},
	pages = {72--78},
}

@article{duarte_action_2018,
	title = {Action {Anticipation}: {Reading} the {Intentions} of {Humans} and {Robots}},
	volume = {3},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Action {Anticipation}},
	url = {https://ieeexplore.ieee.org/document/8423498/},
	doi = {10.1109/LRA.2018.2861569},
	abstract = {Humans have the fascinating capacity of processing nonverbal visual cues to understand and anticipate the actions of other humans. This “intention reading” ability is underpinned by shared motor repertoires and action models, which we use to interpret the intentions of others as if they were our own. We investigate how different cues contribute to the legibility of human actions during interpersonal interactions. Our ﬁrst contribution is a publicly available dataset with recordings of human body motion and eye gaze, acquired in an experimental scenario with an actor interacting with three subjects. From these data, we conducted a human study to analyze the importance of different nonverbal cues for action perception. As our second contribution, we used motion/gaze recordings to build a computational model describing the interaction between two persons. As a third contribution, we embedded this model in the controller of an iCub humanoid robot and conducted a second human study, in the same scenario with the robot as an actor, to validate the model’s “intention reading” capability. Our results show that it is possible to model (nonverbal) signals exchanged by humans during interaction, and how to incorporate such a mechanism in robotic systems with the twin goal of being able to “read” human action intentionsand acting in a way that is legible by humans.},
	language = {en},
	number = {4},
	urldate = {2021-05-20},
	journal = {IEEE Robotics and Automation Letters},
	author = {Duarte, Nuno Ferreira and Raković, Mirko and Tasevski, Jovica and Coco, Moreno Ignazio and Billard, Aude and Santos-Victor, Jose},
	month = oct,
	year = {2018},
	pages = {4132--4139},
}

@article{rakovic_gaze_2021,
	title = {The {Gaze} {Dialogue} {Model}: {Non}-verbal communication in {Human}-{Human} and {Human}-{Robot} {Interaction}},
	abstract = {When humans interact with each other, eye gaze movements support two goals: motor control, and communication. On one hand, we ﬁrst ﬁxate the task goal to retrieve visual information required for safe and precise action-execution. On the other hand, gaze movements fulﬁl the purpose of communication, both for reading the intention of our interaction partners, as well as to signal our action intentions to others.},
	language = {en},
	journal = {Paper under revision for IEEE Transactions on Cybernetics},
	author = {Raković, Mirko and Duarte, Nuno Ferreira and Marques, Jorge and Billard, Aude and Santos-Victor, Jose},
	year = {2021},
	pages = {14},
}

@inproceedings{schenck_visual_2017,
	address = {Singapore, Singapore},
	title = {Visual closed-loop control for pouring liquids},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989307/},
	doi = {10.1109/ICRA.2017.7989307},
	abstract = {Pouring a speciﬁc amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour speciﬁc amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the ﬁrst use of raw visual feedback to pour liquids in robotics.},
	language = {en},
	urldate = {2021-06-16},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Schenck, Connor and Fox, Dieter},
	month = may,
	year = {2017},
	pages = {2629--2636},
}

@inproceedings{do_probabilistic_2016,
	address = {Daejeon, South Korea},
	title = {A probabilistic approach to liquid level detection in cups using an {RGB}-{D} camera},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759326/},
	doi = {10.1109/IROS.2016.7759326},
	abstract = {Robotic assistants have the potential to greatly improve our quality of life by supporting us in our daily activities. A service robot acting autonomously in an indoor environment is faced with very complex tasks. Consider the problem of pouring a liquid into a cup, the robot should ﬁrst determine if the cup is empty or partially ﬁlled. RGB-D cameras provide noisy depth measurements which depend on the opaqueness and refraction index of the liquid. In this paper, we present a novel probabilistic approach for estimating the ﬁlllevel of a liquid in a cup using an RGB-D camera. Our approach does not make any assumptions about the properties of the liquid like its opaqueness or its refraction index. We develop a probabilistic model using features extracted from RGB and depth data. Our experiments demonstrate the robustness of our method and an improvement over the state of the art.},
	language = {en},
	urldate = {2021-06-16},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Do, Chau and Schubert, Tobias and Burgard, Wolfram},
	month = oct,
	year = {2016},
	pages = {2075--2080},
}

@article{do_accurate_2018,
	title = {Accurate {Pouring} with an {Autonomous} {Robot} {Using} an {RGB}-{D} {Camera}},
	url = {http://arxiv.org/abs/1810.03303},
	abstract = {Robotic assistants in a home environment are expected to perform various complex tasks for their users. One particularly challenging task is pouring drinks into cups, which for successful completion, requires the detection and tracking of the liquid level during a pour to determine when to stop. In this paper, we present a novel approach to autonomous pouring that tracks the liquid level using an RGB-D camera and adapts the rate of pouring based on the liquid level feedback. We thoroughly evaluate our system on various types of liquids and under diﬀerent conditions, conducting over 250 pours with a PR2 robot. The results demonstrate that our approach is able to pour liquids to a target height with an accuracy of a few millimeters.},
	language = {en},
	urldate = {2021-06-16},
	journal = {arXiv:1810.03303 [cs]},
	author = {Do, Chau and Burgard, Wolfram},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03303},
	keywords = {Computer Science - Robotics},
}

@article{schenck_reasoning_2017,
	title = {Reasoning {About} {Liquids} via {Closed}-{Loop} {Simulation}},
	url = {http://arxiv.org/abs/1703.01656},
	abstract = {Simulators are powerful tools for reasoning about a robot’s interactions with its environment. However, when simulations diverge from reality, that reasoning becomes less useful. In this paper, we show how to close the loop between liquid simulation and real-time perception. We use observations of liquids to correct errors when tracking the liquid’s state in a simulator. Our results show that closed-loop simulation is an effective way to prevent large divergence between the simulated and real liquid states. As a direct consequence of this, our method can enable reasoning about liquids that would otherwise be infeasible due to large divergences, such as reasoning about occluded liquid.},
	language = {en},
	urldate = {2021-06-16},
	journal = {arXiv:1703.01656 [cs]},
	author = {Schenck, Connor and Fox, Dieter},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.01656},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{modas_improving_2021,
	title = {Improving filling level classification with adversarial training},
	url = {http://arxiv.org/abs/2102.04057},
	abstract = {We investigate the problem of classifying – from a single image –the level of content in a cup or a drinking glass. This problem is made challenging by several ambiguities caused by transparencies, shape variations and partial occlusions, and by the availability of only small training datasets. In this paper, we tackle this problem with an appropriate strategy for transfer learning. Speciﬁcally, we use adversarial training in a generic source dataset and then reﬁne the training with a task-speciﬁc dataset. We also discuss and experimentally evaluate several training strategies and their combination on a range of container types of the CORSMAL Containers Manipulation dataset. We show that transfer learning with adversarial training in the source domain consistently improves the classiﬁcation accuracy on the test set and limits the overﬁtting of the classiﬁer to speciﬁc features of the training data.},
	language = {en},
	urldate = {2021-06-16},
	journal = {arXiv:2102.04057 [cs]},
	author = {Modas, Apostolos and Xompero, Alessio and Sanchez-Matilla, Ricardo and Frossard, Pascal and Cavallaro, Andrea},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.04057},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{sanchez-matilla_benchmark_2020,
	title = {Benchmark for {Human}-to-{Robot} {Handovers} of {Unseen} {Containers} {With} {Unknown} {Filling}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/8968407/},
	doi = {10.1109/LRA.2020.2969200},
	abstract = {The real-time estimation through vision of the physical properties of objects manipulated by humans is important to inform the control of robots for performing accurate and safe grasps of objects handed over by humans. However, estimating the 3D pose and dimensions of previously unseen objects using only RGB cameras is challenging due to illumination variations, reﬂective surfaces, transparencies, and occlusions caused both by the human and the robot. In this letter, we present a benchmark for dynamic human-to-robot handovers that do not rely on a motion capture system, markers, or prior knowledge of speciﬁc objects. To facilitate comparisons, the benchmark focuses on cups with different levels of transparencies and with an unknown amount of an unknown ﬁlling. The performance scores assess the overall system as well as its components in order to help isolate modules of the pipeline that need improvements. In addition to the task description and the performance scores, we also present and distribute as open source a baseline implementation for the overall pipeline to enable comparisons and facilitate progress.},
	language = {en},
	number = {2},
	urldate = {2021-06-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sanchez-Matilla, Ricardo and Chatzilygeroudis, Konstantinos and Modas, Apostolos and Duarte, Nuno Ferreira and Xompero, Alessio and Frossard, Pascal and Billard, Aude and Cavallaro, Andrea},
	month = apr,
	year = {2020},
	pages = {1642--1649},
}

@article{grasso_eye-head_1998,
	title = {Eye-head coordination for the steering of locomotion in humans: an anticipatory synergy},
	volume = {253},
	issn = {03043940},
	shorttitle = {Eye-head coordination for the steering of locomotion in humans},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304394098006259},
	doi = {10.1016/S0304-3940(98)00625-9},
	language = {en},
	number = {2},
	urldate = {2021-06-15},
	journal = {Neuroscience Letters},
	author = {Grasso, Renato and Prévost, Pascal and Ivanenko, Yuri P and Berthoz, Alain},
	month = sep,
	year = {1998},
	pages = {115--118},
}

@article{bianchi_reservoir_2021,
	title = {Reservoir {Computing} {Approaches} for {Representation} and {Classification} of {Multivariate} {Time} {Series}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9127499/},
	doi = {10.1109/TNNLS.2020.3001377},
	abstract = {Classiﬁcation of multivariate time series (MTS) has been tackled with a large variety of methodologies and applied to a wide range of scenarios. Reservoir computing (RC) provides efﬁcient tools to generate a vectorial, ﬁxed-size representation of the MTS that can be further processed by standard classiﬁers. Despite their unrivaled training speed, MTS classiﬁers based on a standard RC architecture fail to achieve the same accuracy of fully trainable neural networks. In this article, we introduce the reservoir model space, an unsupervised approach based on RC to learn vectorial representations of MTS. Each MTS is encoded within the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics. Compared with other RC methods, our model space yields better representations and attains comparable computational performance due to an intermediate dimensionality reduction procedure. As a second contribution, we propose a modular RC framework for MTS classiﬁcation, with an associated open-source Python library. The framework provides different modules to seamlessly implement advanced RC architectures. The architectures are compared with other MTS classiﬁers, including deep learning models and time series kernels. Results obtained on the benchmark and real-world MTS data sets show that RC classiﬁers are dramatically faster and, when implemented using our proposed representation, also achieve superior classiﬁcation accuracy.},
	language = {en},
	number = {5},
	urldate = {2021-06-14},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Bianchi, Filippo Maria and Scardapane, Simone and Lokse, Sigurd and Jenssen, Robert},
	month = may,
	year = {2021},
	pages = {2169--2179},
}

@article{kassner_pupil_2014,
	title = {Pupil: {An} {Open} {Source} {Platform} for {Pervasive} {Eye} {Tracking} and {Mobile} {Gaze}-based {Interaction}},
	shorttitle = {Pupil},
	url = {http://arxiv.org/abs/1405.0006},
	abstract = {Commercial head-mounted eye trackers provide useful features to customers in industry and research but are expensive and rely on closed source hardware and software. This limits the application areas and use of mobile eye tracking to expert users and inhibits user-driven development, customisation, and extension. In this paper we present Pupil – an accessible, affordable, and extensible open source platform for mobile eye tracking and gaze-based interaction. Pupil comprises 1) a light-weight headset with high-resolution cameras, 2) an open source software framework for mobile eye tracking, as well as 3) a graphical user interface (GUI) to playback and visualize video and gaze data. Pupil features high-resolution scene and eye cameras for monocular and binocular gaze estimation. The software and GUI are platform-independent and include state-of-the-art algorithms for real-time pupil detection and tracking, calibration, and accurate gaze estimation. Results of a performance evaluation show that Pupil can provide an average gaze estimation accuracy of 0.6 degree of visual angle (0.08 degree precision) with a latency of the processing pipeline of only 0.045 seconds.},
	language = {en},
	urldate = {2021-06-13},
	journal = {arXiv:1405.0006 [cs]},
	author = {Kassner, Moritz and Patera, William and Bulling, Andreas},
	month = apr,
	year = {2014},
	note = {arXiv: 1405.0006},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
}

@article{sun_review_2020,
	title = {A {Review} of {Designs} and {Applications} of {Echo} {State} {Networks}},
	url = {http://arxiv.org/abs/2012.02974},
	abstract = {Recurrent Neural Networks (RNNs) have demonstrated their outstanding ability in sequence tasks and have achieved state-of-the-art in wide range of applications, such as industrial, medical, economic and linguistic. Echo State Network (ESN) is simple type of RNNs and has emerged in the last decade as an alternative to gradient descent training based RNNs. ESN, with a strong theoretical ground, is practical, conceptually simple, easy to implement. It avoids non-converging and computationally expensive in the gradient descent methods. Since ESN was put forward in 2002, abundant existing works have promoted the progress of ESN, and the recently introduced Deep ESN model opened the way to uniting the merits of deep learning and ESNs. Besides, the combinations of ESNs with other machine learning models have also overperformed baselines in some applications. However, the apparent simplicity of ESNs can sometimes be deceptive and successfully applying ESNs needs some experience. Thus, in this paper, we categorize the ESN-based methods to basic ESNs, DeepESNs and combinations, then analyze them from the perspective of theoretical studies, network designs and speciﬁc applications. Finally, we discuss the challenges and opportunities of ESNs by summarizing the open questions and proposing possible future works.},
	language = {en},
	urldate = {2021-06-11},
	journal = {arXiv:2012.02974 [cs]},
	author = {Sun, Chenxi and Song, Moxian and Hong, Shenda and Li, Hongyan},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.02974},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{palinko_robot_2016,
	title = {Robot reading human gaze: {Why} eye tracking is better than head tracking for human-robot collaboration},
	shorttitle = {Robot reading human gaze},
	doi = {10.1109/IROS.2016.7759741},
	abstract = {Robots are at the position to become our everyday companions in the near future. Still, many hurdles need to be cleared to achieve this goal. One of them is the fact that robots are still not able to perceive some important communication cues naturally used by humans, e.g. gaze. In the recent past, eye gaze in robot perception was substituted by its proxy, head orientation. Such an approach is still adopted in many applications today. In this paper we introduce performance improvements to an eye tracking system we previously developed and use it to explore if this approximation is appropriate. More precisely, we compare the impact of the use of eye- or head-based gaze estimation in a human robot interaction experiment with the iCub robot and naïve subjects. We find that the possibility to exploit the richer information carried by eye gaze has a significant impact on the interaction. As a result, our eye tracking system allows for a more efficient human-robot collaboration than a comparable head tracking approach, according to both quantitative measures and subjective evaluation by the human participants.},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Palinko, Oskar and Rea, Francesco and Sandini, Giulio and Sciutti, Alessandra},
	month = oct,
	year = {2016},
	note = {ISSN: 2153-0866},
	keywords = {Cameras, Gaze tracking, Head, Magnetic heads, Robot kinematics, Visualization},
	pages = {5048--5054},
}

@article{rakovic_dataset_2018,
	title = {A dataset of head and eye gaze during dyadic interaction task for modeling robot gaze behavior},
	volume = {161},
	copyright = {© The Authors, published by EDP Sciences, 2018},
	issn = {2261-236X},
	url = {https://www.matec-conferences.org/articles/matecconf/abs/2018/20/matecconf_erzr2018_03002/matecconf_erzr2018_03002.html},
	doi = {10.1051/matecconf/201816103002},
	abstract = {In this work is presented a dataset of humans‘ head and eye gaze acquired with Pupil Labs gazetracking glasses and Optitrack motion capture system. The dataset contains recordings of adult subjects in dyadic interaction task. During the experiment, the subjects are asked to pick up an object and, based on the randomly defined instructions, to place it on the table in front of her/him or to give the object to a person sitting across the table. If the object is handed over, the second person takes the object and places it on the table it in front of her/him. The dataset is intended to be used to model the behavior of the human’s gaze while interacting with another human and implement the model in a controller of a robot for dyadic interaction with a humans.},
	language = {en},
	urldate = {2021-05-27},
	journal = {MATEC Web of Conferences},
	author = {Raković, Mirko and Duarte, Nuno and Tasevski, Jovica and Santos-Victor, José and Borovac, Branislav},
	year = {2018},
	pages = {03002},
}

@inproceedings{duarte_action_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Action {Alignment} from {Gaze} {Cues} in {Human}-{Human} and {Human}-{Robot} {Interaction}},
	isbn = {9783030110154},
	doi = {10.1007/978-3-030-11015-4_17},
	abstract = {Cognitive neuroscience experiments show how people intensify the exchange of non-verbal cues when they work on a joint task towards a common goal. When individuals share their intentions, it creates a social interaction that drives the mutual alignment of their actions and behavior. To understand the intentions of others, we strongly rely on the gaze cues. According to the role each person plays in the interaction, the resulting alignment of the body and gaze movements will be different. This mechanism is key to understand and model dyadic social interactions.We focus on the alignment of the leader’s behavior during dyadic interactions. The recorded gaze movements of dyads are used to build a model of the leader’s gaze behavior. We use of the follower’s gaze behavior data for two purposes: (i) to determine whether the follower is involved in the interaction, and (ii) if the follower’s gaze behavior correlates to the type of the action under execution. This information is then used to plan the leader’s actions in order to sustain the leader/follower alignment in the social interaction.The model of the leader’s gaze behavior and the alignment of the intentions is evaluated in a human-robot interaction scenario, with the robot acting as a leader and the human as a follower. During the interaction, the robot (i) emits non-verbal cues consistent with the action performed; (ii) predicts the human actions, and (iii) aligns its motion according to the human behavior.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Duarte, Nuno Ferreira and Raković, Mirko and Marques, Jorge and Santos-Victor, José},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	keywords = {Action alignment , Action anticipation , Gaze behavior , Human-robot interaction },
	pages = {197--212},
}

@article{fan_robotic_2017,
	title = {A {Robotic} {Coach} {Architecture} for {Elder} {Care} ({ROCARE}) {Based} on {Multi}-{User} {Engagement} {Models}},
	volume = {25},
	issn = {1534-4320, 1558-0210},
	url = {http://ieeexplore.ieee.org/document/7565740/},
	doi = {10.1109/TNSRE.2016.2608791},
	number = {8},
	urldate = {2021-05-27},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	author = {Fan, Jing and Bian, Dayi and Zheng, Zhi and Beuscher, Linda and Newhouse, Paul A. and Mion, Lorraine C. and Sarkar, Nilanjan},
	month = aug,
	year = {2017},
	pages = {1153--1163},
}

@article{anzalone_evaluating_2015,
	title = {Evaluating the {Engagement} with {Social} {Robots}},
	volume = {7},
	issn = {1875-4791, 1875-4805},
	url = {http://link.springer.com/10.1007/s12369-015-0298-7},
	doi = {10.1007/s12369-015-0298-7},
	language = {en},
	number = {4},
	urldate = {2021-05-27},
	journal = {International Journal of Social Robotics},
	author = {Anzalone, Salvatore M. and Boucenna, Sofiane and Ivaldi, Serena and Chetouani, Mohamed},
	month = aug,
	year = {2015},
	pages = {465--478},
}

@article{lindemann_grasping_2011,
	title = {Grasping the other's attention: {The} role of animacy in action cueing of joint attention},
	volume = {51},
	issn = {00426989},
	url = {http://dx.doi.org/10.1016/j.visres.2010.12.009},
	doi = {10.1016/j.visres.2010.12.009},
	abstract = {The current experiment investigates the role of animacy on grasp-cueing effects as investigated in joint attention research. In a simple detection task participants responded to the colour change of one of two objects of identical size. Before the target onset, we presented a cueing stimulus consisting of either two human hands with a small and a large grip aperture (animate condition) or two comparable U-shaped figures with small and large aperture (inanimate condition). Depending on the size of the objects and the arrangement of the apertures (i.e., large aperture to the left and small aperture to the right or vice versa), either the left or right object matched the grasping hand or U-shapes. Our data show that biological grasping actions modulate the observer's attention whereas the perception of inanimate stimuli does not result in a comparable cueing effect. This strong impact of animacy on attentional priming suggests that grasp cueing represents a marker of a joint attention mechanism that involves spontaneous simulation of the observed motor behaviour. ?? 2011 Elsevier Ltd.},
	number = {8},
	journal = {Vision Research},
	author = {Lindemann, Oliver and Nuku, Pines and Rueschemeyer, Shirley Ann and Bekkering, Harold},
	year = {2011},
	pmid = {21215275},
	note = {Publisher: Elsevier Ltd},
	keywords = {Action observation, Grasp, Grasp cueing, Human Grasp, Human-Human Interaction (HHI), Joint spatial attention, Joint-action, Social cognition, Vision},
	pages = {940--944},
}

@article{kjellstrom_visual_2011,
	title = {Visual object-action recognition: {Inferring} object affordances from human demonstration},
	volume = {115},
	issn = {10773142},
	shorttitle = {Visual object-action recognition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S107731421000175X},
	doi = {10.1016/j.cviu.2010.08.002},
	abstract = {This paper investigates object categorization according to function, i.e., learning the affordances of objects from human demonstration. Object affordances (functionality) are inferred from observations of humans using the objects in different types of actions. The intended application is learning from demonstration, in which a robot learns to employ objects in household tasks, from observing a human performing the same tasks with the objects. We present a method for categorizing manipulated objects and human manipulation actions in context of each other. The method is able to simultaneously segment and classify human hand actions, and detect and classify the objects involved in the action. This can serve as an initial step in a learning from demonstration method. Experiments show that the contextual information improves the classiﬁcation of both objects and actions.},
	language = {en},
	number = {1},
	urldate = {2021-05-21},
	journal = {Computer Vision and Image Understanding},
	author = {Kjellström, Hedvig and Romero, Javier and Kragić, Danica},
	month = jan,
	year = {2011},
	pages = {81--90},
}

@article{jamone_affordances_2018,
	title = {Affordances in {Psychology}, {Neuroscience}, and {Robotics}: {A} {Survey}},
	volume = {10},
	issn = {2379-8920, 2379-8939},
	shorttitle = {Affordances in {Psychology}, {Neuroscience}, and {Robotics}},
	url = {http://ieeexplore.ieee.org/document/7523298/},
	doi = {10.1109/TCDS.2016.2594134},
	abstract = {The concept of affordances appeared in psychology during the late 60’s as an alternative perspective on the visual perception of the environment. It was revolutionary in the intuition that the way living beings perceive the world is deeply inﬂuenced by the actions they are able to perform. Then, across the last 40 years, it has inﬂuenced many applied ﬁelds: e.g. design, human-computer interaction, computer vision, robotics. In this paper we offer a multidisciplinary perspective on the notion of affordances: we ﬁrst discuss the main deﬁnitions and formalizations of the affordance theory, then we report the most signiﬁcant evidence in psychology and neuroscience that support it, and ﬁnally we review the most relevant applications of this concept in robotics.},
	language = {en},
	number = {1},
	urldate = {2021-05-20},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Jamone, Lorenzo and Ugur, Emre and Cangelosi, Angelo and Fadiga, Luciano and Bernardino, Alexandre and Piater, Justus and Santos-Victor, Jose},
	month = mar,
	year = {2018},
	pages = {4--25},
}

@inproceedings{duarte_human_2020,
	address = {Valparaiso, Chile},
	title = {From human action understanding to robot action execution: how the physical properties of handled objects modulate non-verbal cues},
	isbn = {978-1-72817-306-1},
	shorttitle = {From human action understanding to robot action execution},
	url = {https://ieeexplore.ieee.org/document/9278084/},
	doi = {10.1109/ICDL-EpiRob48136.2020.9278084},
	abstract = {Humans manage to communicate action intentions in a non-verbal way, through body posture and movement. We start from this observation to investigate how a robot can decode a human’s non-verbal cues during the manipulation of an object, with speciﬁc physical properties, to learn the adequate level of “carefulness” to use when handling that object. We construct dynamical models of the human behaviour using a human-to-human handover dataset consisting of 3 different cups with different levels of ﬁllings. We then included these models into the design of an online classiﬁer that identiﬁes the type of action, based on the human wrist movement. We close the loop from action understanding to robot action execution with an adaptive and robust controller based on the learned classiﬁer, and evaluate the entire pipeline on a collaborative task with a 7-DOF manipulator. Our results show that it is possible to correctly understand the “carefulness” behaviour of humans during object manipulation, even in the pick and place scenario, that was not part of the training set.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {2020 {Joint} {IEEE} 10th {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL}-{EpiRob})},
	publisher = {IEEE},
	author = {Duarte, Nuno Ferreira and Chatzilygeroudis, Konstantinos and Santos-Victor, Jose and Billard, Aude},
	month = oct,
	year = {2020},
	pages = {1--6},
}

@inproceedings{yu_fill_2015,
	address = {Santiago, Chile},
	title = {Fill and {Transfer}: {A} {Simple} {Physics}-{Based} {Approach} for {Containability} {Reasoning}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Fill and {Transfer}},
	url = {http://ieeexplore.ieee.org/document/7410445/},
	doi = {10.1109/ICCV.2015.88},
	abstract = {The visual perception of object affordances has emerged as a useful ingredient for building powerful computer vision and robotic applications [31]. In this paper we introduce a novel approach to reason about liquid containability - the affordance of containing liquid. Our approach analyzes container objects based on two simple physical processes: the Fill and Transfer of liquid. First, it reasons about whether a given 3D object is a liquid container and its best ﬁlling direction. Second, it proposes directions to transfer its contained liquid to the outside while avoiding spillage. We compare our simpliﬁed model with a common ﬂuid dynamics simulation and demonstrate that our algorithm makes human-like choices about the best directions to ﬁll containers and transfer liquid from them. We apply our approach to reason about the containability of several real-world objects acquired using a consumer-grade depth camera.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yu, Lap-Fai and Duncan, Noah and Yeung, Sai-Kit},
	month = dec,
	year = {2015},
	pages = {711--719},
}

@inproceedings{mottaghi_see_2017,
	address = {Venice, Italy},
	title = {See the {Glass} {Half} {Full}: {Reasoning} {About} {Liquid} {Containers}, {Their} {Volume} and {Content}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {See the {Glass} {Half} {Full}},
	url = {http://ieeexplore.ieee.org/document/8237469/},
	doi = {10.1109/ICCV.2017.207},
	abstract = {Humans have rich understanding of liquid containers and their contents; for example, we can effortlessly pour water from a pitcher to a cup. Doing so requires estimating the volume of the cup, approximating the amount of water in the pitcher, and predicting the behavior of water when we tilt the pitcher. Very little attention in computer vision has been made to liquids and their containers. In this paper, we study liquid containers and their contents, and propose methods to estimate the volume of containers, approximate the amount of liquid in them, and perform comparative volume estimations all from a single RGB image. Furthermore, we show the results of the proposed model for predicting the behavior of liquids inside containers when one tilts the containers. We also introduce a new dataset of Containers Of liQuid contEnt (COQE) that contains more than 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Mottaghi, Roozbeh and Schenck, Connor and Fox, Dieter and Farhadi, Ali},
	month = oct,
	year = {2017},
	pages = {1889--1898},
}

@inproceedings{admoni_robot_2016,
	address = {Christchurch, New Zealand},
	title = {Robot nonverbal behavior improves task performance in difficult collaborations},
	isbn = {978-1-4673-8370-7},
	url = {http://ieeexplore.ieee.org/document/7451733/},
	doi = {10.1109/HRI.2016.7451733},
	abstract = {Nonverbal behaviors increase task efﬁciency and improve collaboration between people and robots. In this paper, we introduce a model for generating nonverbal behavior and investigate whether the usefulness of nonverbal behaviors changes based on task difﬁculty. First, we detail a robot behavior model that accounts for top-down and bottom-up features of the scene when deciding when and how to perform deictic references (looking or pointing). Then, we analyze how a robot’s deictic nonverbal behavior affects people’s performance on a memorization task under differing difﬁculty levels. We manipulate difﬁculty in two ways: by adding steps to memorize, and by introducing an interruption. We ﬁnd that when the task is easy, the robot’s nonverbal behavior has little inﬂuence over recall and task completion. However, when the task is challenging—because the memorization load is high or because the task is interrupted—a robot’s nonverbal behaviors mitigate the negative effects of these challenges, leading to higher recall accuracy and lower completion times. In short, nonverbal behavior may be even more valuable for difﬁcult collaborations than for easy ones.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {2016 11th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	publisher = {IEEE},
	author = {Admoni, Henny and Weng, Thomas and Hayes, Bradley and Scassellati, Brian},
	month = mar,
	year = {2016},
	pages = {51--58},
}

@article{santina_learning_2019,
	title = {Learning {From} {Humans} {How} to {Grasp}: {A} {Data}-{Driven} {Architecture} for {Autonomous} {Grasping} {With} {Anthropomorphic} {Soft} {Hands}},
	volume = {4},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Learning {From} {Humans} {How} to {Grasp}},
	url = {https://ieeexplore.ieee.org/document/8629968/},
	doi = {10.1109/LRA.2019.2896485},
	abstract = {Soft hands are robotic systems that embed compliant elements in their mechanical design. This enables an effective adaptation with the items and the environment, and, ultimately, an increase of their grasping performance. These hands come with clear advantages in terms of ease-to-use and robustness if compared with classic rigid hands, when operated by a human. However, their potential for autonomous grasping is still largely unexplored, due to the lack of suitable control strategies. To address this issue, in this work we propose an approach to enable soft hands to autonomously grasp objects, starting from the observations of human strategies. A classiﬁer realized through a deep neural network takes as input the visual information on the object to be grasped, and predicts which action a human would perform to achieve the goal. This information is hence used to select one among a set of humaninspired primitives, which deﬁne the evolution of soft hand posture as a combination of anticipatory action and touch-based reactive grasp. The architecture is completed by the hardware component, which consists of a RGB camera to look at the scene, a 7-DoF manipulator, and a soft hand. The latter is equipped with IMUs at the ﬁngernails for detecting contact with the object. We extensively tested the proposed architecture with 20 objects, achieving a success rate of 81.1\% over 111 grasps.},
	language = {en},
	number = {2},
	urldate = {2021-05-20},
	journal = {IEEE Robotics and Automation Letters},
	author = {Santina, Cosimo Della and Arapi, Visar and Averta, Giuseppe and Damiani, Francesca and Fiore, Gaia and Settimi, Alessandro and Catalano, Manuel G. and Bacciu, Davide and Bicchi, Antonio and Bianchi, Matteo},
	month = apr,
	year = {2019},
	pages = {1533--1540},
}

@article{khoramshahi_dynamical_2019,
	title = {A dynamical system approach to task-adaptation in physical human–robot interaction},
	volume = {43},
	issn = {0929-5593, 1573-7527},
	url = {http://link.springer.com/10.1007/s10514-018-9764-z},
	doi = {10.1007/s10514-018-9764-z},
	abstract = {The goal of this work is to enable robots to intelligently and compliantly adapt their motions to the intention of a human during physical Human–Robot Interaction in a multi-task setting. We employ a class of parameterized dynamical systems that allows for smooth and adaptive transitions between encoded tasks. To comply with human intention, we propose a mechanism that adapts generated motions (i.e., the desired velocity) to those intended by the human user (i.e., the real velocity) thereby switching to the most similar task. We provide a rigorous analytical evaluation of our method in terms of stability, convergence, and optimality yielding an interaction behavior which is safe and intuitive for the human. We investigate our method through experimental evaluations ranging in different setups: a 3-DoF haptic device, a 7-DoF manipulator and a mobile platform.},
	language = {en},
	number = {4},
	urldate = {2021-05-20},
	journal = {Autonomous Robots},
	author = {Khoramshahi, Mahdi and Billard, Aude},
	month = apr,
	year = {2019},
	pages = {927--946},
}

@article{mavridis_review_2015,
	title = {A review of verbal and non-verbal human–robot interactive communication},
	volume = {63},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889014002164},
	doi = {10.1016/j.robot.2014.09.031},
	abstract = {In this paper, an overview of human–robot interactive communication is presented, covering verbal as well as non-verbal aspects. Following a historical introduction, and motivation towards fluid human–robot communication, ten desiderata are proposed, which provide an organizational axis both of recent as well as of future research on human–robot communication. Then, the ten desiderata are examined in detail, culminating in a unifying discussion, and a forward-looking conclusion.},
	language = {en},
	urldate = {2021-05-20},
	journal = {Robotics and Autonomous Systems},
	author = {Mavridis, Nikolaos},
	month = jan,
	year = {2015},
	pages = {22--35},
}

@article{sciutti_development_2019,
	title = {Development of visual perception of others’ actions: {Children}’s judgment of lifted weight},
	volume = {14},
	issn = {19326203},
	doi = {10.1371/journal.pone.0224979},
	abstract = {Humans are excellent at perceiving different features of the actions performed by others. For instance, by viewing someone else manipulating an unknown object, one can infer its weight–an intrinsic feature otherwise not directly accessible through vision. How such perceptual skill develops during childhood remains unclear. To confront this gap, the current study had children (N:63, 6–10 years old) and adults (N:21) judge the weight of objects after observing videos of an actor lifting them. Although 6-year-olds could already discriminate different weights, judgment accuracy had not reached adult-like levels by 10 years of age. Additionally, children’s stature was a more reliable predictor of their ability to read others’ actions than was their chronological age. This paper discusses the results in light of a potential link between motor development and action perception.},
	number = {11},
	journal = {PLoS ONE},
	author = {Sciutti, Alessandra and Patanè, Laura and Sandini, Giulio},
	year = {2019},
	keywords = {Human Activity, Neuroscience, Objects Weights},
	pages = {1--15},
}

@article{lastrico_careful_2021,
	title = {Careful with {That}! {Observation} of {Human} {Movements} to {Estimate} {Objects} {Properties}},
	volume = {18},
	url = {http://arxiv.org/abs/2103.01555},
	doi = {10.1007/978-3-030-71356-0_10},
	abstract = {Humans are very eﬀective at interpreting subtle properties of the partner’s movement and use this skill to promote smooth interactions. Therefore, robotic platforms that support human partners in daily activities should acquire similar abilities. In this work we focused on the features of human motor actions that communicate insights on the weight of an object and the carefulness required in its manipulation. Our ﬁnal goal is to enable a robot to autonomously infer the degree of care required in object handling and to discriminate whether the item is light or heavy, just by observing a human manipulation. This preliminary study represents a promising step towards the implementation of those abilities on a robot observing the scene with its camera. Indeed, we succeeded in demonstrating that it is possible to reliably deduct if the human operator is careful when handling an object, through machine learning algorithms relying on the stream of visual acquisition from either a robot camera or from a motion capture system. On the other hand, we observed that the same approach is inadequate to discriminate between light and heavy objects.},
	language = {en},
	urldate = {2021-05-20},
	journal = {arXiv:2103.01555 [cs]},
	author = {Lastrico, Linda and Carfì, Alessandro and Vignolo, Alessia and Sciutti, Alessandra and Mastrogiovanni, Fulvio and Rea, Francesco},
	year = {2021},
	note = {arXiv: 2103.01555},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Handovers, Human-Human Interaction (HHI), Manipulation},
	pages = {127--141},
}

@article{senot_effect_2011,
	title = {Effect of weight-related labels on corticospinal excitability during observation of grasping: {A} {TMS} study},
	volume = {211},
	issn = {00144819},
	doi = {10.1007/s00221-011-2635-x},
	abstract = {Recent studies of corticospinal excitability during observation of grasping and lifting of objects of different weight have highlighted the role of agent's kinematics in modulating observer's motor excitability. Here, we investigate whether explicit weight-related information, provided by written labels on the objects, modulate the excitability of the observer's motor system and how this modulation is affected when there is a conflict between label and object's weight. We measured TMS-evoked motor potentials (MEPs) from right hand intrinsic muscles, while subjects were observing an actor lifting objects of different weights, in some trials labeled (heavy/light) in congruent or incongruent way. Results confirmed a weight-related modulation of MEPs based on kinematic cues. Interestingly, any conflict between the labels and the actual weight (i.e., explicit versus implicit information), although never consciously noticed by the observer, deeply affected the mirroring of others' actions. Our findings stress the automatic involvement of the mirror-neuron system. © 2011 Springer-Verlag.},
	number = {1},
	journal = {Experimental Brain Research},
	author = {Senot, Patrice and D'Ausilio, Alessandro and Franca, Michele and Caselli, Luana and Craighero, Laila and Fadiga, Luciano},
	year = {2011},
	pmid = {21533701},
	keywords = {Action observation, Grip force, Human Activity, Mirror system, Neuroscience, Objects Weights, Transcranial magnetic stimulation},
	pages = {161--167},
}

@article{cherubini_collaborative_2016,
	title = {Collaborative manufacturing with physical human-robot interaction},
	volume = {40},
	issn = {07365845},
	url = {http://dx.doi.org/10.1016/j.rcim.2015.12.007},
	doi = {10.1016/j.rcim.2015.12.007},
	abstract = {Although the concept of industrial cobots dates back to 1999, most present day hybrid human-machine assembly systems are merely weight compensators. Here, we present results on the development of a collaborative human-robot manufacturing cell for homokinetic joint assembly. The robot alternates active and passive behaviours during assembly, to lighten the burden on the operator in the first case, and to comply to his/her needs in the latter. Our approach can successfully manage direct physical contact between robot and human, and between robot and environment. Furthermore, it can be applied to standard position (and not torque) controlled robots, common in the industry. The approach is validated in a series of assembly experiments. The human workload is reduced, diminishing the risk of strain injuries. Besides, a complete risk analysis indicates that the proposed setup is compatible with the safety standards, and could be certified.},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Cherubini, Andrea and Passama, Robin and Crosnier, André and Lasnier, Antoine and Fraisse, Philippe},
	year = {2016},
	note = {Publisher: Elsevier},
	keywords = {Cobots, Human Activity, Human-Robot Interaction(HRI), Human-robot interaction, Industrial robotics, Motion, Reactive and sensor-based control},
	pages = {1--13},
}

@article{heydaryan_safety_2018,
	title = {Safety design and development of a human-robot collaboration assembly process in the automotive industry},
	volume = {8},
	issn = {20763417},
	doi = {10.3390/app8030344},
	abstract = {Human-robot collaboration (HRC) is a complex procedure in manufacturing due to the problems posed by compatibility and operational safety among humans and robots, and by task definitions among them in a collaborative order. In this paper, the research results of the human-robot collaboration study for the case of an automotive brake disc assembly is presented. The analytic hierarchy process (AHP) is proposed as a decision-making method for the human-robot collaboration system, and detailed hierarchical task analysis (HTA) is applied to allocate operational tasks to humans and robots, thus reducing the chance of duty interference. Additionally, a virtual environment software (Tecnomatix Process Simulate, version 11.1,80, Siemens, Munich, BY, Germany, 2012) is used to model the assembly workstation, providing an opportunity to evaluate the feasibility of the process through different scenarios. Finally, an experimental test is conducted to evaluate the performance of the assembly procedure. This research proves that, although human-robot collaboration increases the total process time slightly, this collaboration improves human ergonomics considerably and reduces the operator injury risk.},
	number = {3},
	journal = {Applied Sciences (Switzerland)},
	author = {Heydaryan, Sahar and Bedolla, Joel Suaza and Belingardi, Giovanni},
	year = {2018},
	keywords = {Brake disc assembly, Ergonomic, HRC, HTA, Safety, Virtual environment},
}

@article{safeea_end-effector_nodate,
	title = {End-effector precise hand-guiding for collaborative robots},
	author = {Safeea, Mohammad and Bearee, Richard and Neto, Pedro},
	keywords = {collaborative robot, end-effector, hand-guiding},
	pages = {1--11},
}

@article{zanchettin_safety_2016,
	title = {Safety in {Human}-{Robot} {Collaborative} {Manufacturing} {Environments}: {Metrics} and {Control}},
	volume = {13},
	issn = {15455955},
	doi = {10.1109/TASE.2015.2412256},
	abstract = {New paradigms in industrial robotics no longer require physical separation between robotic manipulators and humans. Moreover, in order to optimize production, humans and robots are expected to collaborate to some extent. In this scenario, involving a shared environment between humans and robots, common motion generation algorithms might turn out to be inadequate for this purpose. This paper proposes a kinematic control strategy which enforces safety, while maintaining the maximum level of productivity of the robot. The resulting motion of the (possibly redundant) robot is obtained as an output of an optimization-based real-time algorithm in which safety is regarded as a hard constraint to be satisfied. The methodology is experimentally validated on a dual-arm concept robot with 7-DOF per arm performing a manipulation task.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Zanchettin, Andrea Maria and Ceriani, Nicola Maria and Rocco, Paolo and Ding, Hao and Matthias, Björn},
	year = {2016},
	keywords = {Human-Robot Interaction(HRI), Human-robot collaboration, Motion, Robotics, industrial robots, motion planning, safety standards},
	pages = {882--893},
}

@article{pang_development_2018,
	title = {Development of flexible robot skin for safe and natural human-robot collaboration},
	volume = {9},
	issn = {2072666X},
	doi = {10.3390/mi9110576},
	abstract = {For industrial manufacturing, industrial robots are required to work together with human counterparts on certain special occasions, where human workers share their skills with robots. Intuitive human-robot interaction brings increasing safety challenges, which can be properly addressed by using sensor-based active control technology. In this article, we designed and fabricated a three-dimensional flexible robot skin made by the piezoresistive nanocomposite based on the need for enhancement of the security performance of the collaborative robot. The robot skin endowed the YuMi robot with a tactile perception like human skin. The developed sensing unit in the robot skin showed the one-to-one correspondence between force input and resistance output (percentage change in impedance) in the range of 0-6.5 N. Furthermore, the calibration result indicated that the developed sensing unit is capable of offering a maximum force sensitivity (percentage change in impedance per Newton force) of 18.83\% N-1 when loaded with an external force of 6.5 N. The fabricated sensing unit showed good reproducibility after loading with cyclic force (0-5.5 N) under a frequency of 0.65 Hz for 3500 cycles. In addition, to suppress the bypass crosstalk in robot skin, we designed a readout circuit for sampling tactile data. Moreover, experiments were conducted to estimate the contact/collision force between the object and the robot in a real-time manner. The experiment results showed that the implemented robot skin can provide an efficient approach for natural and secure human-robot interaction.},
	number = {11},
	journal = {Micromachines},
	author = {Pang, Gaoyang and Deng, Jia and Wang, Fangjinhua and Zhang, Junhui and Pang, Zhibo and Yang, Geng},
	year = {2018},
	keywords = {Flexible robot skin, Heterogeneous system, Human Activity, Human-Robot Interaction(HRI), Human-robot collaboration, Inkjet printing, Robotics},
	pages = {1--15},
}

@article{zanchettin_prediction_2019,
	title = {Prediction of {Human} {Activity} {Patterns} for {Human}-{Robot} {Collaborative} {Assembly} {Tasks}},
	volume = {15},
	issn = {15513203},
	doi = {10.1109/TII.2018.2882741},
	abstract = {It is widely agreed that future manufacturing environments will be populated by humans and robots sharing the same workspace. However, the real collaboration can be sporadic, especially in the case of assembly tasks, which might involve autonomous operations to be executed by either the robot or the human worker. In this scenario, it might be beneficial to predict the actions of the human in order to control the robot both safely and efficiently. In this paper, we propose a method to predict human activity patterns in order to early infer when a specific collaborative operation will be requested by the human and to allow the robot to perform alternative autonomous tasks in the meanwhile. The prediction algorithm is based on higher-order Markov chains and is experimentally verified in a realistic scenario involving a dual-arm robot employed in a small part collaborative assembly task.},
	number = {7},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Zanchettin, Andrea Maria and Casalino, Andrea and Piroddi, Luigi and Rocco, Paolo},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Cognitive human-robot interaction, Human Activity, Human-Robot Interaction(HRI), Robotics, intelligent and flexible manufacturing, planning, scheduling and coordination},
	pages = {3934--3942},
}

@article{mendes_flexible_2018,
	title = {Flexible programming and orchestration of collaborative robotic manufacturing systems},
	doi = {10.1109/INDIN.2018.8472058},
	abstract = {A flexible programming and orchestration system for human-robot collaborative tasks is proposed. Five different interaction modes are suggested to test two Task-Managers (TMs) acting as orchestrators between a human co-worker and a robot. Both TMs rely on the task-based programming concept providing modular and scalable capabilities, allowing robot code reuse, fast robot programming and high robot programming flexibility. The TMs provide visual and audio feedback to the user about the robot task sequence being executed, guiding the user during the iterative process. The interaction modes tested were: (1) human arm gestures, (2) human hand gestures, (3) physical contact between human and robot, and (4-5) two hybrid interaction modes combining each one of the two first interaction modes with the last one. Experimental tests indicated that users prefer fast interactions with small number of interaction items to higher flexibility. Both TMs provide intuitive and modular interface for collaborative robots with a human in the loop.},
	journal = {Proceedings - IEEE 16th International Conference on Industrial Informatics, INDIN 2018},
	author = {Mendes, Nuno and Safeea, Mohammad and Neto, Pedro},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538648292},
	keywords = {Collaborative robotics, Human Activity, Human-Robot Interaction(HRI), Human-robot interaction, Industry 4.0, Orchestration, Robotics, Task management},
	pages = {913--918},
}

@article{sadrfaridpour_collaborative_2018,
	title = {Collaborative {Assembly} in {Hybrid} {Manufacturing} {Cells}: {An} {Integrated} {Framework} for {Human}-{Robot} {Interaction}},
	volume = {15},
	issn = {15455955},
	doi = {10.1109/TASE.2017.2748386},
	abstract = {Recent emergence of safe, lightweight, and flexible robots has opened a new realm for human-robot collaboration in manufacturing. Utilizing such robots with the new human-robot interaction (HRI) functionality to interact closely and effectively with a human co-worker, we propose a novel framework for integrating HRI factors (both physical and social interactions) into the robot motion controller for human-robot collaborative assembly tasks in a manufacturing hybrid cell. To meet human physical demands in such assembly tasks, an optimal control problem is formulated for physical HRI (pHRI)-based robot motion control to keep pace with human motion progress. We further augment social HRI (sHRI) into the framework by considering a computational model of the human worker's trust in his/her robot partner as well as robot facial expressions. The human worker's trust in robot is computed and used as a metric for path selection as well as a constraint in the optimal control problem. Robot facial expression is displayed for providing additional visual feedbacks to the human worker. We evaluate the proposed framework by designing a robotic experimental testbed and conducting a comprehensive study with a human-in-the-loop. Results of this paper show that compared to the manual adjustments of robot velocity, an autonomous controller based on pHRI, pHRI and sHRI with trust, or pHRI and sHRI with trust, and emotion result in 34\%, 39\%, and 44\% decrease in human workload and 21\%, 32\%, and 60\% increase in robot's usability, respectively. Compared to the manual framework, human trust in robot increases by 38\% and 42\%, respectively, in the latter two autonomous frameworks. Moreover, the overall efficiency in terms of assembly time remains the same. Note to Practitioners - Conventionally, industrial robots are used to perform repetitive tasks in human-free cages with minimal HRI for safety concerns. Thanks to the new safety and flexibility functions embedded in human-friendly manufacturing robots, humans and robots can now collaborate closely with each other accomplishing the tasks that were previously done by human workers solely. However, existing criteria for designing robot controllers need to be modified by considering the human workers' demands since the performance of a human worker would vary due to factors such as individual strength, working pattern, and interaction with the robot. To address this problem, we propose a novel framework that integrates HRI factors in controlling the motion of a robot for the collaborative assembly tasks. Within this framework, the speed of robot can be controlled such that its motion progress synchronizes with that of the human during the task to improve pHRI. Moreover, for better sHRI, human trust in robot is calculated and used to select robot path and modify its speed control. Furthermore, we dynamically change the robot facial expression to provide visual feedbacks for performance and safety concerns. The results of the experimental study show that integrating both pHRI and sHRI in the robot controller leads to a significant drop of human perceived workload and considerable increase of robot usability and human trust in robot while the overall efficiency in terms of assembly time remains intact. For practical utilization in assembly plants, sensory devices for tracking the human motion are required. The robot is also required to have built-in safety functions that reduce the impact of possible collisions between the human and the robot. We believe that this paper addresses how the implementation of human-friendly robots in the manufacturing environments can improve HRI and reduce workload.},
	number = {3},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Sadrfaridpour, Behzad and Wang, Yue},
	year = {2018},
	keywords = {Assembly in manufacturing, Human Activity, Human-Robot Interaction(HRI), Robotics, collision avoidance, emotion, human arm movement, human-robot collaboration (HRC), robot motion planning and speed control, trust},
	pages = {1178--1192},
}

@article{blaga_augmented_2018,
	title = {Augmented {Reality} for {Digital} {Manufacturing}},
	doi = {10.1109/MED.2018.8443028},
	abstract = {The focus of this paper is on enhancing the possibilities of manufacturing operations by taking advantage of augmented reality (AR) technology and highlighting its benefits by implementing a product manufacturing case. The latter appearance of augmented headsets, such as the Microsoft HoloLens allows more opportunities for creating innovative solutions. After introducing concepts about smart manufacturing, improvements regarding human-robot collaboration in assembly tasks are presented. The developed scenario is based on the integration of AR, a cobot, a see-through device, a digital twin and an algorithm for assembly visualization. This approach pledges a compelling interaction of the 3D real and virtual unit, so that the operator can work in a more intuitive environment. This methodology has been implemented on a assembly case to investigate users' enhanced perception using the virtual world while cooperating with the robot.},
	journal = {MED 2018 - 26th Mediterranean Conference on Control and Automation},
	author = {Blaga, Andreea and Tamas, Levente},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538678909},
	keywords = {Assembly, Augmented reality, Human Activity, Human-Robot Interaction(HRI), Human-robot collaboration, Manufacturing, Robotics},
	pages = {173--178},
}

@article{makrini_design_2017,
	title = {Design of a collaborative architecture for human-robot assembly tasks},
	volume = {2017-Septe},
	issn = {21530866},
	doi = {10.1109/IROS.2017.8205971},
	abstract = {Collaborative robots, the so-called cobots, that work together with the human, are becoming more and more popular in the industrial world. An example of an application where these robots are useful is the assembly task. In this case, the human and the robot complement each other. On one side, the human can achieve more dexterous tasks, while on the other side, the robot can assist the assembly process to lower the physical and cognitive work load, e.g. to avoid errors, and in the same way reduce absenteeism. This paper describes a novel collaborative architecture for human-robot assembly tasks. The developed architecture is composed of four modules; face recognition, gesture recognition and human-like robot behavior modules are used to enhance the human-robot interaction, while the visual inspection module is utilized for quality control during the assembly process. A collaborative task consisting of the assembly of a box whereby the robot assists the human was designed and implemented on the Baxter robot. This was used as the application use case to validate the developed collaborative architecture.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Makrini, Ilias El and Merckaert, Kelly and Lefeber, Dirk and Vanderborght, Bram},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781538626825},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics, collaborative architecture, face recognition, gesture recognition, human-robot assembly, visual inspection},
	pages = {1624--1629},
}

@article{bos_iteratively_2017,
	title = {Iteratively {Learned} and {Temporally} {Scaled} {Force} {Control} with application to robotic assembly in unstructured environments},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989344},
	abstract = {Robotic assembly tasks are subject to uncertainties arising from part tolerances. A popular approach to deal with such partially unstructured environments is to introduce compliance by using admittance control schemes. For such schemes, contact forces and torques are speed dependent, which often limits the achievable assembly speed. To overcome this limitation, we present a new method for increasing the achievable speed of compliant manipulators by iteratively reducing contact forces. The presented concept of Iteratively Learned and Temporally Scaled Force Control (ILTSFC) is based on two coupled iterative learning controllers where one increases the assembly speed and the other one adjusts the reference trajectory to reduce contact forces. The approach is verified by an experimental peg-in-hole study on an ABB YuMi, a dual-arm collaborative robot with 7DOF each arm.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Bos, Johannes and Wahrburg, Arne and Listmann, Kim D.},
	year = {2017},
	note = {ISBN: 9781509046331},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {3000--3007},
}

@article{haage_teaching_2017,
	title = {Teaching {Assembly} by {Demonstration} {Using} {Advanced} {Human} {Robot} {Interaction} and a {Knowledge} {Integration} {Framework}},
	volume = {11},
	issn = {23519789},
	url = {http://dx.doi.org/10.1016/j.promfg.2017.07.221},
	doi = {10.1016/j.promfg.2017.07.221},
	abstract = {Conventional industrial robots are heavily dependent on hard automation that requires pre-specified fixtures and time-consuming (re)programming performed by experienced operators. In this work, teaching by human-only demonstration is used for reducing required time and expertise to setup a robotized assembly station. This is achieved by the proposed framework enhancing the robotic system with advanced perception and cognitive abilities, accessed through a user-friendly Human Robot Interaction interface. The approach is evaluated on a small parts’ assembly use case deployed onto a collaborative industrial robot testbed. Experiments indicate that the proposed approach allows inexperienced users to efficiently teach robots new assembly tasks.},
	number = {June},
	journal = {Procedia Manufacturing},
	author = {Haage, Mathias and Piperagkas, Grigoris and Papadopoulos, Christos and Mariolis, Ioannis and Malec, Jacek and Bekiroglu, Yasemin and Hedelind, Mikael and Tzovaras, Dimitrios},
	year = {2017},
	note = {Publisher: The Author(s)},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Knowledge Integration, Robotic Assembly, Robotics, Sequential Function Charts, Teaching by Demonstration},
	pages = {164--173},
}

@article{darvish_interleaved_2018,
	title = {Interleaved {Online} {Task} {Planning}, {Simulation}, {Task} {Allocation} and {Motion} {Control} for {Flexible} {Human}-{Robot} {Cooperation}},
	doi = {10.1109/ROMAN.2018.8525644},
	abstract = {Modern manufacturing paradigms introduce the need for robots able to naturally cooperate with humans in an unstructured and dynamic environment. In this article we extend FlexHRC, an architecture for flexible and collaborative manufacturing robots, with an online perception-simulation-planning framework that allows the robot to assess the status of the workspace, keeping track at all times of the stage at which the cooperative manufacturing process is, to identify its next action, to simulate it to check its feasibility and, as a consequence, to dynamically allocate tasks to itself or the human operator. We have tested the FlexHRC with a dual-arm manipulator cooperating with a person to assemble a table with one tabletop and four legs.},
	journal = {RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication},
	author = {Darvish, Kourosh and Bruno, Barbara and Simetti, Enrico and Mastrogiovanni, Fulvio and Casalino, Giuseppe},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538679807},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {58--65},
}

@article{tlach_design_2018,
	title = {The {Design} of {Method} {Intended} for {Implementation} of {Collaborative} {Assembly} {Tasks}},
	volume = {12},
	issn = {2080-4075},
	doi = {10.12913/22998624/86476},
	abstract = {One of the main trends in industrial robotics is collaboration between human and robot. Collaboration is applied especially in the field of assembly tasks. The current automation technology is not at the level that could ensure demanding tasks. These tasks are often associated with actions requiring human skills. Many manufacturers of industrial robot offer a solution in a form of a collaborative robot. These robots represent new opportunities for industry. In order to further develop the area of collaborative robotics, it is necessary to look for new technologies. This article deals with design of method which is based on open source platform. The method is applied to solve a real assembly task.},
	number = {1},
	journal = {Advances in Science and Technology Research Journal},
	author = {Tlach, Vladimir and Kuric, Ivan and Zajačko, Ivan and Kumičáková, Darina and Rengevič, Alexander},
	year = {2018},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics, human-robot collaboration, industrial robot, mi -, software platform ros},
	pages = {244--250},
}

@article{wojtynek_collaborative_2018,
	title = {Collaborative and robot-based plug \& produce for rapid reconfiguration of modular production systems},
	volume = {2018-Janua},
	doi = {10.1109/SII.2017.8279364},
	abstract = {The manufacturing of individualized products down to batch size 1 poses ongoing challenges for the design and integration of future production systems. Today's production lines with a high degree of automation achieve high efficiency, but usually come with high costs for adaptation to product variants. In order to combine full automation with high flexibility, we propose a concept for the dynamic composition of automation components in a modular production system that facilitates the rapid adaptation of collaborative and robot-supported manufacturing processes. To achieve this, we integrate self-descriptive automation components at runtime into the control architecture of the production system using a Plug-and-Produce approach. While the location and orientation of automation components in the modular production system are derived from physical human-robot interaction, the adaptation and verification of the robot behavior is made possible through a simulation-based planning subsystem. Once this dynamic reconfiguration process by the machine setter is finished, the adapted production process is executed in a fully automated way with high efficiency. A case study carried out in an industrial collaboration project on flexible assembly demonstrates the benefits of the presented approach.},
	journal = {SII 2017 - 2017 IEEE/SICE International Symposium on System Integration},
	author = {Wojtynek, Michael and Oestreich, Hendrik and Beyer, Oliver and Wrede, Sebastian},
	year = {2018},
	note = {ISBN: 9781538622636},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {1067--1073},
}

@article{whitsell_physical_2017,
	title = {Physical {Human}-{Robot} {Interaction} ({pHRI}) in 6 {DOF} with {Asymmetric} {Cooperation}},
	volume = {5},
	issn = {21693536},
	doi = {10.1109/ACCESS.2017.2708658},
	abstract = {Human-robot interaction is a growing area of research as robotic applications expand into unstructured environments. However, much of the current research has focused on tasks involving limited degrees of freedom (DOF), while not allowing the human the ability to choose the DOF on which they wish to focus. In this paper, a controller that allows human-robot cooperation in six-DOF Cartesian space is presented, which allows the human to direct their focus as they desire. The developed scheme was tested using a virtual reality system while maintaining physical interaction with the robot. Overall, the subjects were 100\% successful in completion of the tasks and were able to exchange leader/follower roles with the robot bidirectionally. In addition, a reinforcement learning algorithm was shown to decrease the estimated mechanical power applied by the human to exchange roles. The latter proves the efficiency of the proposed scheme and makes it a strong candidate for applications that involve sophisticated human-robot interaction in collaborative tasks found in a plethora of cases, e.g., industry, manufacturing, semi-autonomous driving, and so on.},
	journal = {IEEE Access},
	author = {Whitsell, Bryan and Artemiadis, Panagiotis},
	year = {2017},
	note = {Publisher: IEEE},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics, augmented reality, cooperation, pHRI, physical human-robot interaction, reinforcement learning, virtual reality},
	pages = {10834--10845},
}

@article{gustavsson_human-robot_2017,
	title = {Human-robot {Collaboration} {Demonstrator} {Combining} {Speech} {Recognition} and {Haptic} {Control}},
	volume = {63},
	issn = {22128271},
	url = {http://dx.doi.org/10.1016/j.procir.2017.03.126},
	doi = {10.1016/j.procir.2017.03.126},
	abstract = {In recent years human-robot collaboration has been an important topic in manufacturing industries. By introducing robots into the same working cell as humans, the advantages of both humans and robots can be utilized. A robot can handle heavy lifting, repetitive and high accuracy tasks while a human can handle tasks that require the flexibility of humans. If a worker is to collaborate with a robot it is important to have an intuitive way of communicating with the robot. Currently, the way of interacting with a robot is through a teaching pendant, where the robot is controlled using buttons or a joystick. However, speech and touch are two communication methods natural to humans, where speech recognition and haptic control technologies can be used to interpret these communication methods. These technologies have been heavily researched in several research areas, including human-robot interaction. However, research of combining these two technologies to achieve a more natural communication in industrial human-robot collaboration is limited. A demonstrator has thus been developed which includes both speech recognition and haptic control technologies to control a collaborative robot from Universal Robots. This demonstrator will function as an experimental platform to further research on how the speech recognition and haptic control can be used in human-robot collaboration. The demonstrator has proven that the two technologies can be integrated with a collaborative industrial robot, where the human and the robot collaborate to assemble a simple car model. The demonstrator has been used in public appearances and a pilot study, which have contributed in further improvements of the demonstrator. Further research will focus on making the communication more intuitive for the human and the demonstrator will be used as the platform for continued research.},
	journal = {Procedia CIRP},
	author = {Gustavsson, Patrik and Syberfeldt, Anna and Brewster, Rodney and Wang, Lihui},
	year = {2017},
	note = {Publisher: The Author(s)},
	keywords = {Haptic control, Human Activity, Human-Robot Interaction(HRI), Human-robot collaboration, Robotics, Speech recognition},
	pages = {396--401},
}

@article{michalos_seamless_2018,
	title = {Seamless human robot collaborative assembly – {An} automotive case study},
	volume = {55},
	issn = {09574158},
	url = {https://doi.org/10.1016/j.mechatronics.2018.08.006},
	doi = {10.1016/j.mechatronics.2018.08.006},
	abstract = {This paper presents the implementation of a robotic system for advanced human robot collaboration assembly and discusses all the technological approach that has been implemented for facilitating the interaction and support of human operators. Unlike current industrial practice where the assembly is performed by operators, the proposed approach aims at combining the benefits of high payload industrial robots with human capabilities under a fenceless environment, by assigning to them each task based on their capabilities. Enabling technologies involve manual guidance techniques and new wearables devices allowing for multi – modal interaction as well as robot safety control functionalities. Wearable devices such as Augmented Reality glasses and smartwatches are used for closing the communication loop between operators and robots under a service-oriented architecture. The complete system is validated in a case study from the automotive industry under the ROBO-PARTNER project. A detailed safety analysis of the scenario has been performed supported by a Risk Assessment, safety concept and Safety Related Parts/Control Systems (SRC/CS) design. The findings support the concept that humans’ and robots’ destiny is collaboration rather than competition.},
	number = {August},
	journal = {Mechatronics},
	author = {Michalos, George and Kousi, Niki and Karagiannis, Panagiotis and Gkournelos, Christos and Dimoulas, Konstantinos and Koukas, Spyridon and Mparis, Konstantinos and Papavasileiou, Apostolis and Makris, Sotiris},
	year = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Augmented reality, Human Activity, Human robot collaboration, Human-Robot Interaction(HRI), Integration and communication, Interaction, Robotics, Safety, Wearable devices},
	pages = {194--211},
}

@article{fakhruldeen_human_2016,
	title = {Human robot cooperation planner using plans embedded in objects},
	volume = {49},
	issn = {24058963},
	url = {http://dx.doi.org/10.1016/j.ifacol.2016.10.677},
	doi = {10.1016/j.ifacol.2016.10.677},
	abstract = {A human robot cooperation (HRC) planner for joint assembly tasks is presented. By combining first order logic (FoL) planning and object oriented programming, plans embedded in objects (PeO) can be produced. This concept shifts the focus of planning from actions to objects, thus defining them in terms of objects and combinations of their actions. The produced plans are represented using tree structures, which capture the task execution status in terms of completeness, agents’ roles and actions’ order. The planner and its application program interface (API) were written and developed using SWI-Prolog. This API allowed the planner to take various inputs to create different plans, and simultaneously to get a variety of useful outputs. The planner was tested in a collaborative pipe assembly task as a part of a bigger architecture implemented on the Baxter robot. It was able to generate the plan and achieve successful execution. This proof-of-concept planner shows great potential; allowing for a novel description in the HRC domain and execution of human-robot interaction tasks with industrial production in mind. However, first results are presented here; more work is required to take the work to higher technology readiness levels.},
	number = {21},
	journal = {IFAC-PapersOnLine},
	author = {Fakhruldeen, Hatem and Maheshwari, Pranav and Lenz, Alexander and Dailami, Farid and Pipe, Anthony G.},
	year = {2016},
	note = {Publisher: Elsevier B.V.},
	keywords = {Artificial intelligence, Assembly robots, Human Activity, Human robot interaction, Human-Robot Interaction(HRI), Planning, Robotics},
	pages = {668--674},
}

@article{gopinath_safety-focussed_2018,
	title = {Safety-{Focussed} {Design} of {Collaborative} {Assembly} {Station} with {Large} {Industrial} {Robots}},
	volume = {25},
	issn = {23519789},
	url = {https://doi.org/10.1016/j.promfg.2018.06.124},
	doi = {10.1016/j.promfg.2018.06.124},
	abstract = {The perceived benefits of large industrial robots for collaborative operations are characteristics such as long reach with heavy load carrying capability. Collaborative operations refers to situations where operators and robots share a workspace to complete tasks in close proximity. This mode of operation coupled with the physical characteristics of large robots represents high risks to injury and for these reasons, the safeguarding of the workspaces needs to be achieved in conjunction with the tasks to be performed within the workstation. This article will detail two workstations that was developed in a laboratory environment and are partial results of a research project titled ToMM2, whose aim was to understand safety issues associated with collaborative operations with large robots.},
	journal = {Procedia Manufacturing},
	author = {Gopinath, Varun and Ore, Fredrik and Grahn, Sten and Johansen, Kerstin},
	year = {2018},
	note = {Publisher: Elsevier B.V.},
	keywords = {Collaborative Operations, Hazards, Human Activity, Human-Robot Collaboration, Human-Robot Interaction(HRI), Industrial Robot Safety, Risks, Robotics},
	pages = {503--510},
}

@article{koch_skill-based_2017,
	title = {A {Skill}-based {Robot} {Co}-worker for {Industrial} {Maintenance} {Tasks}},
	volume = {11},
	issn = {23519789},
	doi = {10.1016/j.promfg.2017.07.141},
	abstract = {This paper investigates the concept of a sensor based robot co-worker working in flexible industrial environments together with and alongside human operators. In this particular work, a realisation of a robot co-worker scenario is developed in order to demonstrate the implementation of a robot co-worker from the starting point of an autonomous industrial mobile manipulator. The cobot is applied on the industrially relevant task of screwing by the use of a skill-based approach. The technical work on the human-robot interface and the screwing skill is described.},
	number = {June},
	journal = {Procedia Manufacturing},
	author = {Koch, Paul J. and van Amstel, Marike K. and Dębska, Patrycja and Thormann, Moritz A. and Tetzlaff, Adrian J. and Bøgh, Simon and Chrysostomou, Dimitrios},
	year = {2017},
	keywords = {Collaborative Robot, Human Activity, Human-Robot Interaction(HRI), Industry 4.0, Intelligent Manufacturing, Maintenance Task, Robotics, Screwing Operation, Skill-based Programing},
	pages = {83--90},
}

@article{makris_augmented_2016,
	title = {Augmented reality system for operator support in human–robot collaborative assembly},
	volume = {65},
	issn = {17260604},
	url = {http://dx.doi.org/10.1016/j.cirp.2016.04.038},
	doi = {10.1016/j.cirp.2016.04.038},
	abstract = {This paper presents the design and implementation of an augmented reality (AR) tool in aid of operators being in a hybrid, human and robot collaborative industrial environment. The system aims to provide production and process related information as well as to enhance the operators’ immersion in the safety mechanisms, dictated by the collaborative workspace. The developed system has been integrated with a service based station controller, which is responsible for orchestrating the flow of information to the operator, according to the task execution status. The tool has been applied to a case study from the automotive sector, resulting in an enhanced operator's integration with the assembly process.},
	number = {1},
	journal = {CIRP Annals - Manufacturing Technology},
	author = {Makris, Sotiris and Karagiannis, Panagiotis and Koukas, Spyridon and Matthaiakis, Aleksandros Stereos},
	year = {2016},
	note = {Publisher: CIRP},
	keywords = {Augmented reality, Human Activity, Human-Robot Interaction(HRI), Hybrid assembly system, Robot, Robotics},
	pages = {61--64},
}

@article{craighero_hand_2002,
	title = {Hand action preparation influences the responses to hand pictures},
	volume = {40},
	issn = {00283932},
	url = {http://dx.doi.org/10.1016/j.visres.2010.12.009},
	doi = {10.1016/S0028-3932(01)00134-8},
	abstract = {The relations between stimuli triggering a hand grasping movement and the subsequent action were studied in normal human participants. Participants were instructed to prepare to grasp a bar, oriented either clockwise or counterclockwise, and to grasp it as fast as possible on presentation of a visual stimulus with their right hand. The visual stimuli were pictures of the right hand as seen in a mirror. In Experiment 1, they represented the mirror image of the hand final posture as achieved in grasping the bar oriented either clockwise or counterclockwise. In Experiment 2, in addition to the pictures of Experiment 1, another two pictures, obtained rotating the hands represented in the previous ones of 90??, were also used. Both experiments showed that the reaction times were faster when there was a similarity between hand position as depicted in the triggering visual stimulus and the grasping hand final position, the fastest responses being those where this similarity was the closest. In addition, Experiment 2 showed that reaction times to not rotated stimuli were faster than reaction times to the rotated stimuli, thus excluding a simple stimulus-response compatibility explanation of the findings. The data are interpreted as behavioral evidence that there is a close link between specific visual stimuli and specific motor actions. A neurophysiological model for this visuo-motor link is presented. Copyright ?? 2001 Elsevier Science Ltd.},
	number = {5},
	journal = {Neuropsychologia},
	author = {Craighero, Laila and Bello, Arianna and Fadiga, Luciano and Rizzolatti, Giacomo and Aggarwal, Jk and Ryoo, Ms and Bisio, Ambra and Sciutti, Alessandra and Nori, Francesco and Metta, Giorgio and Fadiga, Luciano and Sandini, Giulio and Pozzo, Thierry and Castiello, Umberto and Chinellato, E and Huber, Markus and Kupferberg, Aleksandra and Lenz, Claus and Knoll, Alois and Brandt, Thomas and Glasauer, Stefan and Jamone, Lorenzo and Ugur, Emre and Cangelosi, Angelo and Fadiga, Luciano and Bernardino, Alexandre and Piater, Justus and Santos-Victor, Jose and Koskinopoulou, Maria and Trahanias, Panos and Lenz, I. and Lee, H. and Saxena, A. and Lewis, Gary J. and Bates, Timothy C. and Lindemann, Oliver and Nuku, Pines and Rueschemeyer, Shirley Ann and Bekkering, Harold and Oztop, Erhan and Ugur, Emre and Shimizu, Yu and Imamizu, Hiroshi and Paulignan, Y and Frak, V G and Toni, I and Jeannerod, M and Pezzulo, Giovanni and Donnarumma, Francesco and Dindo, Haris and Sartori, Luisa and Camperio-Ciani, Andrea and Bulgheroni, Maria and Castiello, Umberto and Sciutti, Alessandra and Bisio, Ambra and Nori, Francesco and Metta, Giorgio and Fadiga, Luciano and Sandini, Giulio and Sebanz, Natalie and Bekkering, Harold and Knoblich, G??nther and Urbaniak, Anthony and Craighero, Laila and Olivier, Etienne},
	year = {2002},
	pmid = {11749979},
	note = {arXiv: 1301.3592v5
Publisher: Elsevier Ltd
ISBN: 0028-3932},
	keywords = {2, 3d feature learning, Action observation, Animals, Anticipation, Arm, Arm: physiology, Biomechanical Phenomena, Brain, Brain: physiology, Brain: radionuclide imaging, Female, Fingers, Grasp cueing, Hand, Hand Strength, Hand Strength: physiology, Hand: physiology, Humans, Joint spatial attention, Magnetic Resonance Imaging, Male, Mirror system, Motor Activity, Motor Resonance, Motor preparation, Movement, Movement: physiology, Neuropsychology, Neuropsychology: methods, Neurosciences, Neurosciences: methods, Positron-Emission Tomography, Posture, Proactive Gaze, Psychomotor Performance, Social cognition, Space Perception, Thumb, Visual discrimination, Wrist Joint, a review, acm computing surveys, aggarwal 1 and m, appear, baxter, deep learning, human activity analysis, j, k, pr2, rgb-d multi-modal data, robotic grasping, ryoo 1, s},
	pages = {492--502},
}

@article{rahman_trust-based_2016,
	title = {Trust-based compliant robot-human handovers of payloads in collaborative assembly in flexible manufacturing},
	volume = {2016-Novem},
	issn = {21618089},
	doi = {10.1109/COASE.2016.7743428},
	abstract = {A human-robot hybrid cell is developed for performing assembly in flexible manufacturing in collaboration between a robot and its human co-worker. Robot trust in human is considered, a computational model for the trust is derived, and a method to measure and display the trust in real-time is developed. The collaborative assembly includes robot-to-human handovers of payloads (assembly tools). A novel trust-based compliant handover motion planning strategy for the robot is derived. The robot varies its handover configuration and motion based on robot trust in human through kinematic redundancy with the aim of reducing potential impulse forces on human body through payload during handover. A comprehensive scheme is developed to evaluate the collaborative assembly including the trust-based handover strategy. The evaluation results show that consideration of robot trust in human during the assembly and adjustment in handover configuration and motion based on robot's trust levels in human significantly improve human-robot interaction and assembly performance through increasing safety, human trust in robot, handover success rate, and the overall assembly efficiency by 20\%, 37.58\%, 30\% and 6.73\% respectively and reducing cognitive workload by 25.63\%, with a minor reduction in the handover efficiency by 1.87\%.},
	journal = {IEEE International Conference on Automation Science and Engineering},
	author = {Rahman, S. M.Mizanoor and Wang, Yue and Walker, Ian D. and Mears, Laine and Pak, Richard and Remy, Sekou},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509024094},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {355--360},
}

@article{walker_robot-human_2016,
	title = {Robot-{Human} {Handovers} {Based} on {Trust}},
	doi = {10.1109/MCSI.2015.50},
	abstract = {We present a new approach to payload handovers between robots and humans in collaborative human-co-robot operations. The key innovation is the incorporation of robot trust (in the human) in the underlying robot motion planning algorithm. Using a weighted Jacobian pseudoinverse algorithm, the robot motions are varied (trading off collision risk against task efficiency) based on the current value of trust. The targeted application is small-scale manufacturing, but the approach can be applied to many forms of robot-human handovers and interactions.},
	journal = {Proceedings - 2015 2nd International Conference on Mathematics and Computers in Sciences and in Industry, MCSI 2015},
	author = {Walker, Ian D. and Mears, Laine and Mizanoor, Rahman S.M. and Pak, Richard and Remy, Sekou and Wang, Yue},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781479986736},
	keywords = {Handovers, Human Activity, Human-Robot Interaction(HRI), Robotics, human-robot interaction, kinematic redundancy, trust},
	pages = {119--124},
}

@article{hawkins_anticipating_2014,
	title = {Anticipating human actions for collaboration in the presence of task and sensor uncertainty},
	issn = {10504729},
	doi = {10.1109/ICRA.2014.6907165},
	abstract = {A representation for structured activities is developed that allows a robot to probabilistically infer which task actions a human is currently performing and to predict which future actions will be executed and when they will occur. The goal is to enable a robot to anticipate collaborative actions in the presence of uncertain sensing and task ambiguity. The system can represent multi-path tasks where the task variations may contain partially ordered actions or even optional actions that may be skipped altogether. The task is represented by an AND-OR tree structure from which a probabilistic graphical model is constructed. Inference methods for that model are derived that support a planning and execution system for the robot which attempts to minimize a cost function based upon expected human idle time. We demonstrate the theory in both simulation and actual human-robot performance of a two-way-branch assembly task. In particular we show that the inference model can robustly anticipate the actions of the human even in the presence of unreliable or noisy detections because of its integration of all its sensing information along with knowledge of task structure.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Hawkins, Kelsey P. and Bansal, Shray and Vo, Nam N. and Bobick, Aaron F.},
	year = {2014},
	note = {Publisher: IEEE
ISBN: 9781479936854},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {2215--2222},
}

@article{hamabe_programming_2015,
	title = {A programming by demonstration system for human-robot collaborative assembly tasks},
	doi = {10.1109/ROBIO.2015.7418934},
	abstract = {Programming by demonstration (PbD) has been one of the promising approaches to robot programming, where a robot learns how to operate by observing human demonstrations. While most of existing PbD research dealt with single-agent tasks, this paper deals with collaborative tasks, in which a robot and a human collaborate to assemble a structure by coordinating their operations. The system has three parts: demonstration observation parts, task modeling parts, and task execution parts. A scene recognition system is developed for reliable action recognition in human demonstrations. The developed PbD system has successfully been applied to a collaborative assembly tasks with more than twenty assembly steps.},
	journal = {2015 IEEE International Conference on Robotics and Biomimetics, IEEE-ROBIO 2015},
	author = {Hamabe, Takuma and Goto, Hiraki and Miura, Jun},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781467396745},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Programming by demonstration, Robotics, collaborative assembly, humanoid},
	pages = {1195--1201},
}

@article{rozo_learning_2016,
	title = {Learning {Physical} {Collaborative} {Robot} {Behaviors} {From} {Human} {Demonstrations}},
	volume = {32},
	issn = {15523098},
	doi = {10.1109/TRO.2016.2540623},
	abstract = {Robots are becoming safe and smart enough to work alongside people not only on manufacturing production lines, but also in spaces such as houses, museums, or hospitals. This can be significantly exploited in situations in which a human needs the help of another person to perform a task, because a robot may take the role of the helper. In this sense, a human and the robotic assistant may cooperatively carry out a variety of tasks, therefore requiring the robot to communicate with the person, understand his/her needs, and behave accordingly. To achieve this, we propose a framework for a user to teach a robot collaborative skills from demonstrations. We mainly focus on tasks involving physical contact with the user, in which not only position, but also force sensing and compliance become highly relevant. Specifically, we present an approach that combines probabilistic learning, dynamical systems, and stiffness estimation to encode the robot behavior along the task. Our method allows a robot to learn not only trajectory following skills, but also impedance behaviors. To show the functionality and flexibility of our approach, two different testbeds are used: a transportation task and a collaborative table assembly.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Rozo, Leonel and Calinon, Sylvain and Caldwell, Darwin G. and Jiménez, Pablo and Torras, Carme},
	year = {2016},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Physical human-robot interaction, Robot learning, Robotics, programming by demonstration (PbD), stiffness estimation},
	pages = {513--527},
}

@article{rahman_regret-based_2016,
	title = {A regret-based autonomy allocation scheme for human-robot shared vision systems in collaborative assembly in manufacturing},
	volume = {2016-Novem},
	issn = {21618089},
	doi = {10.1109/COASE.2016.7743497},
	abstract = {In human-robot collaboration (HRC) for assembly, a robot needs to use its onboard vision systems to perform many sensing tasks. The robot's observations may not be always reliable due to erroneous sensing caused by limitations of the detection systems and disturbances. Hence, additional observations made by the robot or the human co-worker seem to be necessary. However, too much human involvement may increase human workload. Hence, allocation of autonomy through switching between autonomous and manual sensing (observation) modes seems to be reasonable. Bayesian sequential decision-making may be an approach to determine the optimal allocation of these modes in the presence of sensing uncertainties. However, optimal decision-making approaches such as the Bayesian approach may not necessarily fit with human's decision style. It has been shown that human regret plays a critical role in decision-making under uncertainty. It is rational for humans to make suboptimal choices to avoid regret. In this paper, we include human regret analysis in Bayesian decision-making in the automated allocation of sensing modes for detection of right assembly parts. We evaluate this approach in a human-robot collaborative assembly task. The experimental results show that regret-based human-like suboptimal allocation of autonomous and manual sensing modes improves human-robot interaction and assembly performance.},
	number = {Cmmi},
	journal = {IEEE International Conference on Automation Science and Engineering},
	author = {Rahman, S. M.Mizanoor and Liao, Zhanrui and Jiang, Longsheng and Wang, Yue},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509024094},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {897--902},
}

@article{guerin_framework_2015,
	title = {A framework for end-user instruction of a robot assistant for manufacturing},
	volume = {2015-June},
	issn = {10504729},
	doi = {10.1109/ICRA.2015.7140065},
	abstract = {Small Manufacturing Entities (SMEs) have not incorporated robotic automation as readily as large companies due to rapidly changing product lines, complex and dexterous tasks, and the high cost of start-up. While recent low-cost robots such as the Universal Robots UR5 and Rethink Robotics Baxter are more economical and feature improved programming interfaces, based on our discussions with manufacturers further incorporation of robots into the manufacturing work flow is limited by the ability of these systems to generalize across tasks and handle environmental variation. Our goal is to create a system designed for small manufacturers that contains a set of capabilities useful for a wide range of tasks, is both powerful and easy to use, allows for perceptually grounded actions, and is able to accumulate, abstract, and reuse plans that have been taught. We present an extension to Behavior Trees that allows for representing the system capabilities of a robot as a set of generalizable operations that are exposed to an end-user for creating task plans. We implement this framework in CoSTAR, the Collaborative System for Task Automation and Recognition, and demonstrate its effectiveness with two case studies. We first perform a complex tool-based object manipulation task in a laboratory setting. We then show the deployment of our system in an SME where we automate a machine tending task that was not possible with current off the shelf robots.},
	number = {June},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Guerin, Kelleher R. and Lea, Colin and Paxton, Chris and Hager, Gregory D.},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781479969234},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {6167--6174},
}

@article{zanchettin_path-consistent_2013,
	title = {Path-consistent safety in mixed human-robot collaborative manufacturing environments},
	issn = {21530858},
	doi = {10.1109/IROS.2013.6696492},
	abstract = {In order to improve production flexibility, it is widely agreed that future working environments will be populated by both humans and robot manipulators, sharing the same workspace. This scenario introduces a series of safety issues which are uncommon in industrial settings where physical separation of robot areas is typically enforced. While several approaches for safe human-robot interaction exist, none of them can be easily integrated with production constraints. This paper discusses the composition of safety constraints with production ones. An algorithm is derived in order to maximize productivity, while guaranteeing a safe separation distance of the robot from the human. Experimental results showing the effectiveness of the approach in a typical industrial setting are also discussed. © 2013 IEEE.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Zanchettin, Andrea Maria and Rocco, Paolo},
	year = {2013},
	note = {Publisher: IEEE
ISBN: 9781467363587},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {1131--1136},
}

@article{ding_structured_2013,
	title = {Structured collaborative behavior of industrial robots in mixed human-robot environments},
	issn = {21618070},
	doi = {10.1109/CoASE.2013.6653962},
	abstract = {In mixed human-robot environments safety and productivity are two important factors for the design of robotic collaborative behaviors. The collaborative behavior should be able to uphold productivity as far as possible while respecting safety constraints for varying collaboration modes and evaluation criteria. In this paper, a concept of using a finite state automaton for structuring the collaborative behavior of industrial robots is proposed to systematically handle the following exceptions. Safety and productivity (S\&P) exceptions which either interfere with productivity or compromise workers safety are caught by the corresponding exception reaction. The regular production operation after an exception reaction is restored with exception recovery strategies. The state automaton model for an industrial assembly scenario is finally presented to illustrate this concept. A demonstration realizing the example for interaction between an ABB Dual-Arm Concept Robot and a human is discussed. © 2013 IEEE.},
	journal = {IEEE International Conference on Automation Science and Engineering},
	author = {Ding, Hao and Heyn, Jakob and Matthias, Bjorn and Staab, Harald},
	year = {2013},
	note = {ISBN: 9781479915156},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {1101--1106},
}

@article{arai_new_2009,
	title = {A new cell production assembly system with twin manipulators on mobile base},
	number = {November},
	journal = {Management},
	author = {Arai, T. and Duan, F. and Kato, R. and Tan, J.T.C. and Fujita, M. and Morioka, M. and Sakakibara, S},
	year = {2009},
	note = {Publisher: IEEE
ISBN: 9781424446285},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {1--5},
}

@article{tan_human_2009,
	title = {Human factors studies in information support development for human-robot collaborative cellular manufacturing system},
	doi = {10.1109/ROMAN.2009.5326263},
	abstract = {The purpose of this work is to conduct human factors studies in the development of an information support system for human-robot collaborative cellular manufacturing system. Multimedia technologies are being utilized to enhance the support system to function as operational information support and interface between human operator and robot system. In the study, five experiments were carried out in both system hardware and support information developments to investigate the human factors design of the support system. In the information display approach study, comparison of information presentation ability among paper manual, LCD TV and projector was conducted. Similar information was displayed in different visual areas by vertical and horizontal LCD TV configurations to study the effect on human visual working area. The comparison experiment on image, text and voice formats had resulted varying modality effectiveness of information. Different information formats require different human cognitive processes even though the information contents are similar. The investigation on multimedia displays in five combinations of information elements was performed in multimedia display design study. The information support system is implemented in a human-robot collaborative prototype production cell. The cable harness assembly experiment had proven the effectiveness of the information support system in guiding the operators on the assembly operation and also to collaborate well with the robot system. © 2009 IEEE.},
	journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
	author = {Tan, Jeffrey Too Chuan and Zhang, Ye and Duan, Feng and Watanabe, Kei and Kato, Ryu and Arai, Tamio},
	year = {2009},
	note = {Publisher: IEEE
ISBN: 9781424450817},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {334--339},
}

@article{lenz_constraint_2009,
	title = {Constraint task-based control in industrial settings},
	doi = {10.1109/IROS.2009.5354631},
	abstract = {Direct physical human-robot interaction has become a central part in the research field of robotics today. To use the advantages of the potential for humans and robots to work together as a team in industrial settings, the most important issues are safety for the human and an easy way to describe tasks for the robot. In this work, we present an approach of a hierarchical structured control of industrial robots for joint-action scenarios. Multiple atomic tasks including dynamic collision avoidance, operational position, and posture can be combined in an arbitrary order respecting constraints of higher priority tasks. The controller flow is based on the theory of orthogonal projection using nullspaces and constraint least-square optimization. To proof the approach, we present three collaboration scenarios between a human and an industrial robot. © 2009 IEEE.},
	journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
	author = {Lenz, Claus and Rickert, Markus and Panin, Giorgio and Knoll, Alois},
	year = {2009},
	note = {ISBN: 9781424438044},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {3058--3063},
}

@article{ding_collaborative_2013,
	title = {Collaborative behavior design of industrial robots for multiple human-robot collaboration},
	doi = {10.1109/ISR.2013.6695707},
	abstract = {Industrial robots are being introduced to assist human workers in performing assembly tasks such as small-parts assembly. A mixed environment is the best choice for certain assembly operations, of which some are better executed by robots and others are better handled by human workers. However, it is a challenge to design the collaborative behavior of robots to maximize productivity respecting to safety constraints while interacting with human workers. This becomes more complex for multiple human-robot collaboration. An approach has been proposed for structuring the collaborative behavior using finite state automata (FSA). In this paper, the approach is extended to deal with multiple human-robot collaboration, by applying the composition rules of FSA. The approach is finally demonstrated in an ABB Dual-Arm Concept Robot working with multiple human workers in an industrial assembly scenario. © 2013 IEEE.},
	number = {November},
	journal = {2013 44th International Symposium on Robotics, ISR 2013},
	author = {Ding, Hao and Schipper, Malte and Matthias, Bjorn},
	year = {2013},
	note = {ISBN: 9781479911738},
	keywords = {Collaborative behavior, Finite state automata, Human Activity, Human-Robot Interaction(HRI), Multiple human-robot collaboration, Robotics, Small parts assembly},
}

@article{sartori_intersegmental_2015,
	title = {Intersegmental coordination in the kinematics of prehension movements of macaques},
	volume = {10},
	issn = {19326203},
	doi = {10.1371/journal.pone.0132937},
	abstract = {The most popular model to explain how prehensile movements are organized assumes that they comprise two "components", the reaching component encoding information regarding the object's spatial location and the grasping component encoding information on the object's intrinsic properties such as size and shape. Comparative kinematic studies on grasping behavior in the humans and in macaques have been carried out to investigate the similarities and differences existing across the two species. Although these studies seem to favor the hypothesis that macaques and humans share a number of kinematic features it remains unclear how the reaching and grasping components are coordinated during prehension movements in free-ranging macaque monkeys. Twelve hours of video footage was filmed of the monkeys as they snatched food items from one another (i.e., snatching) or collect them in the absence of competitors (i.e., unconstrained). The video samples were analyzed frame-by-frame using digitization techniques developed to perform two-dimensional post-hoc kinematic analyses of the two types of actions. The results indicate that only for the snatching condition when the reaching variability increased there was an increase in the amplitude of maximum grip aperture. Besides, the start of a break-point along the deceleration phase of the velocity profile correlated with the time at which maximum grip aperture occurred. These findings suggest that macaques can spatially and temporally couple the reaching and the grasping components when there is pressure to act quickly. They offer a substantial contribution to the debate about the nature of how prehensile actions are programmed.},
	number = {7},
	journal = {PLoS ONE},
	author = {Sartori, Luisa and Camperio-Ciani, Andrea and Bulgheroni, Maria and Castiello, Umberto and Castiello., Umberto},
	year = {2015},
	pmid = {26176232},
	keywords = {Grasp, Human, Human Activity, Macaque, Motion, Neuroscience},
	pages = {1--11},
}

@article{kupcsik_learning_2016,
	title = {Learning {Dynamic} {Robot}-to-{Human} {Object} {Handover} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/1603.06390},
	doi = {10.1007/978-3-319-51532-8_10},
	abstract = {Object handover is a basic, but essential capability for robots interacting with humans in many applications, e.g., caring for the elderly and assisting workers in manufacturing workshops. It appears deceptively simple, as humans perform object handover almost flawlessly. The success of humans, however, belies the complexity of object handover as collaborative physical interaction between two agents with limited communication. This paper presents a learning algorithm for dynamic object handover, for example, when a robot hands over water bottles to marathon runners passing by the water station. We formulate the problem as contextual policy search, in which the robot learns object handover by interacting with the human. A key challenge here is to learn the latent reward of the handover task under noisy human feedback. Preliminary experiments show that the robot learns to hand over a water bottle naturally and that it adapts to the dynamics of human motion. One challenge for the future is to combine the model-free learning algorithm with a model-based planning approach and enable the robot to adapt over human preferences and object characteristics, such as shape, weight, and surface texture.},
	author = {Kupcsik, Andras and Hsu, David and Lee, Wee Sun},
	year = {2016},
	note = {arXiv: 1603.06390},
	keywords = {Cup, Handovers, Human-Robot Interaction(HRI)},
	pages = {161--176},
}

@article{hayes_interpretable_2017,
	title = {Interpretable models for fast activity recognition and anomaly explanation during collaborative robotics tasks},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989778},
	abstract = {In this paper, we present Rapid Activity Prediction Through Object-oriented Regression (RAPTOR), a scalable method for performing rapid, real-time activity recognition and prediction that achieves state-of-the-art classification accuracy on both a generic human activity dataset and two domain-specific collaborative robotics manufacturing datasets. Our approach is designed to be human-interpretable: able to provide explanations for its reasoning such that non-experts can better understand and improve its activity models. We incorporate methods to increase RAPTOR's resilience against confusion due to temporal variations, as well as against learning false correlations between features. We report full and partial trajectory classification results across three datasets and conclude by demonstrating our model's ability to provide interpretable explanations of its reasoning using outlier detection techniques.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Hayes, Bradley and Shah, Julie A.},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509046331},
	keywords = {Action Recognition, Markov, Object Placement, Robotics},
	pages = {6586--6593},
}

@article{koskinopoulou_methodological_2016,
	title = {A {Methodological} {Framework} for {Robotic} {Reproduction} of {Observed} {Human} {Actions} : {Formulation} using {Latent} {Space} {Representation} *},
	issn = {21640580},
	doi = {10.1109/HUMANOIDS.2016.7803331},
	abstract = {The current work presents a comprehensive methodological framework that facilitates robots to acquire human-like behavioral acts by observing human demonstrators. Accordingly, the introduced framework is established as a Learning from Demonstration (LfD) process that enables the reproduction of either learned or novel actions. Mapping of human actions to the respective robotic ones is achieved via an indeterminate depiction, termed latent space representation. The latter accomplishes a compact, yet precise abstraction of action trajectories, effectively representing high dimensional raw actions in a low dimensional space. Extensive experimen- tation with a real robotic arm demonstrates the robustness and applicability of the introduced framework.},
	number = {December},
	journal = {2016 IEEE-RAS International Conference on Humanoid Robots (Humanoids 2016)},
	author = {Koskinopoulou, Maria and Trahanias, Panos},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509047178},
	keywords = {Action understanding, Human-Robot Interaction(HRI), Robotics},
	pages = {565--572},
}

@article{liu_human_2017,
	title = {Human motion prediction for human-robot collaboration},
	volume = {44},
	issn = {02786125},
	url = {http://dx.doi.org/10.1016/j.jmsy.2017.04.009},
	doi = {10.1016/j.jmsy.2017.04.009},
	abstract = {In human-robot collaborative manufacturing, industrial robots would work alongside human workers who jointly perform the assigned tasks seamlessly. A human-robot collaborative manufacturing system is more customised and flexible than conventional manufacturing systems. In the area of assembly, a practical human-robot collaborative assembly system should be able to predict a human worker's intention and assist human during assembly operations. In response to the requirement, this research proposes a new human-robot collaborative system design. The primary focus of the paper is to model product assembly tasks as a sequence of human motions. Existing human motion recognition techniques are applied to recognise the human motions. Hidden Markov model is used in the motion sequence to generate a motion transition probability matrix. Based on the result, human motion prediction becomes possible. The predicted human motions are evaluated and applied in task-level human-robot collaborative assembly.},
	journal = {Journal of Manufacturing Systems},
	author = {Liu, Hongyi and Wang, Lihui},
	month = jul,
	year = {2017},
	note = {Publisher: The Society of Manufacturing Engineers},
	keywords = {Assembly, Human motion prediction, Human-Robot Interaction(HRI), Human-robot collaboration, Motion, Prediction},
	pages = {287--294},
}

@article{munzer_efficient_2018,
	title = {Efficient behavior learning in human–robot collaboration},
	volume = {42},
	issn = {15737527},
	url = {https://doi.org/10.1007/s10514-017-9674-5},
	doi = {10.1007/s10514-017-9674-5},
	abstract = {We present a novel method for a robot to interactively learn, while executing, a joint human–robot task. We consider collaborative tasks realized by a team of a human operator and a robot helper that adapts to the human’s task execution preferences. Different human operators can have different abilities, experiences, and personal preferences so that a particular allocation of activities in the team is preferred over another. Our main goal is to have the robot learn the task and the preferences of the user to provide a more efficient and acceptable joint task execution. We cast concurrent multi-agent collaboration as a semi-Markov decision process and show how to model the team behavior and learn the expected robot behavior. We further propose an interactive learning framework and we evaluate it both in simulation and on a real robotic setup to show the system can effectively learn and adapt to human expectations.},
	number = {5},
	journal = {Autonomous Robots},
	author = {Munzer, Thibaut and Toussaint, Marc and Lopes, Manuel},
	year = {2018},
	note = {Publisher: Springer US},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Human–robot collaboration, Interactive learning, Joint-action, Markov, Relational learning},
	pages = {1103--1115},
}

@article{tan_safety_2010,
	title = {Safety strategy for human-robot collaboration: {Design} and development in cellular manufacturing},
	volume = {24},
	issn = {01691864},
	doi = {10.1163/016918610X493633},
	abstract = {Our research aims to design and develop a safety strategy for a human-robot collaboration system. Although robotic assistance in a cellular manufacturing system is promising, safety is the uppermost consideration before it can be materialized. Five main safety designs are developed in this work. (i) Safe working areas for humans and robots. (ii) To control the behavior of the robot based on the collaboration requirements, light curtains defined safe collaborative working zones. (iii) Additionally, the robot system was developed using safe mechanical design and Dual Check Safety control strategies in terms of robot speed and travel area to minimize collaboration risks. (iv) A vision system using IP cameras was developed to monitor operator safety conditions by measuring the body posture and position of the operator. (v) The operation control system coordinated the collaborative flow between the operator and robot system. Apart from these developments, risk assessments were conducted to evaluate the safety design of the system, and a mental safety study was performed to investigate robot motion speed and working distance on the operator's physiological effects. Our findings demonstrate the feasibility of the prototype system to safely perform assembly operations. © 2010 Koninklijke Brill NV, Leiden and The Robotics Society of Japan.},
	number = {5-6},
	journal = {Advanced Robotics},
	author = {Tan, Jefferey Too Chuan and Duan, Feng and Kato, Ryu and Arai, Tamio},
	year = {2010},
	note = {Publisher: IEEE
ISBN: 9781424438044},
	keywords = {Cellular manufacturing, Human Activity, Human-Robot Interaction(HRI), Human-robot collaboration, Robotics, Safety},
	pages = {839--860},
}

@article{ende_human-centered_2011,
	title = {A human-centered approach to robot gesture based communication within collaborative working processes},
	doi = {10.1109/IROS.2011.6048257},
	abstract = {The increasing ability of industrial robots to perform complex tasks in collaboration with humans requires more capable ways of communication and interaction. Traditional systems use separate interfaces such as touchscreens or control panels in order to operate the robot, or to communicate its state and prospective actions to the user. Transferring human communication, such as gestures to technical non-humanoid robots, creates various opportunities for more intuitive human-robot-interaction. Interaction shall no longer require a separate interface such as a control panel. Instead, it should take place directly between human and robot. To explore intuitive interaction, we identified gestures that are relevant for co-working tasks from human observations. Based on a decomposition approach we transferred them to robotic systems of increasing abstraction and experimentally evaluated how well these gestures are recognized by humans. We created a human-robot interaction use-case in order to perform the task of handling dangerous liquid. Results indicate that several gestures are well perceived when displayed with context information regarding the task. © 2011 IEEE.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Ende, Tobias and Haddadin, Sami and Parusel, Sven and Wüsthoff, Tilo and Hassenzahl, Marc and Albu-Schäffer, Alin},
	year = {2011},
	note = {ISBN: 9781612844541},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Robotics},
	pages = {3367--3374},
}

@article{wang_facilitating_2019,
	title = {Facilitating {Human}-{Robot} {Collaborative} {Tasks} by {Teaching}-{Learning}-{Collaboration} from {Human} {Demonstrations}},
	volume = {16},
	issn = {15455955},
	doi = {10.1109/TASE.2018.2840345},
	abstract = {Collaborative robots are widely employed in strict hybrid assembly tasks involved in intelligent manufacturing. In this paper, we develop a teaching-learning-collaboration (TLC) model for the collaborative robot to learn from human demonstrations and assist its human partner in shared working situations. The human could program the robot using natural language instructions according to his/her personal working preferences via this approach. Afterward, the robot learns from human assembly demonstrations by taking advantage of the maximum entropy inverse reinforcement learning algorithm and updates its task-based knowledge using the optimal assembly strategy. In the collaboration process, the robot is able to leverage its learned knowledge to actively assist the human in the collaborative assembly task. Experimental results and analysis demonstrate that the proposed approach presents considerable robustness and applicability in human-robot collaborative tasks. Note to Practitioners - This paper is motivated by the human-robot collaborative assembly problem in the context of advanced manufacturing. Collaborative robotics makes a huge shift from the traditional robot-in-a-cage model to robots interacting with people in an open working environment. When the human works with the robot in the shared workspace, it is significant to lessen human programming effort and improve the human-robot collaboration efficiency once the task is updated. We develop a TLC model for the robot to learn from human demonstrations and assist its human partner in collaborative tasks. Once the task is changed, the human may code the robot via natural language instructions according to his/her personal working preferences. The robot can learn from human assembly demonstrations to update its task-based knowledge, which can be leveraged by the robot to actively assist the human to accomplish the collaborative task. We demonstrate the advantages of the proposed approach via a set of experiments in realistic human-robot collaboration contexts.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Wang, Weitian and Li, Rui and Chen, Yi and Diekel, Z. Max and Jia, Yunyi},
	month = apr,
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Human-centered manufacture, human-robot collaboration, learning from demonstrations, maximum entropy inverse reinforcement learning (MaxEnt-IRL), natural language},
	pages = {640--653},
}

@article{michalos_method_2018,
	title = {A method for planning human robot shared tasks},
	volume = {22},
	issn = {17555817},
	url = {https://doi.org/10.1016/j.cirpj.2018.05.003},
	doi = {10.1016/j.cirpj.2018.05.003},
	abstract = {A multi criteria method, in support of the planning of shared human robot assembly tasks is presented in this paper. CAD models are used for the extraction of a product's assembly sequence, whilst an algorithm is applied for the generation and examination of alternative assignment scenarios. The design algorithm performs the joint planning of task-to-resource assignment and cell layout at the same time. The evaluation of each planning scenario against a set of ergonomics, quality and productivity criteria is made for the identification of the most efficient ones. The method's implementation, in the form of a prototype planning tool (Task Planner), enables its application to a pilot case from the automotive industry. The results indicate that the method can provide high quality solutions with respect to the end users’ criteria and has potential applicability as a decision making support tool in the planning stage.},
	journal = {CIRP Journal of Manufacturing Science and Technology},
	author = {Michalos, George and Spiliotopoulos, Jason and Makris, Sotiris and Chryssolouris, George},
	year = {2018},
	note = {Publisher: CIRP},
	keywords = {Assembly, Human-Robot Interaction(HRI), Hybrid assembly system, Modelling, Planning, Robot},
	pages = {76--90},
}

@article{maurtua_human-robot_2017,
	title = {Human-robot collaboration in industrial applications: {Safety}, interaction and trust},
	volume = {14},
	issn = {17298814},
	doi = {10.1177/1729881417716010},
	abstract = {Human-robot collaboration is a key factor for the development of factories of the future, a space in which humans and robots can work and carry out tasks together. Safety is one of the most critical aspects in this collaborative human-robot paradigm. This article describes the experiments done and results achieved by the authors in the context of the Four-ByThree project, aiming to measure the trust of workers on fenceless human-robot collaboration in industrial robotic applications as well as to gauge the acceptance of different interaction mechanisms between robots and human beings.},
	number = {4},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Maurtua, Iñaki and Ibarguren, Aitor and Kildal, Johan and Susperregi, Loreto and Sierra, Basilio},
	year = {2017},
	keywords = {Collaborative robots, Human Activity, Human-Robot Interaction(HRI), Manufacturing, Multimodal interaction, Natural communication, Robotics, Safe human-robot collaboration},
	pages = {1--10},
}

@article{peternel_selective_2019,
	title = {A selective muscle fatigue management approach to ergonomic human-robot co-manipulation},
	volume = {58},
	issn = {07365845},
	url = {https://doi.org/10.1016/j.rcim.2019.01.013},
	doi = {10.1016/j.rcim.2019.01.013},
	abstract = {In this paper, we propose a method for selective monitoring and management of human muscle fatigue in human-robot co-manipulation scenarios. The proposed approach uses a machine learning technique to learn the complex relationship between individual human muscle forces, arm configuration and arm endpoint force that are provided by a sophisticated offline musculoskeletal model. The estimated muscle forces are used in the fatigue model to estimate the individual muscle fatigue levels online. Two fatigue management protocols are proposed that enable the robot to handle and reduce the human fatigue by altering the configuration of task execution. The first protocol uses optimisation technique to find the optimal position for task execution, where the fatigue-related endurance time can be maximised. The second protocol divides the arm muscles into groups and then alters the direction of endpoint force so that the fatigued muscle group can relax and the relaxed muscle group becomes active. The proposed method has a potential to enable the robot to facilitate safer and more ergonomic working conditions for the human coworker. The main advantage of this approach is that it can operate online, and that all the measurements can be performed by the robot sensory system, which can significantly increase the applicability in real world scenarios. To validate the proposed method, we performed multiple experiments with two collaborative tasks (polishing and drilling) under different conditions.},
	number = {January},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Peternel, Luka and Fang, Cheng and Tsagarakis, Nikos and Ajoudani, Arash},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Machine learning, Motion, Muscle fatigue, Muscle force estimation, Physical human-robot collaboration},
	pages = {69--79},
}

@article{pellegrinelli_motion_2017,
	title = {Motion planning and scheduling for human and industrial-robot collaboration},
	volume = {66},
	issn = {17260604},
	url = {http://dx.doi.org/10.1016/j.cirp.2017.04.095},
	doi = {10.1016/j.cirp.2017.04.095},
	abstract = {Step-changes in safety technologies have opened robotic cells to human workers in real industrial scenarios. However, the lack of methodologies for a productive and effective motion planning and scheduling of human–robot cooperative (HRC) tasks is still limiting the spread of HRC systems. Standard methods fail due to the high-variability of the robot execution time, caused by the necessity to continuously modify the robot motion to grant human safety. In this context, the paper introduces an innovative integrated motion planning and scheduling methodology that (i) provides a set of robot trajectories for each task as well as an interval on the robot execution time for each trajectory and (ii) optimizes, at relevant time steps, a task plan, minimizing the cycle time through trajectory selection, task sequence and task allocation. The application of the approach to an industrial case is presented and discussed.},
	number = {1},
	journal = {CIRP Annals - Manufacturing Technology},
	author = {Pellegrinelli, Stefania and Orlandini, Andrea and Pedrocchi, Nicola and Umbrico, Alessandro and Tolio, Tullio},
	year = {2017},
	note = {Publisher: CIRP},
	keywords = {Assembly, Human-Robot Interaction(HRI), Motion, Planning, Planning \& scheduling, Robot},
	pages = {1--4},
}

@article{banziger_optimizing_2020,
	title = {Optimizing human–robot task allocation using a simulation tool based on standardized work descriptions},
	volume = {31},
	issn = {15728145},
	url = {https://doi.org/10.1007/s10845-018-1411-1},
	doi = {10.1007/s10845-018-1411-1},
	abstract = {Human–robot collaboration is enabled by the digitization of production and has become a key technology for the factory of the future. It combines the strengths of both the human worker and the assistant robot and allows the implementation of an varying degree of automation in workplaces in order to meet the increasing demand of flexibility of manufacturing systems. Intelligent planning and control algorithms are needed for the organization of the work in hybrid teams of humans and robots. This paper introduces an approach to use standardized work description for automated procedure generation of mobile assistant robots. A simulation tool is developed that implements the procedure model and is therefore capable of calculating different objective parameters like production time or ergonomics during a production cycle as a function of the human–robot task allocation. The simulation is validated with an existing workplace in an assembly line at the Volkswagen plant in Wolfsburg, Germany. Furthermore, a new method is presented to optimize the task allocation in human–robot teams for a given workplace, using the simulation as fitness function in a genetic algorithm. The advantage of this new approach is the possibility to evaluate different distributions of the tasks, while considering the dynamics of the interaction between the worker and the robot in their shared workplace. Using the presented approach for a given workplace, an optimized human–robot task allocation is found, in which the tasks are allocated in an intelligent and comprehensible way.},
	number = {7},
	journal = {Journal of Intelligent Manufacturing},
	author = {Bänziger, Timo and Kunz, Andreas and Wegener, Konrad},
	year = {2020},
	note = {Publisher: Springer US},
	keywords = {Human-Robot Interaction(HRI), Human–robot collaboration, Optimization, Planning, Robotics, Simulation, Task allocation},
	pages = {1635--1648},
}

@article{wang_symbiotic_2019,
	title = {Symbiotic human-robot collaborative assembly},
	volume = {68},
	issn = {17260604},
	url = {https://doi.org/10.1016/j.cirp.2019.05.002},
	doi = {10.1016/j.cirp.2019.05.002},
	abstract = {In human-robot collaborative assembly, robots are often required to dynamically change their pre-planned tasks to collaborate with human operators in a shared workspace. However, the robots used today are controlled by pre-generated rigid codes that cannot support effective human-robot collaboration. In response to this need, multi-modal yet symbiotic communication and control methods have been a focus in recent years. These methods include voice processing, gesture recognition, haptic interaction, and brainwave perception. Deep learning is used for classification, recognition and context awareness identification. Within this context, this keynote provides an overview of symbiotic human-robot collaborative assembly and highlights future research directions.},
	number = {2},
	journal = {CIRP Annals},
	author = {Wang, L. and Gao, R. and Váncza, J. and Krüger, J. and Wang, X. V. and Makris, S. and Chryssolouris, G.},
	year = {2019},
	note = {Publisher: CIRP},
	keywords = {Assembly, Human-Robot Interaction(HRI), Human-robot collaboration, Review Paper, Robot, Robotics},
	pages = {701--726},
}

@article{bansal_interaction-aware_2019,
	title = {Interaction-{Aware} {Planning} via {Nash} {Equilibria} for {Manipulation} in a {Shared} {Workspace}},
	author = {Bansal, Shray and Mukadam, Mustafa and Isbell, Charles L},
	year = {2019},
	keywords = {Motion, Planning, Robotics},
	pages = {1--2},
}

@article{barros_facechannel_2020,
	title = {The {FaceChannel}: {A} light-weight deep neural network for {Facial} {Expression} {Recognition}},
	doi = {10.1109/FG47880.2020.00070},
	abstract = {Current state-of-the-art models for automatic Facial Expression Recognition (FER) are based on very deep neural networks that are difficult to train. This makes it challenging to adapt these models to changing conditions, a requirement from FER models given the subjective nature of affect perception and understanding. In this paper, we address this problem by formalising the FaceChannel, a light-weight neural network that has much fewer parameters than common deep neural networks. We perform a series of experiments on different benchmark datasets to demonstrate how the FaceChannel achieves a comparable, if not better, performance, as compared to the current state-of-the-art in FER.},
	number = {June},
	journal = {arXiv},
	author = {Barros, Pablo and Churamani, Nikhil and Sciutti, Alessandra},
	year = {2020},
	note = {arXiv: 2004.08195},
	keywords = {Action Recognition, Deep, Human Activity},
}

@article{tsarouchi_decision_2016,
	title = {A {Decision} {Making} {Framework} for {Human} {Robot} {Collaborative} {Workplace} {Generation}},
	volume = {44},
	issn = {22128271},
	url = {http://dx.doi.org/10.1016/j.procir.2016.02.103},
	doi = {10.1016/j.procir.2016.02.103},
	abstract = {The Human Robot Collaborative (HRC) workplace design and the automatic task allocation can significantly decrease the time of a new set-up or a cell's reconfiguration. This paper proposes a method for an HRC workplace layout generation and the preliminary assignment of human and robot tasks. A decision making framework is proposed in order for the location of all components in the available layout space to be decided upon. The evaluation of the alternative HRC workplace layouts is based on multiple criteria. The system has been integrated with a user interface into a 3D simulation tool and tested on an automotive industry case study.},
	journal = {Procedia CIRP},
	author = {Tsarouchi, Panagiota and Spiliotopoulos, Jason and Michalos, George and Koukas, Spyros and Athanasatos, Athanasios and Makris, Sotiris and Chryssolouris, George},
	year = {2016},
	note = {Publisher: Elsevier B.V.},
	keywords = {Design, HRC, Human-Robot Interaction(HRI), Multiple Criteria, Planning, Robotics, Search Algorithm, Workplace generation},
	pages = {228--232},
}

@article{twardowski_motor_2019,
	title = {A {Motor} {Unit} {Based} {Human}-{Machine} {Interface} for the {Control} of {Protheses} and {Assitive} {Robotic} {Devices}},
	author = {Twardowski, Michael D and Roy, Serge H and Contessa, Paola and Kline, Joshua H},
	year = {2019},
	keywords = {Human-Robot Interaction(HRI), Motion, Robotics, human-machine interface, motor unit, prosthetic},
	pages = {1--3},
}

@article{bullock_neural_1988,
	title = {Neural {Dynamics} of {Planned} {Arm} {Movements}: {Emergent} {Invariants} and {Speed}-{Accuracy} {Properties} {During} {Trajectory} {Formation}},
	volume = {95},
	issn = {0033295X},
	doi = {10.1037/0033-295X.95.1.49},
	abstract = {A real-time neural network model, called the vector-integration-to-endpoint (VITE) model is developed and used to simulate quantitatively behavioral and neural data about planned and passive arm movements. Invariants of arm movements emerge through network interactions rather than through an explicitly precomputed trajectory. Motor planning occurs in the form of a target position command (TPC), which specifies where the arm intends to move, and an independently controlled GO command, which specifies the movement's overall speed. Automatic processes convert this information into an arm trajectory with invariant properties. These automatic processes include computation of a present position command (PPC) and a difference vector (DV). The DV is the difference between the PPC and the TPC at any time. The PPC is gradually updated by integrating the DV through time. The GO signal multiplies the DV before it is integrated by the PPC. The PPC generates an outflow movement command to its target muscle groups. Opponent interactions regulate the PPCs to agonist and antagonist muscle groups. This system generates synchronous movements across synergetic muscles by automatically compensating for the different total contractions that each muscle group must undergo. Quantitative simulations are provided of Woodworth's law, of the speed-accuracy trade-off known as Fitts's law, of isotonic arm-movement properties before and after deafferentation, of synchronous and compensatory "central-error-correction" properties of isometric contractions, of velocity amplification during target switching, of velocity profile invariance and asymmetry, of the changes in velocity profile asymmetry at higher movement speeds, of the automatic compensation for staggered onset times of synergetic muscles, of vector cell properties in precentral motor cortex, of the inverse relation between movement duration and peak velocity, and of peak acceleration as a function of movement amplitude and duration. It is shown that TPC, PPC, and DV computations are needed to actively modulate, or gate, the learning of associative maps between TPCs of different modalities, such as between the eye-head system and the hand-arm system. By using such an associative map, looking at an object can activate a TPC of the hand-arm system, as Piaget noted. Then a VITE circuit can translate this TPC into an invariant movement trajectory. An auxiliary circuit, called the Passive Update of Position (PUP) model is described for using inflow signals to update the PPC during passive arm movements owing to external forces. Other uses of outflow and inflow signals are also noted, such as for adaptive linearization of a nonlinear muscle plant, and sequential readout of TPCs during a serial plan, as in reaching and grasping. Comparisons are made with other models of motor control, such as the mass-spring and minimum-jerk models.},
	number = {1},
	journal = {Psychological Review},
	author = {Bullock, Daniel and Grossberg, Stephen},
	year = {1988},
	pmid = {3281179},
	keywords = {Modelling, Motion, Planning},
	pages = {49--90},
}

@misc{taylor_m_gambon_james_p_schmiedeler_characterizing_2019,
	title = {Characterizing {Intent} {Changes} in {Exoskeleton}-{Assisted} {Walking} {Through} {Onboard} {Sensors}},
	abstract = {Robotic exoskeletons are a promising technology for rehabilitation and locomotion following musculoskeletal injury, but their adoption outside the physical therapy clinic has been limited by relatively primitive methods for iden- tifying and incorporating the user’s gait intentions. Various intent detection approaches have been demonstrated using electromyography and electroencephalography signals. These technologies sense the human directly but introduce compli- cations for donning/doffing the device and in measurement consistency. By contrast, sensors onboard the exoskeleton avoid these complications but sense the human indirectly via the human-robot interface. This pilot study examines if onboard sensors alone may enable identification of user intent. Joint positions and commanded motor currents are compared prior to and after changes in the user’s intended gait speed. Pre- liminary experimental results confirm that these measures are significantly different following intent changes for both able- bodied and non-able-bodied users. The findings suggest that intent detection is possible with onboard sensors alone, but the intent signals depend on exoskeleton control settings, user ability, and temporal considerations.},
	author = {Taylor M. Gambon, James P. Schmiedeler, {and} Patrick M. Wensing},
	year = {2019},
	keywords = {Human Activity, Human Experiment, Prediction},
}

@article{tirupachuri_partner-aware_2019,
	title = {Partner-{Aware} {Control} {Framework} for {pHRI} {Leveraging} {Physical} {Interactions}},
	author = {Tirupachuri, Yeshasvi and Nava, Gabriele and Ferigo, Diego and Rapetti, Lorenzo and Nori, Francesco and Pucci, Daniele},
	year = {2019},
	keywords = {Activity Detection, Human Activity, Human-Robot Interaction(HRI)},
	pages = {1--2},
}

@article{hermus_features_2019,
	title = {Features of {Free} {Motion} {Persist} in {Constrained} {Actions}},
	author = {Hermus, James and Sternad, Dagmar and Hogan, Neville},
	year = {2019},
	keywords = {Action understanding, Human Activity, Motion},
}

@article{hetherington_towards_2019,
	title = {Towards {Social}-{Acceptability} of {Mobile} {Robots} through {Motion} {Communication} {Cues}},
	url = {http://hms2019icra.mit.edu/posters/},
	journal = {ICRA 2019 Workshop on Human Movement Science for Physical Human-Robot Collaboration},
	author = {Hetherington, Nicholas J and Williams, Katherine A and Croft, Elizabeth and Loos, H F Machiel Van Der},
	year = {2019},
	keywords = {Human Activity, Motion, Robotics},
	pages = {1--2},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {14764687},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	pmid = {26017442},
	keywords = {Cognition, Deep, Review Paper},
	pages = {436--444},
}

@article{nuckols_investigating_2019,
	title = {Investigating the role of muscle dynamics in individual response to soft exosuit assistance},
	author = {Nuckols, Richard W and Swaminathan, Krithika and Lee, Sangjun and Revi, Dheepak Arumukhom and Walsh, Conor J and Howe, Robert D},
	year = {2019},
	keywords = {Human Activity, Motion, Robotics, contraction dynamics of the, exoskeleton, exosuit, exosuit assistance, explain, gait, individual response to ankle, muscle, our aim in this, tendon, the efficiency, triceps surae can help, walking, whether muscle, work is to evaluate},
}

@article{hu_interact_2019,
	title = {Interact with me : active physical human robot interaction},
	author = {Hu, Yue and Benallegue, Mehdi and Venture, Gentiane and Yoshida, Eiichi},
	year = {2019},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Motion},
	pages = {3--4},
}

@article{nair_action_2020,
	title = {Action similarity judgment based on kinematic primitives},
	abstract = {Understanding which features humans rely on – in visually recognizing action similarity is a crucial step towards a clearer picture of human action perception from a learning and developmental perspective. In the present work, we investigate to which extent a computational model based on kinematics can determine action similarity and how its performance relates to human similarity judgments of the same actions. To this aim, twelve participants perform an action similarity task, and their performances are compared to that of a computational model solving the same task. The chosen model has its roots in developmental robotics and performs action classification based on learned kinematic primitives. The comparative experiment results show that both the model and human participants can reliably identify whether two actions are the same or not. However, the model produces more false hits and has a greater selection bias than human participants. A possible reason for this is the particular sensitivity of the model towards kinematic primitives of the presented actions. In a second experiment, human participants’ performance on an action identification task indicated that they relied solely on kinematic information rather than on action semantics. The results show that both the model and human performance are highly accurate in an action similarity task based on kinematic-level features, which can provide an essential basis for classifying human actions.},
	journal = {arXiv},
	author = {Nair, Vipul and Hemeren, Paul and Vignolo, Alessia and Noceti, Nicoletta and Nicora, Elena and Sciutti, Alessandra and Rea, Francesco and Billing, Erik and Odone, Francesca and Sandini, Giulio},
	year = {2020},
	note = {arXiv: 2008.13176},
	keywords = {Action primitives, Action similarity, Action understanding, Biological motion, Comparative study, Computational model, Human Activity, Primitives},
}

@article{hietanen_ar-based_2020,
	title = {{AR}-based interaction for human-robot collaborative manufacturing},
	volume = {63},
	issn = {07365845},
	url = {https://doi.org/10.1016/j.rcim.2019.101891},
	doi = {10.1016/j.rcim.2019.101891},
	abstract = {Industrial standards define safety requirements for Human-Robot Collaboration (HRC) in industrial manufacturing. The standards particularly require real-time monitoring and securing of the minimum protective distance between a robot and an operator. This paper proposes a depth-sensor based model for workspace monitoring and an interactive Augmented Reality (AR) User Interface (UI) for safe HRC. The AR UI is implemented on two different hardware: a projector-mirror setup and a wearable AR gear (HoloLens). The workspace model and UIs are evaluated in a realistic diesel engine assembly task. The AR-based interactive UIs provide 21–24\% and 57–64\% reduction in the task completion and robot idle time, respectively, as compared to a baseline without interaction and workspace sharing. However, user experience assessment reveal that HoloLens based AR is not yet suitable for industrial manufacturing while the projector-mirror setup shows clear improvements in safety and work ergonomics.},
	number = {November 2019},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Hietanen, Antti and Pieters, Roel and Lanz, Minna and Latokartano, Jyrki and Kämäräinen, Joni Kristian},
	year = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {AR, Assembly, Augmented reality, Human-Robot Interaction(HRI), Human-robot collaboration, Robotics, User studies},
	pages = {101891},
}

@article{Moosavian1998,
	title = {Multiple {Impedance} {Control} for object manipulation},
	volume = {1},
	doi = {10.1109/iros.1998.724662},
	abstract = {Impedance Control was formulated originally to impose a desired behavior on a single manipulator interacting with its environment. In this paper, a new algorithm called Multiple Impedance Control (MIC) is proposed for the cooperative manipulation of a common object. The general formulation for the MIC algorithm is developed and is shown that under the MIC law all cooperating manipulators, and the manipulated object exhibit the same designated impedance behavior. At the same time, the potentially large object inertia and other forces are taken into account. An estimation procedure for contact force determination is given which results in a good approximation, even during an impact. Using an example, the response of the MIC algorithm is compared to that of the Object Impedance Control (OIC). It is shown that in the presence of flexibility, the MIC algorithm results in an improved performance.},
	number = {October},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Moosavian, S. Ali A. and Papadopoulos, Evangelos},
	year = {1998},
	note = {ISBN: 0780344650},
	keywords = {Human Activity, Modelling, Object Pose},
	pages = {461--466},
}

@article{howard_intelligent_2000,
	title = {Intelligent learning for deformable object manipulation},
	volume = {9},
	issn = {09295593},
	doi = {10.1023/A:1008924218273},
	abstract = {The majority of manipulation systems are designed with the assumption that the objects being handled are rigid and do not deform when grasped. This paper addresses the problem of robotic grasping and manipulation of 3-D deformable objects, such as rubber balls or bags filled with sand. Specifically, we have developed a generalized learning algorithm for handling of 3-D deformable objects in which prior knowledge of object attributes is not required and thus it can be applied to a large class of object types. Our methodology relies on the implementation of two main tasks. Our first task is to calculate deformation characteristics for a non-rigid object represented by a physically-based model. Using nonlinear partial differential equations, we model the particle motion of the deformable object in order to calculate the deformation characteristics. For our second task, we must calculate the minimum force required to successfully lift the deformable object. This minimum lifting force can be learned using a technique called `iterative lifting'. Once the deformation characteristics and the associated lifting force term are determined, they are used to train a neural network for extracting the minimum force required for subsequent deformable object manipulation tasks. Our developed algorithm is validated with two sets of experiments. The first experimental results are derived from the implementation of the algorithm in a simulated environment. The second set involves a physical implementation of the technique whose outcome is compared with the simulation results to test the real world validity of the developed methodology.},
	number = {1},
	journal = {Autonomous Robots},
	author = {Howard, Ayanna M. and Bekey, George A.},
	year = {2000},
	note = {ISBN: 0780358066},
	keywords = {- deformable object manipulation, Modelling, Motion, Object Pose, ing, iterative lifting, learn-},
	pages = {51--58},
}

@article{DalleMura2019,
	title = {Designing assembly lines with humans and collaborative robots: {A} genetic approach},
	volume = {68},
	issn = {17260604},
	url = {https://doi.org/10.1016/j.cirp.2019.04.006},
	doi = {10.1016/j.cirp.2019.04.006},
	abstract = {Human-robot collaboration represents a significant evolutionary step in manufacturing. A crucial point is to establish a proper task assignment to combine robot productivity with human flexibility. In this regard, this paper proposes a genetic algorithm to approach the Assembly Line Balancing Problem (ALBP) in the case of human-robot collaborative work. The aim is the minimization of: i) the assembly line cost, evaluated according to the number of workers and equipment on the line, including collaborative robots, ii) the number of skilled workers on the line, iii) the energy load variance among workers, based on their energy expenditures and thus on their physical capabilities and on the level of collaboration with robots.},
	number = {1},
	journal = {CIRP Annals},
	author = {Dalle Mura, Michela and Dini, Gino},
	year = {2019},
	note = {Publisher: CIRP},
	keywords = {Assembly, Genetic algorithm, Human Activity, Human-Robot Interaction(HRI), Human–robot collaboration, Robotics},
	pages = {1--4},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {14691825},
	doi = {10.1017/S0140525X16001837},
	abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	number = {2017},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	pmid = {27881212},
	note = {arXiv: 1604.00289},
	keywords = {Action understanding, Automation, Review Paper},
}

@article{storli_play_2019,
	title = {Play, {Dreams}, and {Imitation}},
	volume = {9},
	issn = {17519004},
	url = {https://doi.org/10.1080/21594937.2019.1580338},
	abstract = {© 2015 John Wiley \& Sons Ltd Although research has established play behavior and playfulness as important to the well-being of children and animals, researchers have not typically considered the importance or implications of play in human adults. This is surprising given that play behavior is a topic of high relevance to social psychology. Definitional issues, a lack of a theoretical framework, and a dearth of standardized measures have posed obstacles in the advancement of this interesting research area, which is ripe for investigation. In this review, we summarize the extant literature, identify strengths and weaknesses of this literature, propose definitions to guide future work in this area, identify relevant theories that may be expanded to provide a framework for programmatic research on play in adulthood, and elucidate avenues for future research. Our aim is to encourage the development of this research area within social psychology.},
	number = {1},
	journal = {Social and Personality Psychology Compass},
	author = {Storli, Rune and Hansen Sandseter, Ellen Beate and Coates, Janine K. and Pimlott-Wilson, Helena and Van Vleet, Meredith and Feeney, Brooke C.},
	year = {2019},
	keywords = {ECEC institutions, Human Experiment, Imitation, Neurophysiological, alternative education, indoor and outdoor environment, involvement, non-play, outdoor learning, peer interaction, play, play-pedagogy, well-being},
	pages = {21--40},
}

@article{lemme_open-source_2015,
	title = {Open-source benchmarking for learned reaching motion generation in robotics},
	volume = {6},
	issn = {2081-4836},
	doi = {10.1515/pjbr-2015-0002},
	abstract = {This paper introduces a benchmark framework to evaluate the performance of reaching motion generation approaches that learn from demonstrated examples. The system implements ten different performance measures for typical generalization tasks in robotics using open source MATLAB software. Systematic comparisons are based on a default training data set of human motions, which specify the respective ground truth. In technical terms, an evaluated motion generation method needs to compute velocities, given a state provided by the simulation system. This however is agnostic to how this is done by the method or how the methods learns from the provided demonstrations. The framework focuses on robustness, which is tested statistically by sampling from a set of perturbation scenarios. These perturbations interfere with motion generation and challenge its generalization ability. The benchmark thus helps to identify the strengths and weaknesses of competing approaches, while allowing the user the opportunity to configure the weightings between different measures.},
	number = {1},
	journal = {Paladyn, Journal of Behavioral Robotics},
	author = {Lemme, A. and Meirovitch, Y. and Khansari-Zadeh, M. and Flash, T. and Billard, A. and Steil, J. J.},
	year = {2015},
	keywords = {Human, Motion, Robotics, benchmarking, dynamical systems, human-like motions, itive, learning from demonstrations, movement prim-, programming by demonstrations, reaching motions, standardized comparisons},
	pages = {30--41},
}

@article{gateau_considering_2017,
	title = {Considering {Human} ’ s {Non}-{Deterministic} {Behavior} and his {Availability} {State} {When} {Designing} a {Collaborative} {Human}-{Robots} {System}},
	abstract = {The objective of this study is to design a human- robots system that takes into account the non-deterministic nature of the human operator’s behavior. Such a system is implemented in a proof of concept scenario relying on a (MO)MDP decision framework that takes advantage of an eye-tracker device to estimate the cognitive availability of the human operator, and, some human operator’s inputs to deduce where he is focusing his attention. An experiment was conducted with ten participants interacting with a team of autonomous vehicles in a Search \& Rescue scenario. Our results demonstrate the advantages of considering the cognitive availability of a human operator in such a complex context and also the interest of using such a decisional framework that can formally integrate the non-deterministic outcomes which model the human behavior.},
	journal = {Thibault Gateau, Caroline Ponzoni Carvalho Chanel, Lee Mai-Huy, Frédéric Dehais},
	author = {Gateau, Thibault and Ponzoni, Caroline and Chanel, Carvalho and Mai-huy, Lee and Gateau, Thibault and Ponzoni, Caroline and Chanel, Carvalho and Mai-huy, Lee and Hu-, Frédéric Dehais Considering},
	year = {2017},
	keywords = {Human Experiment, Markov, Robotics},
}

@article{olivier_precision_2007,
	title = {Precision grasping in humans: from motor control to cognition},
	volume = {17},
	issn = {09594388},
	doi = {10.1016/j.conb.2008.01.008},
	abstract = {In the past decade, functional neuroimaging has proved extremely useful in mapping the human motor circuits involved in skilled hand movements. However, one major drawback of this approach is the impossibility to determine the exact contribution of each individual cortical area to precision grasping. Because transcranial magnetic stimulation (TMS) makes it possible to induce a transient 'virtual' lesion of discrete brain regions in healthy subjects, it has been extensively used to provide direct insight into the causal role of a given area in human motor behaviour. Recent TMS studies have allowed us to determine the specific contribution, as well as the timing and the hemispheric lateralisation, of distinct parietal and frontal areas to the control of both the kinematics and dynamics of precision grasping. Moreover, recent researches have shown that the same cortical network may contribute to language and number processing, supporting the existence of tight interactions between processes involved in cognition and actions. The aim of this paper is to offer a concise overview of recent studies that have investigated the neural correlates of precision grasping and the possible contribution of the motor system to higher cognitive functions such as language and number processing. © 2008 Elsevier Ltd. All rights reserved.},
	number = {6},
	journal = {Current Opinion in Neurobiology},
	author = {Olivier, Etienne and Davare, Marco and Andres, Michael and Fadiga, Luciano},
	year = {2007},
	pmid = {18337084},
	keywords = {Grasp, Neuroscience, Review Paper},
	pages = {644--648},
}

@article{mayer_walking_2012,
	title = {Walking with coffee: {Why} does it spill?},
	volume = {85},
	issn = {15393755},
	doi = {10.1103/PhysRevE.85.046117},
	abstract = {In our busy lives, almost all of us have to walk with a cup of coffee. While often we spill the drink, this familiar phenomenon has never been explored systematically. Here we report on the results of an experimental study of the conditions under which coffee spills for various walking speeds and initial liquid levels in the cup. These observations are analyzed from the dynamical systems and fluid mechanics viewpoints as well as with the help of a model developed here. Particularities of the common cup sizes, the coffee properties, and the biomechanics of walking proved to be responsible for the spilling phenomenon. The studied problem represents an example of the interplay between the complex motion of a cup, due to the biomechanics of a walking individual, and the low-viscosity-liquid dynamics in it. © 2012 American Physical Society.},
	number = {4},
	journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	author = {Mayer, H. C. and Krechetnikov, R.},
	year = {2012},
	keywords = {Human Activity, Modelling, Motion},
	pages = {1--7},
}

@article{mohammad_khansari-zadeh_dynamical_2012,
	title = {A dynamical system approach to realtime obstacle avoidance},
	volume = {32},
	issn = {09295593},
	doi = {10.1007/s10514-012-9287-y},
	abstract = {This paper presents a novel approach to real-time obstacle avoidance based on Dynamical Systems (DS) that ensures impenetrability of multiple convex shaped objects. The proposed method can be applied to perform obstacle avoidance in Cartesian and Joint spaces and using both autonomous and non-autonomous DS-based controllers. Obstacle avoidance proceeds by modulating the original dynamics of the controller. The modulation is parameterizable and allows to determine a safety margin and to increase the robot's reactiveness in the face of uncertainty in the localization of the obstacle. The method is validated in simulation on different types of DS including locally and globally asymptotically stable DS, autonomous and non-autonomous DS, limit cycles, and unstable DS. Further, we verify it in several robot experiments on the 7 degrees of freedom Barrett WAM arm. © 2011 Springer-Verlag.},
	number = {4},
	journal = {Autonomous Robots},
	author = {Mohammad Khansari-Zadeh, Seyed and Billard, Aude},
	year = {2012},
	keywords = {DS, Harmonic potential function, Modelling, Motion, Nonlinear dynamical system, Realtime obstacle avoidance, Robot manipulator},
	pages = {433--454},
}

@article{mohammad_khansari-zadeh_learning_2014,
	title = {Learning control {Lyapunov} function to ensure stability of dynamical system-based robot reaching motions},
	volume = {62},
	issn = {09218890},
	url = {http://dx.doi.org/10.1016/j.robot.2014.03.001},
	doi = {10.1016/j.robot.2014.03.001},
	abstract = {We consider an imitation learning approach to model robot point-to-point (also known as discrete or reaching) movements with a set of autonomous Dynamical Systems (DS). Each DS model codes a behavior (such as reaching for a cup and swinging a golf club) at the kinematic level. An estimate of these DS models are usually obtained from a set of demonstrations of the task. When modeling robot discrete motions with DS, ensuring stability of the learned DS is a key requirement to provide a useful policy. In this paper we propose an imitation learning approach that exploits the power of Control Lyapunov Function (CLF) control scheme to ensure global asymptotic stability of nonlinear DS. Given a set of demonstrations of a task, our approach proceeds in three steps: (1) Learning a valid Lyapunov function from the demonstrations by solving a constrained optimization problem, (2) Using one of the-state-of-the-art regression techniques to model an (unstable) estimate of the motion from the demonstrations, and (3) Using (1) to ensure stability of (2) during the task execution via solving a constrained convex optimization problem. The proposed approach allows learning a larger set of robot motions compared to existing methods that are based on quadratic Lyapunov functions. Additionally, by using the CLF formalism, the problem of ensuring stability of DS motions becomes independent from the choice of regression method. Hence it allows the user to adopt the most appropriate technique based on the requirements of the task at hand without compromising stability. We evaluate our approach both in simulation and on the 7 degrees of freedom Barrett WAM arm. © 2014 Elsevier B.V. All rights reserved.},
	number = {6},
	journal = {Robotics and Autonomous Systems},
	author = {Mohammad Khansari-Zadeh, S. and Billard, Aude},
	year = {2014},
	note = {Publisher: Elsevier B.V.},
	keywords = {Control Lyapunov function, DS, Imitation learning, Modelling, Motion, Movement primitives, Nonlinear dynamical systems, Robot point-to-point movements, Stability analysis},
	pages = {752--765},
}

@article{kronander_incremental_2015,
	title = {Incremental motion learning with locally modulated dynamical systems},
	volume = {70},
	issn = {09218890},
	url = {http://dx.doi.org/10.1016/j.robot.2015.03.010},
	doi = {10.1016/j.robot.2015.03.010},
	abstract = {Dynamical Systems (DS) for robot motion modeling are a promising approach for efficient robot learning and control. Our focus in this paper is on autonomous dynamical systems, which represent a motion plan without dependency on time. We develop a method that allows to locally reshape an existing, stable nonlinear autonomous DS while preserving important stability properties of the original system. Our system is based on local transformations of the dynamics. We propose an incremental learning algorithm based on Gaussian Processes for learning to reshape dynamical systems using this representation. The approach is validated in a 2d task of learning handwriting motions, a periodic polishing motion and in a manipulation task with the 7 degrees of freedom Barrett WAM manipulator.},
	journal = {Robotics and Autonomous Systems},
	author = {Kronander, K. and Khansari, M. and Billard, A.},
	year = {2015},
	note = {Publisher: Elsevier B.V.},
	keywords = {DS, Dynamical systems, Modelling, Motion, Motion modeling, Robotics},
	pages = {52--62},
}

@article{villani_survey_2018,
	title = {Survey on human–robot collaboration in industrial settings: {Safety}, intuitive interfaces and applications},
	volume = {55},
	issn = {09574158},
	url = {https://doi.org/10.1016/j.mechatronics.2018.02.009},
	doi = {10.1016/j.mechatronics.2018.02.009},
	abstract = {Easy-to-use collaborative robotics solutions, where human workers and robots share their skills, are entering the market, thus becoming the new frontier in industrial robotics. They allow to combine the advantages of robots, which enjoy high levels of accuracy, speed and repeatability, with the flexibility and cognitive skills of human workers. However, to achieve an efficient human–robot collaboration, several challenges need to be tackled. First, a safe interaction must be guaranteed to prevent harming humans having a direct contact with the moving robot. Additionally, to take full advantage of human skills, it is important that intuitive user interfaces are properly designed, so that human operators can easily program and interact with the robot. In this survey paper, an extensive review on human–robot collaboration in industrial environment is provided, with specific focus on issues related to physical and cognitive interaction. The commercially available solutions are also presented and the main industrial applications where collaborative robotic is advantageous are discussed, highlighting how collaborative solutions are intended to improve the efficiency of the system and which the open issue are.},
	number = {February},
	journal = {Mechatronics},
	author = {Villani, Valeria and Pini, Fabio and Leali, Francesco and Secchi, Cristian},
	year = {2018},
	note = {Publisher: Elsevier},
	keywords = {Collaborative robots, Human-Robot Interaction(HRI), Human–robot collaboration, Industrial applications, Intuitive robot programming, Motion, Review Paper, Safety, User interfaces},
	pages = {248--266},
}

@article{mah_generalization_2003,
	title = {Generalization of object manipulation skills learned without limb motion},
	volume = {23},
	issn = {02706474},
	doi = {10.1523/jneurosci.23-12-04821.2003},
	abstract = {Recent work suggests that human subjects may learn mappings between object motion and exerted torque during manipulation of freely pivoting or unstable objects. In the present work, we studied an object manipulation task involving no arm movement to determine how subjects internally represent the force-motion relationship of an object during a skilled manipulation task. Human subjects learned to balance a simulated inverted pendulum. The simulation was controlled by pressing on a fixed force sensor, and applied forces resulted in motion of the simulated pendulum on a computer screen according to its equation of motion. Each subject initially learned the task in one arm posture and was tested 1 d later in a new arm posture. In one test condition, the effects of arm torque were matched to the original task, and in the other test condition, the simulation was unchanged. The pattern of skill transfer to different arm postures suggested that subjects had learned joint torque responses rather than a general model of the object interface forces. A second experiment showed that the advantage of training with matched arm torques was object specific, because torque-matched training on a tracking task involving similar forces was not a substitute for training in the balancing task.},
	number = {12},
	journal = {Journal of Neuroscience},
	author = {Mah, Christopher D. and Mussa-Ivaldi, Ferdinando A.},
	year = {2003},
	pmid = {12832503},
	keywords = {Balancing, Human, Human Activity, Internal model, Isometric force, Modelling, Motion, Motor skills, Object manipulation},
	pages = {4821--4825},
}

@article{reed_replicating_2007,
	title = {Replicating {Human}-{Human} {Physical} {Interaction}},
	abstract = {Machines might physically interact with humans more smoothly if we better understood the subtlety of human- human physical interaction. We recently reported that two people working cooperatively on a physical task will quickly negotiate an emergent strategy: typically subjects formed a temporal specialization such that one member commands the early parts of motion and the other the late parts [1]. In our current study, we replaced one of the humans with a robot programmed to perform one of the typical human specialized roles. We expected the remaining human to adopt the complementary specialized role. Subjects did believe that they were interacting with another human but did not adopt a specialized behavior as subjects would when physically working with another human; our negative result suggests a very subtle negotiation takes place in human-human physical interaction.},
	number = {April},
	journal = {IEEE International Conference on Robotics and Automation},
	author = {Reed, Kyle B and Patton, James and Peshkin, Michael},
	year = {2007},
	note = {ISBN: 1424406021},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Imitation},
	pages = {10--14},
}

@article{georgiou_different_2007,
	title = {Different action patterns for cooperative and competitive behaviour},
	volume = {102},
	issn = {00100277},
	doi = {10.1016/j.cognition.2006.01.008},
	abstract = {The aim of the present study is to elucidate the influence of context on the kinematics of the reach-to-grasp movement. In particular, we consider two basic modes of social cognition, namely cooperation and competition. In two experiments kinematics of the very same action - reaching-to-grasp a wooden block - were analyzed in two different contexts provided by a cooperative task and competitive task. For the 'cooperation' tasks two participants were required to reach and grasp their respective objects and to cooperate to join the two objects in specific configurations in the middle of the working surface. For the 'competition' tasks, the two participants had to compete to place their own object first in the middle of the working surface. Results revealed specific kinematic patterns for cooperation and competition which were distinct from similar actions performed by each participant in isolation. Further, during the cooperation tasks, a high level of correlation between key kinematical parameters of the two participants was found. In accordance with evidence from neuroimaging, developmental and social psychology our results suggest the existence of motor patterns which reflect the intention to act in a social context. © 2006 Elsevier B.V. All rights reserved.},
	number = {3},
	journal = {Cognition},
	author = {Georgiou, Ioanna and Becchio, Cristina and Glover, Scott and Castiello, Umberto},
	year = {2007},
	keywords = {Action Recognition, Competition, Cooperation, Humans, Kinematics, Motion, Neurophysiological, Reach-to-grasp, Social cognition},
	pages = {415--433},
}

@article{maeda_phase_2017,
	title = {Phase estimation for fast action recognition and trajectory generation in human–robot collaboration},
	volume = {36},
	issn = {17413176},
	doi = {10.1177/0278364917693927},
	abstract = {This paper proposes a method to achieve fast and fluid human–robot interaction by estimating the progress of the movement of the human. The method allows the progress, also referred to as the phase of the movement, to be estimated even when observations of the human are partial and occluded; a problem typically found when using motion capture systems in cluttered environments. By leveraging on the framework of Interaction Probabilistic Movement Primitives, phase estimation makes it possible to classify the human action, and to generate a corresponding robot trajectory before the human finishes his/her movement. The method is therefore suited for semi-autonomous robots acting as assistants and coworkers. Since observations may be sparse, our method is based on computing the probability of different phase candidates to find the phase that best aligns the Interaction Probabilistic Movement Primitives with the current observations. The method is fundamentally different from approaches based on Dynamic Time Warping that must rely on a consistent stream of measurements at runtime. The resulting framework can achieve phase estimation, action recognition and robot trajectory coordination using a single probabilistic representation. We evaluated the method using a seven-degree-of-freedom lightweight robot arm equipped with a five-finger hand in single and multi-task collaborative experiments. We compare the accuracy achieved by phase estimation with our previous method based on dynamic time warping.},
	number = {13-14},
	journal = {International Journal of Robotics Research},
	author = {Maeda, Guilherme and Ewerton, Marco and Neumann, Gerhard and Lioutikov, Rudolf and Peters, Jan},
	year = {2017},
	keywords = {Action Recognition, Human-Robot Interaction(HRI), Motion, Physical human-robot interaction, action recognition, human-robot collaboration, imitation learning, learning and adaptive systems, probabilistic modelling, trajectory generation},
	pages = {1579--1594},
}

@article{byner_dynamic_2019,
	title = {Dynamic speed and separation monitoring for collaborative robot applications – {Concepts} and performance},
	volume = {58},
	issn = {07365845},
	url = {https://doi.org/10.1016/j.rcim.2018.11.002},
	doi = {10.1016/j.rcim.2018.11.002},
	abstract = {Speed and separation monitoring (SSM) allows safeguarding the operator in collaborative robot applications by maintaining a certain minimum separation distance during operation. A continuous adaptation of the robot velocity in response to relative operator and robot motion can be employed to improve the efficiency of SSM-type applications. The present paper presents two approaches for obtaining a robot velocity limit for this adaptation, considering the separation distance as well as the direction of robot motion. Using a collaborative machine tending task as an example, the impact of these approaches on application productivity was assessed in physical trials and compared to that of conventional safeguarding methods, i.e. zone-based supervision and safeguarding by physical barriers. The trials confirmed that the continuous speed adaptation has a notable productivity benefit over the state of industrial practice. Major factors that influence the particular benefit, such as the frequency and timing of operator presence near the robot, have been identified and investigated. Before these concepts can be applied in industry, machinery safety requirements must be satisfied; at present, this depends particularly on the availability of sufficiently reliable information on the human position, which is not provided by safety sensors today.},
	number = {May 2018},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Byner, Christoph and Matthias, Björn and Ding, Hao},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Human-Robot Interaction(HRI), Human-robot collaboration (HRC), Motion, Robotics, Speed and separation monitoring (SSM)},
	pages = {239--252},
}

@article{jain_recursive_2018,
	title = {Recursive {Bayesian} {Human} {Intent} {Recognition} in {Shared}-{Control} {Robotics}},
	issn = {21530866},
	doi = {10.1109/IROS.2018.8593766},
	abstract = {Effective human-robot collaboration in shared control requires reasoning about the intentions of the human user. In this work, we present a mathematical formulation for human intent recognition during assistive teleoperation under shared autonomy. Our recursive Bayesian filtering approach models and fuses multiple non-verbal observations to probabilistically reason about the intended goal of the user. In addition to contextual observations, we model and incorporate the human agent's behavior as goal-directed actions with adjustable rationality to inform the underlying intent. We examine human inference on robot motion and furthermore validate our approach with a human subjects study that evaluates autonomy intent inference performance under a variety of goal scenarios and tasks, by novice subjects. Results show that our approach outperforms existing solutions and demonstrates that the probabilistic fusion of multiple observations improves intent inference and performance for shared-control operation.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Jain, Siddarth and Argall, Brenna},
	year = {2018},
	note = {ISBN: 9781538680940},
	keywords = {Action Recognition, Observation, Robotics},
	pages = {3905--3912},
}

@article{lamb_pass_2017,
	title = {To pass or not to pass: {Modeling} the movement and affordance dynamics of a pick and place task},
	volume = {8},
	issn = {16641078},
	doi = {10.3389/fpsyg.2017.01061},
	abstract = {Humans commonly engage in tasks that require or are made more efficient by coordinating with other humans. In this paper we introduce a task dynamics approach for modeling multi-agent interaction and decision making in a pick and place task where an agent must move an object from one location to another and decide whether to act alone or with a partner. Our aims were to identify and model (1) the affordance related dynamics that define an actor's choice to move an object alone or to pass it to their co-actor and (2) the trajectory dynamics of an actor's hand movements when moving to grasp, relocate, or pass the object. Using a virtual reality pick and place task, we demonstrate that both the decision to pass or not pass an object and the movement trajectories of the participants can be characterized in terms of a behavioral dynamics model. Simulations suggest that the proposed behavioral dynamics model exhibits features observed in human participants including hysteresis in decision making, non-straight line trajectories, and non-constant velocity profiles. The proposed model highlights how the same low-dimensional behavioral dynamics can operate to constrain multiple (and often nested) levels of human activity and suggests that knowledge of what, when, where and how to move or act during pick and place behavior may be defined by these low dimensional task dynamics and, thus, can emerge spontaneously and in real-time with little a priori planning.},
	number = {JUN},
	journal = {Frontiers in Psychology},
	author = {Lamb, Maurice and Kallen, Rachel W. and Harrison, Steven J. and Di Bernardo, Mario and Minai, Ali and Richardson, Michael J.},
	year = {2017},
	keywords = {Affordance dynamics, Behavioral dynamics, Dynamical systems theory, Handovers, Joint-action, Motion, Neurophysiological, Pick and place},
	pages = {1--23},
}

@article{gabler_game-theoretic_2017,
	title = {A game-theoretic approach for adaptive action selection in close proximity human-robot-collaboration},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989336},
	abstract = {With the integration of Human-Robot Collaboration (HRC) in industrial assembly scenarios, robot systems face numerous challenges. In contrast to classic robot systems which follow a pre-programmed and fixed sequence of actions, an interaction scenario with humans in the loop requires mutual adaptation. In this paper a framework based on game theory is presented that allows robots to choose appropriate actions with respect to the action of human coworkers when collaborating in close proximity. The proposed framework models HRC scenarios as iterative games and selects action-strategies for the Human-Robot Team (HRT) by finding the Nash-Equilibria (NEs) of these games. In contrast to most common approaches, our proposed HRC-game treats the decision-making behavior equally for all agents involved. Therefore, the concept of game theory is applied to evaluate the mutual interference of all actions on the HRT to obtain pareto-optimal NEs, i.e. team-optimal action-allocations. The general framework of the proposed HRC-game is realized on an interactive pick-and-place scenario in close proximity. This exemplary HRC-game is tested in a human subject experiment of a KUKA LWR 4+ robot and a human coworker assembling toy-bricks in close proximity. The experimental measurements and statistically significant improvements in the subjective feedback hold as a proof-of-concept of the proposed HRC-game model.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Gabler, Volker and Stahl, Tim and Huber, Gerold and Oguz, Ozgur and Wollherr, Dirk},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509046331},
	keywords = {Action understanding, Human Activity, Human-Robot Interaction(HRI)},
	pages = {2897--2903},
}

@article{tsarouchi_humanrobot_2017,
	title = {On a human–robot workplace design and task allocation system},
	volume = {30},
	issn = {13623052},
	url = {https://doi.org/10.1080/0951192X.2017.1307524},
	doi = {10.1080/0951192X.2017.1307524},
	abstract = {This paper proposes a method for human–robot (HR) task planning, considering at the same time, the design of the workplace. A model for the representation of humans and robots as a team of active resources is proposed, while equipment such as working tables and fixtures are considered passive resources. The HR workload is structured in a three-level model. A multi-criteria decision-making framework is used for the formulation of alternative layouts and task allocations. Both analytical models and simulation are used for the estimation of the criteria values, allowing for the evaluation of the different alternatives. A software prototype has been implemented and tested in white goods and in automotive industry cases, demonstrating that the tool can identify good quality solutions in a short time frame.},
	number = {12},
	journal = {International Journal of Computer Integrated Manufacturing},
	author = {Tsarouchi, Panagiota and Michalos, George and Makris, Sotiris and Athanasatos, Thanasis and Dimoulas, Konstantinos and Chryssolouris, George},
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	keywords = {Cell scheduling, Human Experiment, Human-Robot Interaction(HRI), Robotics, ergonomics, hybrid assembly system, multi-criteria decision-making, simulation},
	pages = {1272--1279},
}

@article{gombolay_computational_2017,
	title = {Computational design of mixed-initiative human–robot teaming that considers human factors: situational awareness, workload, and workflow preferences},
	issn = {17413176},
	doi = {10.1177/0278364916688255},
	abstract = {Advancements in robotic technology are making it increasingly possible to integrate robots into the human workspace in order to improve productivity and decrease worker strain resulting from the performance of repetitive, arduous physical tasks. While new computational methods have significantly enhanced the ability of people and robots to work flexibly together, there has been little study of the ways in which human factors influence the design of these computational techniques. In particular, collaboration with robots presents unique challenges related to the preservation of human situational awareness and the optimization of workload allocation for human teammates while respecting their workflow preferences. We conducted a series of human subject experiments to investigate these human factors, and provide design guidelines for the development of intelligent collaborative robots based on our results.},
	journal = {International Journal of Robotics Research},
	author = {Gombolay, Matthew and Bair, Anna and Huang, Cindy and Shah, Julie},
	year = {2017},
	keywords = {Human Experiment, Human-Robot Interaction(HRI), Human–robot teaming, Robotics, human–robot interaction, planning and scheduling, preference scheduling, situational awareness, workload},
}

@article{liu_gesture_2018,
	title = {Gesture recognition for human-robot collaboration: {A} review},
	volume = {68},
	issn = {18728219},
	url = {https://doi.org/10.1016/j.ergon.2017.02.004},
	doi = {10.1016/j.ergon.2017.02.004},
	abstract = {Recently, the concept of human-robot collaboration has raised many research interests. Instead of robots replacing human workers in workplaces, human-robot collaboration allows human workers and robots working together in a shared manufacturing environment. Human-robot collaboration can release human workers from heavy tasks with assistive robots if effective communication channels between humans and robots are established. Although the communication channels between human workers and robots are still limited, gesture recognition has been effectively applied as the interface between humans and computers for long time. Covering some of the most important technologies and algorithms of gesture recognition, this paper is intended to provide an overview of the gesture recognition research and explore the possibility to apply gesture recognition in human-robot collaborative manufacturing. In this paper, an overall model of gesture recognition for human-robot collaboration is also proposed. There are four essential technical components in the model of gesture recognition for human-robot collaboration: sensor technologies, gesture identification, gesture tracking and gesture classification. Reviewed approaches are classified according to the four essential technical components. Statistical analysis is also presented after technical analysis. Towards the end of this paper, future research trends are outlined.},
	journal = {International Journal of Industrial Ergonomics},
	author = {Liu, Hongyi and Wang, Lihui},
	year = {2018},
	note = {Publisher: Elsevier B.V},
	keywords = {Gesture, Gesture recognition, Human-Robot Interaction(HRI), Human-robot collaboration, Motion, Review Paper},
	pages = {355--367},
}

@article{nikolaidis_human-robot_2017,
	title = {Human-{Robot} {Mutual} {Adaptation} in {Shared} {Autonomy}},
	volume = {Part F1271},
	issn = {21672148},
	doi = {10.1145/2909824.3020252},
	abstract = {Shared autonomy integrates user input with robot autonomy in order to control a robot and help the user to complete a task. Our work aims to improve the performance of such a human-robot team: the robot tries to guide the human towards an effective strategy, sometimes against the human's own preference, while still retaining his trust. We achieve this through a principled human-robot mutual adaptation formalism. We integrate a bounded-memory adaptation model of the human into a partially observable stochastic decision model, which enables the robot to adapt to an adaptable human. When the human is adaptable, the robot guides the human towards a good strategy, maybe unknown to the human in advance. When the human is stubborn and not adaptable, the robot complies with the human's preference in order to retain their trust. In the shared autonomy setting, unlike many other common human-robot collaboration settings, only the robot actions can change the physical state of the world, and the human and robot goals are not fully observable. We address these challenges and show in a human subject experiment that the proposed mutual adaptation formalism improves human-robot team performance, while retaining a high level of user trust in the robot, compared to the common approach of having the robot strictly following participants' preference.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Nikolaidis, Stefanos and Zhu, Yu Xiang and Hsu, David and Srinivasa, Siddhartha},
	year = {2017},
	note = {arXiv: 1701.07851
ISBN: 9781450343367},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Markov, human-robot mutual adaptation, planning under uncertainty, shared autonomy},
	pages = {294--302},
}

@article{mazzola_interacting_2020,
	title = {Interacting with a social robot affects visual perception of space},
	issn = {21672148},
	doi = {10.1145/3319502.3374819},
	abstract = {Human partners are very effective at coordinating in space and time. Such ability is particular remarkable considering that visual perception of space is a complex inferential process, which is affected by individual prior experience (e.g. the history of previous stimuli). As a result, two partners might perceive differently the same stimulus. Yet, they find a way to align their perception, as demonstrated by the high degree of coordination observed in sports or even in everyday gestures as shaking hands. Robots would need a similar ability to align with their partner's perception. However, to date there is no knowledge of how the inferential mechanism supporting visual perception operates during social interaction. In the current work, we use a humanoid robot to address this question. We replicate a standard protocol for the quantification of perceptual inference in a HRI setting. Participants estimated the length of a set of segments presented by the humanoid robot iCub. The robot behaved in one condition as a mechanical arm driven by a computer and in another condition as an interactive, social partner. Even if the stimuli presented were the same in the two conditions, length perception was different when the robot was judged as an interactive agent rather than a mechanical tool. When playing with the social robot, participants relied significantly less on stimulus history. This result suggests that the brain changes optimization strategies during interaction and lay the foundations to design humanaware robot visual perception.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Mazzola, Carlo and Aroyo, Alexander Mois and Rea, Francesco and Sciutti, Alessandra},
	year = {2020},
	note = {ISBN: 9781450367462},
	keywords = {Alignment, Bayesian models, Central tendency effect, Eyes, Gaze, Humanoid, Interaction, Robotics, Shared perception, Space perception, iCub},
	pages = {549--557},
}

@article{hilt_motor_2020,
	title = {Motor {Recruitment} during {Action} {Observation}: {Effect} of {Interindividual} {Differences} in {Action} {Strategy}},
	volume = {30},
	issn = {14602199},
	doi = {10.1093/cercor/bhaa006},
	abstract = {Visual processing of other's actions is supported by sensorimotor brain activations. Access to sensorimotor representations may, in principle, provide the top-down signal required to bias search and selection of critical visual features. For this to happen, it is necessary that a stable one-to-one mapping exists between observed kinematics and underlying motor commands. However, due to the inherent redundancy of the human musculoskeletal system, this is hardly the case for multijoint actions where everyone has his own moving style (individual motor signature-IMS). Here, we investigated the influence of subject's IMS on subjects' motor excitability during the observation of an actor achieving the same goal by adopting two different IMSs. Despite a clear dissociation in kinematic and electromyographic patterns between the two actions, we found no group-level modulation of corticospinal excitability (CSE) in observers. Rather, we found a negative relationship between CSE and actor-observer IMS distance, already at the single-subject level. Thus, sensorimotor activity during action observation does not slavishly replicate the motor plan implemented by the actor, but rather reflects the distance between what is canonical according to one's own motor template and the observed movements performed by other individuals.},
	number = {7},
	journal = {Cerebral cortex (New York, N.Y. : 1991)},
	author = {Hilt, P. M. and Cardellicchio, P. and Dolfini, E. and Pozzo, T. and Fadiga, L. and D'Ausilio, A.},
	year = {2020},
	pmid = {32043124},
	keywords = {Neurophysiological, Neuroscience, Observation, action observation, individual motor signatures, multijoint actions, transcranial magnetic stimulation, variability},
	pages = {3910--3920},
}

@article{sciutti_understanding_2014,
	title = {Understanding object weight from human and humanoid lifting actions},
	volume = {6},
	issn = {19430604},
	doi = {10.1109/TAMD.2014.2312399},
	abstract = {Humans are very good at interacting with each other. This natural ability depends, among other factors, on an implicit communication mediated by motion observation. By simple action observation we can easily infer not only the goal of an agent, but often also some "hidden" properties of the object he is manipulating, as its weight or its temperature. This implicit understanding is developed early in childhood and is supposedly based on a common motor repertoire between the cooperators. In this paper, we have investigated whether and under which conditions it is possible for a humanoid robot to foster the same kind of automatic communication, focusing on the ability to provide cues about object weight with action execution. We have evaluated on which action properties weight estimation is based in humans and we have accordingly designed a set of simple robotic lifting behaviors. Our results show that subjects can reach a performance in weight recognition from robot observation comparable to that obtained during human observation, with no need of training. These findings suggest that it is possible to design robot behaviors that are implicitly understandable by nonexpert partners and that this approach could be a viable path to obtain more natural human-robot collaborations. © 2014 IEEE.},
	number = {2},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Sciutti, Alessandra and Patanè, Laura and Nori, Francesco and Sandini, Giulio},
	year = {2014},
	keywords = {Action understanding, Human-Robot Interaction(HRI), Motion, Objects Weights, human robot interaction, motion kinematics, object perception},
	pages = {80--92},
}

@article{palinko_communicative_2015,
	title = {Communicative lifting actions in human-humanoid interaction},
	volume = {2015-Febru},
	issn = {21640580},
	doi = {10.1109/HUMANOIDS.2014.7041508},
	abstract = {Passing an object to someone else is one of the simplest collaborative actions. However, it entails a high degree of coordination between the two partners. The efficiency of the result relies heavily on the non-verbal communication associated to the passer's motion. The kinematic properties of the movement convey to the receiver implicit information about when, where and what is going to be passed. In this paper we focus on the 'what', by proposing a simple architecture which allows a humanoid robot to autonomously plan lifting movements which implicitly inform the human partner of the weight of the lifted object. We implemented the system on the humanoid robot iCub in a 'robot waiter' scenario and experimentally verified the readability of the robot motion. We suggest that the implementation of such human-aware motion planning could ensure a seamless and natural interaction with nonexpert users, yielding in turn to safer and more efficient object passing.},
	journal = {IEEE-RAS International Conference on Humanoid Robots},
	author = {Palinko, Oskar and Sciutti, Alessandra and Patané, Laura and Rea, Francesco and Nori, Francesco and Sandini, Giulio},
	year = {2015},
	note = {ISBN: 9781479971749},
	keywords = {Human-Robot Interaction(HRI), Motion, Objects Weights},
	pages = {1116--1121},
}

@article{bingham_kinematic_1987,
	title = {Kinematic {Form} and {Scaling}: {Further} {Investigations} on the {Visual} {Perception} of {Lifted} {Weight}},
	volume = {13},
	issn = {00961523},
	doi = {10.1037/0096-1523.13.2.155},
	abstract = {Observers are able to judge accurately the weight lifted by another person when only the motions of reflective patches attached to the lifter's major limb joints and head can be seen (Runeson \& Frykholm, 1981). What properties of these complex kinematic patterns allow judgments of weight to be made? The pattern of variation in velocity of the lifted object over position is explored as a source of information for weight: It is found to provide limited information. How are variations in kinematic patterns scaled to allow judgments of weight, a kinetic quantity? The possibility of a source of information for scaling in the kinematics is investigated. Judgments based only on patch-light displays are accurate to a degree that is improved by an extrinsic scaling basis. Finally, the sensitivity to scaling of alternative metrics used in judging is explored. Intrinsic metrics are discovered to be less sensitive to the absence of an extrinsic basis for scaling. © 1987 American Psychological Association.},
	number = {2},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Bingham, Geoffrey P.},
	year = {1987},
	keywords = {Motion, Objects Weights, Psychology},
	pages = {155--177},
}

@article{hamilton_kinematic_2007,
	title = {Kinematic cues in perceptual weight judgement and their origins in box lifting},
	volume = {71},
	issn = {03400727},
	doi = {10.1007/s00426-005-0032-4},
	abstract = {When accepting a parcel from another person, we are able to use information about that person's movement to estimate in advance the weight of the parcel, that is, to judge its weight from observed action. Perceptual weight judgment provides a powerful method to study our interpretation of other people's actions, but it is not known what sources of information are used in judging weight. We have manipulated full form videos to obtain precise control of the perceived kinematics of a box lifting action, and use this technique to explore the kinematic cues that affect weight judgment. We find that observers rely most on the duration of the lifting movement to judge weight, and make less use of the durations of the grasp phase, when the box is first gripped, or the place phase, when the box is put down. These findings can be compared to the kinematics of natural box lifting behaviour, where we find that the duration of the grasp component is the best predictor of true box weight. The lack of accord between the optimal cues predicted by the natural behaviour and the cues actually used in the perceptual task has implications for our understanding of action observation in terms of a motor simulation. The differences between perceptual and motor behaviour are evidence against a strong version of the motor simulation hypothesis. © Springer-Verlag 2006.},
	number = {1},
	journal = {Psychological Research},
	author = {Hamilton, Antonia F.De C. and Joyce, D. W. and Flanagan, J. R. and Frith, C. D. and Wolpert, D. M.},
	year = {2007},
	keywords = {Human Activity, Objects Weights, Psychology},
	pages = {13--21},
}

@article{ravichandar_learning_2017,
	title = {Learning {Contracting} {Nonlinear} {Dynamics} {From} {Human} {DEMONSTRATION} {FOR} {ROBOT} {MOTION} {PLANNING}},
	abstract = {In this paper, we present an algorithm to learn the dynam- ics of human arm motion from the data collected from human actions. Learning the motion plans from human demonstra- tions is essential in making robot programming possible by non- expert programmers as well as realizing human-robot collabora- tion. The highly complex human reaching motion is generated by a stable closed-loop dynamical system. To capture the com- plexity a neural network (NN) is used to represent the dynamics of the human motion states. The trajectories of arm generated by humans for reaching to a place are contracting towards the goal location from various initial conditions with built in obsta- cle avoidance. To take into consideration the contracting nature of the human motion dynamics the unknown motion model is learned using a NN subject to contraction analysis constraints. To learn the NN parameters an optimization problem is formu- lated by relaxing the non-convex contraction constraints to Lin- ear matrix inequality (LMI) constraints. Sequential Quadratic Programming (SQP) is used to solve the optimization problem subject to the LMI constraints. For obstacle avoidance a nega- tive gradient of the repulsive potential function is added to the learned contracting NN model. Experiments are conducted on Baxter robot platform to show that the robot can generate reach- ing paths from the contracting NN dynamics learned from human demonstrated data recorded using Microsoft Kinect sensor. The algorithm is able to adapt to situations for which the demonstra- tions are not available, e.g., an obstacle placed in the path.},
	author = {Ravichandar, Harish and Dani, Ashwin},
	year = {2017},
	keywords = {DSCC2015-9870, Human Demonstration, Motion, Planning},
	pages = {1--8},
}

@article{alaerts_observing_2010,
	title = {Observing how others lift light or heavy objects: {Which} visual cues mediate the encoding of muscular force in the primary motor cortex?},
	volume = {48},
	issn = {00283932},
	url = {http://dx.doi.org/10.1016/j.neuropsychologia.2010.03.029},
	doi = {10.1016/j.neuropsychologia.2010.03.029},
	abstract = {Observers are able to judge quite accurately the weights lifted by others. Only recently, neuroscience has focused on the role of the motor system to accomplish this task. In this respect, a previous transcranial magnetic stimulation (TMS) study showed that the muscular force requirements of an observed action are encoded by the primary motor cortex (M1).Overall, three distinct visual sources may provide information on the applied force of an observed lifting action, namely, (i) the perceived kinematics, (ii) the hand contraction state and finally (iii) intrinsic object properties. The principal aim of the present study was to disentangle these three visual sources and to explore their importance in mediating the encoding of muscular force requirements in the observer's motor system. A series of experiments are reported in which TMS was used to measure 'force-related' responses from the hand representation in left M1 while subjects observed distinct action-stimuli.Overall, results indicated that observation-induced activity in M1 reflects the level of observed force when kinematic cues of the lift (exp. 1) or cues on the hand contraction state (exp. 2) are available. Moreover, when kinematic cues and intrinsic object properties provide distinct information on the force requirements of an observed lifting action, results from experiment 3 indicated a strong preference for the use of kinematic features in mapping the force requirements of the observed action. In general, these findings support the hypothesis that the primary motor cortex contributes to action observation by mapping the muscle-related features of observed actions. © 2010 Elsevier Ltd.},
	number = {7},
	journal = {Neuropsychologia},
	author = {Alaerts, Kaat and Swinnen, Stephan P. and Wenderoth, Nicole},
	year = {2010},
	note = {Publisher: Elsevier Ltd},
	keywords = {Action observation, Human Experiment, Intrinsic object properties, Kinematics, Mirror system, Neuroscience, Objects Weights, Transcranial magnetic stimulation},
	pages = {2082--2090},
}

@article{alaerts_force_2010,
	title = {Force requirements of observed object lifting are encoded by the observer's motor system: {A} {TMS} study},
	volume = {31},
	issn = {0953816X},
	doi = {10.1111/j.1460-9568.2010.07124.x},
	abstract = {Several transcranial magnetic stimulation (TMS) studies have reported facilitation of the primary motor cortex (M1) during the mere observation of actions. This facilitation was shown to be highly congruent, in terms of somatotopy, with the observed action, even at the level of single muscles. With the present study, we investigated whether this muscle-specific facilitation of the observer's motor system reflects the degree of muscular force that is exerted in an observed action. Two separate TMS experiments are reported in which corticospinal excitability was measured in the hand area of M1 while subjects observed the lifting of objects of different weights. The type of action 'grasping-and-lifting-the-object' was always identical, but the grip force varied according to the object's weight. In accordance to previous findings, excitability of M1 was shown to modulate in a muscle-specific way, such that only the cortical representation areas in M1 that control the specific muscles used in the observed lifting action became increasingly facilitated. Moreover, muscle-specific M1 facilitation was shown to modulate to the force requirements of the observed actions, such that M1 excitability was considerably higher when observing heavy object lifting compared with light object lifting. Overall, these results indicate that different levels of observed grip force are mirrored onto the observer's motor system in a highly muscle-specific manner. The measured force-dependent modulations of corticospinal excitability in M1 are hypothesized to be functionally relevant for scaling the observed grip force in the observer's own motor system. In turn, this mechanism may contribute, at least partly, to the observer's ability to infer the weight of the lifted object. © Federation of European Neuroscience Societies and Blackwell Publishing Ltd.},
	number = {6},
	journal = {European Journal of Neuroscience},
	author = {Alaerts, Kaat and Senot, Patrice and Swinnen, Stephan P. and Craighero, Laila and Wenderoth, Nicole and Fadiga, Luciano},
	year = {2010},
	keywords = {Action observation, Grip force, Human Experiment, Mirror system, Neuroscience, Objects Weights, Transcranial magnetic stimulation},
	pages = {1144--1153},
}

@article{pellegrinelli_human-robot_2016,
	title = {Human-robot shared workspace collaboration via hindsight optimization},
	volume = {2016-Novem},
	issn = {21530866},
	doi = {10.1109/IROS.2016.7759147},
	abstract = {Our human-robot collaboration research aims to improve the fluency and efficiency of interactions between humans and robots when executing a set of tasks in a shared workspace. During human-robot collaboration, a robot and a user must often complete a disjoint set of tasks that use an overlapping set of objects, without using the same object simultaneously. A key challenge is deciding what task the robot should perform next in order to facilitate fluent and efficient collaboration. Most prior work does so by first predicting the human's intended goal, and then selecting actions given that goal. However, it is often difficult, and sometimes impossible, to infer the human's exact goal in real time, and this serial predict-then-act method is not adaptive to changes in human goals. In this paper, we present a system for inferring a probability distribution over human goals, and producing assistance actions given that distribution in real time. The aim is to minimize the disruption caused by the nature of human-robot shared workspace. We extend recent work utilizing Partially Observable Markov Decision Processes (POMDPs) for shared autonomy in order to provide assistance without knowing the exact goal. We evaluate our system in a study with 28 participants, and show that our POMDP model outperforms state of the art predict-then-act models by producing fewer human-robot collisions and less human idling time.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Pellegrinelli, Stefania and Admoni, Henny and Javdani, Shervin and Srinivasa, Siddhartha},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509037629},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Prediction},
	pages = {831--838},
}

@article{zhao_considering_2018,
	title = {Considering {Human} {Behavior} in {Motion} {Planning} for {Smooth} {Human}-{Robot} {Collaboration} in {Close} {Proximity}},
	doi = {10.1109/ROMAN.2018.8525607},
	abstract = {It is well-known that a deep understanding of coworkers' behavior and preference is important for collaboration effectiveness. In this work, we present a method to accomplish smooth human-robot collaboration in close proximity by taking into account the human's behavior while planning the robot's trajectory. In particular, we first use an occupancy map to summarize human's movement preference over time, and such prior information is then considered in an optimization-based motion planner via two cost items: 1) avoidance of the workspace previously occupied by human, to eliminate the interruption and to increase the task success rate; 2) tendency to keep a safe distance between the human and the robot to improve the safety. In the experiments, we compare the collaboration performance among planners using different combinations of human-aware cost items, including the avoidance factor, both the avoidance and safe distance factor, and a baseline where no human-related factors are considered. The trajectories generated are tested in both simulated and real-world environments, and the results show that our method can significantly increase the collaborative task success rates and is also human-friendly.},
	journal = {RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication},
	author = {Zhao, Xuan and Pan, Jia},
	year = {2018},
	note = {arXiv: 1807.07749
Publisher: IEEE
ISBN: 9781538679807},
	keywords = {Human-Robot Interaction(HRI), Motion, Planning},
	pages = {985--990},
}

@article{weitschat_safe_2018,
	title = {Safe and efficient human-robot collaboration part {I}: {Estimation} of human arm motions},
	issn = {10504729},
	doi = {10.1109/ICRA.2018.8461190},
	abstract = {A significant barrier regarding a successful implementation of fenceless robot cells into manufacturing areas with humans is given by the inefficiency due to safety requirements. Robot motions have to be slowed down so that an unexpected collision with a human does not result in human injuries. This velocity reduction leads to longer cycle times and, hence, fenceless robot cells turn out as uneconomic. In this paper, a new approach for human-robot collaboration in assembly tasks is presented. For a better performance of the robot, methods are investigated on how the robot can exploit a maximum performance while maintaining the safety of collaborating humans. For this purpose, the kinematics and dynamics of a human arm are described by a control-oriented dynamic model to determine its capability and reachability. Successful experiments validate the dynamic model as well as a corresponding projection approach for calculating possible movements of the human arm that may lead to a collision with the robot. Finally, this information is used to calculate an admissible path velocity that minimizes the danger of human injuries.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Weitschat, Roman and Ehrensperger, Jan and Maier, Moritz and Aschemann, Harald},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538630815},
	keywords = {Human-Robot Interaction(HRI), Motion, Prediction},
	pages = {1993--1999},
}

@article{knepper_implicit_2017,
	title = {Implicit {Communication} in a {Joint} {Action}},
	volume = {Part F1271},
	issn = {21672148},
	doi = {10.1145/2909824.3020226},
	abstract = {Robots must be cognizant of how their actions will be interpreted in context. Actions performed in the context of a joint activity comprise two aspects: functional and communicative. The functional component achieves the goal of the action, whereas its communicative component, when present, expresses some information to the actor's partners in the joint activity. The interpretation of such communication requires leveraging information that is public to all participants, known as common ground. Much of human communication is performed through this implicit mechanism, and humans cannot help but infer some meaning - whether or not it was intended by the actor - from most actions. We present a framework for robots to utilize this communicative channel on top of normal functional actions to work more effectively with human partners. We consider the role of the actor and the observer, both individually and jointly, in implicit communication, as well as the effects of timing. We also show how the framework maps onto various modes of action, including natural language and motion. We consider these modes of action in various human-robot interaction domains, including social navigation and collaborative assembly.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Knepper, Ross A. and Mavrogiannis, Christoforos I. and Proft, Julia and Liang, Claire},
	year = {2017},
	note = {ISBN: 9781450343367},
	keywords = {Human Activity, Joint-action, Robotics, common ground, generation, implicit, indirect, joint activity, multi-modal communication, understanding},
	pages = {283--292},
}

@article{sunderhauf_limits_2018,
	title = {The limits and potentials of deep learning for robotics},
	volume = {37},
	issn = {17413176},
	doi = {10.1177/0278364918770733},
	abstract = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.},
	number = {4-5},
	journal = {International Journal of Robotics Research},
	author = {Sünderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, Jürgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
	year = {2018},
	note = {arXiv: 1804.06557
ISBN: 0278364918},
	keywords = {Deep, Review Paper, Robotics, deep learning, machine learning, robotic vision},
	pages = {405--420},
}

@book{tomoki_ojisakuragi_weight_2018,
	title = {Weight {Estimation} of {Lifted} {Object} from {Body} {Motions} {Using} {Neural} {Network}},
	volume = {1},
	isbn = {978-3-319-93399-3},
	url = {http://dx.doi.org/10.1007/978-3-319-93399-3_1},
	abstract = {In this paper, we propose a method based on machine learn- ing, which estimates the mass of an object from a body motion performed to lift it. In the field of behavior recognition and prediction, some pre- vious studies had focused on estimating the current or future state of a person from his/her motion. In contrast, this research estimates the information of an object in contact with a person. Using this method, we can obtain a rough estimate of an object’s mass without using a weighing machine. Such a measurement system will be useful in several applications, for example, for estimating the excess weight of baggage before checking-in at the airport. We believe that this system can also be used for the evaluation of haptic illusions such as the size–weight illusion. The proposed system detects human-body joints as the input dataset for machine learning. We created a neural network that esti- mated an object’s mass in real-time, u/sing data from a single person for training. The experimental results showed that the proposed system could estimate an object’s mass more accurately than human senses.},
	publisher = {Springer International Publishing},
	author = {Tomoki OjiSakuragi, Rei and Makino, Yasutoshi and Shinoda, Hiroyuki},
	year = {2018},
	doi = {10.1007/978-3-319-93399-3},
	note = {Publication Title: Proc. EuroHaptics 2018
Issue: June},
	keywords = {Cross-Modal, Motion, Object, Prediction, Vibrotactile, Virtual reality, cross-modal, vibrotactile, virtual reality},
}

@article{nemlekar_object_2019,
	title = {Object transfer point estimation for fluent human-robot handovers},
	volume = {2019-May},
	issn = {10504729},
	doi = {10.1109/ICRA.2019.8794008},
	abstract = {Handing over objects is the foundation of many human-robot interaction and collaboration tasks. In the scenario where a human is handing over an object to a robot, the human chooses where the object needs to be transferred. The robot needs to accurately predict this point of transfer to reach out proactively, instead of waiting for the final position to be presented. This work presents an efficient method for predicting the Object Transfer Point (OTP), which synthesizes (1) an offline OTP calculated based on human preferences observed in a human-robot motion study with (2) a dynamic OTP predicted based on the observed human motion. Our proposed OTP predictor is implemented on a humanoid nursing robot and experimentally validated in human-robot handover tasks. Compared to only using static or dynamic OTP estimators, it has better accuracy at the earlier phase of handover (up to 45\% of the handover motion) and can render fluent handovers with a reach-to-grasp response time (about 3.1 secs) close to natural human receiver's response. In addition, the OTP prediction accuracy is maintained across the robot's visible workspace by utilizing a user-adaptive reference frame.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Nemlekar, Heramb and Dutia, Dharini and Li, Zhi},
	year = {2019},
	note = {ISBN: 9781538660263},
	keywords = {Handovers, Human-Robot Interaction(HRI), Object},
	pages = {2627--2633},
}

@article{welschehold_learning_2016,
	title = {Learning manipulation actions from human demonstrations},
	volume = {2016-Novem},
	issn = {21530866},
	doi = {10.1109/IROS.2016.7759555},
	abstract = {Learning from demonstration is a popular approach for teaching robots as it allows service robots to acquire new skills without explicit programming. However, for manipulation actions mostly kinesthetic teaching is used as these actions require precise knowledge about the interactions between the robot and the object. In this paper, we present a novel approach that allows a robot to learn actions carried out by a teacher from observations. We achieve this by first transforming RGBD observations to consistent hand-object trajectories, which are then adapted to the robot's grasping capabilities. Experimental results show that the robot is able to learn complex tasks such as opening doors or drawers.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Welschehold, Tim and Dornhege, Christian and Burgard, Wolfram},
	year = {2016},
	note = {ISBN: 9781509037629},
	keywords = {Human Demonstration, Imitation, RGB-D},
	pages = {3772--3777},
}

@article{paletta_towards_2017,
	title = {Towards real-time probabilistic evaluation of situation awareness from human gaze in human-robot interaction},
	issn = {21672148},
	doi = {10.1145/3029798.3038322},
	abstract = {Human attention processes play a major role for optimization in human-robot interaction (HRI). This work describes a novel methodology to measure situation awareness in real-time from gaze interaction with scene objects of interest using eye tracking glasses and 3D gaze analysis. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position tracking. Comprehensive experiments on HRI were conducted with tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict a standard measure of situation awareness (SAGAT) in real-time and will open new opportunities for human factors based performance optimization in HRI applications.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Paletta, Lucas and Dini, Amir and Murko, Cornelia and Yahyanejad, Saeed and Schwarz, Michael and Lodron, Gerald and Ladstätter, Stefan and Paar, Gerhard and Velik, Rosemarie},
	year = {2017},
	note = {ISBN: 9781450348850},
	keywords = {Gaze, Human Activity, Human-Robot Interaction(HRI), eye tracking, situation awareness, visual attention},
	pages = {247--248},
}

@article{do_accurate_2019,
	title = {Accurate pouring with an autonomous robot using an {RGB}-{D} camera},
	volume = {867},
	issn = {21945357},
	doi = {10.1007/978-3-030-01370-7_17},
	abstract = {Robotic assistants in a home environment are expected to perform various complex tasks for their users. One particularly challenging task is pouring drinks into cups, which for successful completion, requires the detection and tracking of the liquid level during a pour to determine when to stop. In this paper, we present a novel approach to autonomous pouring that tracks the liquid level using an RGB-D camera and adapts the rate of pouring based on the liquid level feedback. We thoroughly evaluate our system on various types of liquids and under different conditions, conducting over 250 pours with a PR2 robot. The results demonstrate that our approach is able to pour liquids to a target height with an accuracy of a few millimeters.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Do, Chau and Burgard, Wolfram},
	year = {2019},
	note = {arXiv: 1810.03303
ISBN: 9783030013691},
	keywords = {Cup, Household robotics, Human Activity, Liquid perception, RGB-D, Robot pouring},
	pages = {210--221},
}

@article{azhar_study_2017,
	title = {A study measuring the impact of shared decision making in a human-robot team},
	volume = {36},
	issn = {17413176},
	doi = {10.1177/0278364917710540},
	abstract = {This paper presents the results of a user study in which the impact of sharing decision making in a human-robot team was measured. In the experiments outlined here, a human and robot play a game together in which the robot searches an arena for items, with input from the human, and the human-robot team earns points for finding and correctly identifying the items. The user study reported here involved 60 human subjects. Each subject interacted with two different robots. With one robot, the human acted as a supervisor: the human issued commands and the robot obeyed. With the other robot, the human acted as a collaborator: the human and robot shared decisions and were required to reach agreement about the robot’s actions in the arena before any actions were taken, facilitated using computational argumentation. Objective performance metrics were collected and analyzed for both types of human-robot team, as well subjective feedback from human subjects regarding attitudes toward working with a robot. The objective results showed significant improvement in performance metrics with the human-as-collaborator pairs versus the human-as-supervisor pairs. Subjective results demonstrated significant differences across many subjective measures and indicated a distinct preference for the humanas- collaborator mode. The primary contribution of this work lies in the demonstration and evaluation of a computational argumentation approach to human-robot interaction, particularly in proving the efficacy of this approach over a less autonomous mode of interaction.},
	number = {5-7},
	journal = {International Journal of Robotics Research},
	author = {Azhar, Mohammad Q. and Sklar, Elizabeth I.},
	year = {2017},
	keywords = {Argumentation-based dialogue, Human Activity, Human Experiment, Human-Robot Interaction(HRI), Human-robot team, Shared decision making},
	pages = {461--482},
}

@article{li_continuous_2015,
	title = {Continuous {Role} {Adaptation} for {Human}-{Robot} {Shared} {Control}},
	volume = {31},
	issn = {15523098},
	doi = {10.1109/TRO.2015.2419873},
	abstract = {In this paper, we propose a role adaptation method for human-robot shared control. Game theory is employed for fundamental analysis of this two-agent system. An adaptation law is developed such that the robot is able to adjust its own role according to the human's intention to lead or follow, which is inferred through the measured interaction force. In the absence of human interaction forces, the adaptive scheme allows the robot to take the lead and complete the task by itself. On the other hand, when the human persistently exerts strong forces that signal an unambiguous intent to lead, the robot yields and becomes the follower. Additionally, the full spectrum of mixed roles between these extreme scenarios is afforded by continuous online update of the control that is shared between both agents. Theoretical analysis shows that the resulting shared control is optimal with respect to a two-agent coordination game. Experimental results illustrate better overall performance, in terms of both error and effort, compared with fixed-role interactions.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Li, Yanan and Tee, Keng Peng and Chan, Wei Liang and Yan, Rui and Chua, Yuanwei and Limbu, Dilip Kumar},
	year = {2015},
	note = {Publisher: IEEE},
	keywords = {Adaptive control, Human-Robot Interaction(HRI), Robotics, physical human-robot interaction, shared control},
	pages = {672--681},
}

@article{aroyo_trust_2018,
	title = {Trust and {Social} {Engineering} in {Human} {Robot} {Interaction}: {Will} a {Robot} {Make} {You} {Disclose} {Sensitive} {Information}, {Conform} to {Its} {Recommendations} or {Gamble}?},
	volume = {3},
	issn = {23773766},
	doi = {10.1109/LRA.2018.2856272},
	abstract = {Robots such as information security and overtrust in them are gaining increasing relevance. This research aims at giving an insight into how trust toward robots could be exploited for the purpose of social engineering. Drawing on Mitnick's model, a well-known social engineering framework, an interactive scenario with the humanoid robot iCub was designed to emulate a social engineering attack. At first, iCub attempted to collect the kind of personal information usually gathered by social engineers by asking a series of private questions. Then, the robot tried to develop trust and rapport with participants by offering reliable clues during a treasure hunt game. At the end of the treasure hunt, the robot tried to exploit the gained trust in order to make participants gamble the money they won. The results show that people tend to build rapport with and trust toward the robot, resulting in the disclosure of sensitive information, conformation to its suggestions and gambling.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Aroyo, Alexander Mois and Rea, Francesco and Sandini, Giulio and Sciutti, Alessandra},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Social human-robot interaction, cognitive human-robot interaction, ethics and philosophy},
	pages = {3701--3708},
}

@article{wang_controlling_2019,
	title = {Controlling object hand-over in human-robot collaboration via natural wearable sensing},
	volume = {49},
	issn = {21682291},
	doi = {10.1109/THMS.2018.2883176},
	abstract = {With the deployment of collaborative robots in intelligent manufacturing, object hand-over between humans and robots plays a significant role in human-robot collaborations. In most collaboration studies, human hand-over intentions were usually assumed to be known by the robot, and the research mainly focused on robot motion planning and control during the hand-over process. Several approaches have been developed to control the human-robot hand-over, such as vision-based approach and physical contact-based approach, but their applications in manufacturing environments are limited due to various constraints, such as limited human working ranges and safety concerns. In this paper, we develop a practical approach using a wearable sensory system, which has a natural and simple configuration and can be easily utilized by humans. This approach could make a robot recognize a human's hand-over intentions and enable the human to effectively and naturally control the hand-over process. In addition, the approach could recognize the attribute classes of the objects in the human's hand using the wearable sensing and enable the robot to actively make decisions to ensure that graspable objects are handed over from the human to the robot. Results and evaluations illustrate the effectiveness and advantages of the proposed approach in human-robot hand-over control.},
	number = {1},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Wang, Weitian and Li, Rui and Diekel, Zachary Max and Chen, Yi and Zhang, Zhujun and Jia, Yunyi},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {EMG, Hand-over control, Handovers, Human-Robot Interaction(HRI), human intention understanding, human-robot collaboration, object attribute recognition},
	pages = {59--71},
}

@article{cakmak_using_2011,
	title = {Using {Spatial} and {Temporal} {Contrast} for {Fluent} {Robot}-{Human} {Hand}-overs},
	abstract = {For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an ob- servational study with our robot HERB, we identify the key problems with a baseline hand-over action. We find that the failure to convey the intention of handing over causes delays in the transfer, while the lack of an intuitive signal to indicate timing of the hand-over causes early, unsuccessful attempts to take the object. We propose to address these problems with the use of spatial contrast, in the form of dis- tinct hand-over poses, and temporal contrast, in the form of unambiguous transitions to the hand-over pose. We conduct a survey to identify distinct hand-over poses, and determine variables of the pose that have most communicative poten- tial for the intent of handing over. We present an experiment that analyzes the effect of the two types of contrast on the fluency of hand-overs. We find that temporal contrast is particularly useful in impro},
	author = {Cakmak, Maya and Srinivasa, Siddhartha S and Lee, Min Kyung and Kiesler, Sara and Forlizzi, Jodi},
	year = {2011},
	note = {ISBN: 9781450305617},
	keywords = {Handovers, Human-Robot Interaction(HRI), Robotics, fluency, robot-human hand-overs},
}

@article{liu_goal_2016,
	title = {Goal inference improves objective and perceived performance in human-robot collaboration},
	issn = {15582914},
	abstract = {The study of human-robot interaction is fundame design and use of robotics in real-world application will need to predict and adapt to the actions of h laborators in order to achieve good performanc prove safety and end-user adoption. This paper e human-robot collaboration scheme that combine allocation and motion levels of reasoning: the rob uses Bayesian inference to predict the next goal man partner from his or her ongoing motion, an its own actions in real time. This anticipative ada desirable in many practical scenarios, where huma able or unwilling to take on the cognitive overhea to explicitly communicate their intent to the rob havioral experiment indicates that the combinati inference and dynamic task planning significantly both objective and perceived performance of th robot team. Participants were highly sensitive ferences between robot behaviors, preferring to w robot that adapted to their actions over one that did not.},
	number = {Aamas},
	journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
	author = {Liu, Chang and Hamrick, Jessica B. and Fisac, Jaime F. and Dragan, Anca D. and Hedrick, J. Karl and Sastry, S. Shankar and Griffiths, Thomas L.},
	year = {2016},
	note = {arXiv: 1802.01780
ISBN: 9781450342391},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Human-agent interaction, Intention collaborative task allocation, Prediction, Teamwork},
	pages = {940--948},
}

@article{hoffman_evaluating_2019,
	title = {Evaluating {Fluency} in {Human}-{Robot} {Collaboration}},
	volume = {49},
	issn = {21682305},
	doi = {10.1109/THMS.2019.2904558},
	abstract = {Collaborative fluency is the coordinated meshing of joint activities between members of a well-synchronized team. In recent years, researchers in human-robot collaboration have been developing robots to work alongside humans aiming not only at task efficiency, but also at human-robot fluency. As part of this effort, we have developed a number of metrics to evaluate the level of fluency in human-robot shared-location teamwork. While these metrics are being used in existing research, there has been no systematic discussion on how to measure fluency and how the commonly used metrics perform and compare. In this paper, we codify subjective and objective human-robot fluency metrics, provide an analytical model for four objective metrics, and assess their dynamics in a turn-taking framework. We also report on a user study linking objective and subjective fluency metrics and survey recent use of these metrics in the literature.},
	number = {3},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Hoffman, Guy},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Artificial intelligence, Human Activity, Human-Robot Interaction(HRI), Review Paper, computational and artificial intelligence, cooperative systems, cybernetics, human-robot interaction, intelligent robots, intelligent systems, man, man-machine systems, systems, user interfaces},
	pages = {209--218},
}

@article{vogt_one-shot_2018,
	title = {One-shot learning of human–robot handovers with triadic interaction meshes},
	volume = {42},
	issn = {15737527},
	url = {https://doi.org/10.1007/s10514-018-9699-4},
	doi = {10.1007/s10514-018-9699-4},
	abstract = {We propose an imitation learning methodology that allows robots to seamlessly retrieve and pass objects to and from human users. Instead of hand-coding interaction parameters, we extract relevant information such as joint correlations and spatial relationships from a single task demonstration of two humans. At the center of our approach is an interaction model that enables a robot to generalize an observed demonstration spatially and temporally to new situations. To this end, we propose a data-driven method for generating interaction meshes that link both interaction partners to the manipulated object. The feasibility of the approach is evaluated in a within user study which shows that human–human task demonstration can lead to more natural and intuitive interactions with the robot.},
	number = {5},
	journal = {Autonomous Robots},
	author = {Vogt, David and Stepputtis, Simon and Jung, Bernhard and Amor, Heni Ben},
	year = {2018},
	note = {Publisher: Springer US},
	keywords = {Handover, Handovers, Human-Robot Interaction(HRI), Human–human demonstration, Human–robot interaction, Interaction mesh, Modelling},
	pages = {1053--1065},
}

@article{bhat_humanoid_2016,
	title = {Humanoid infers {Archimedes}' principle: {Understanding} physical relations and object affordances through cumulative learning experiences},
	volume = {13},
	issn = {17425662},
	doi = {10.1098/rsif.2016.0310},
	abstract = {Emerging studies indicate that several species such as corvids, apes and children solve 'The Crow and the Pitcher' task (from Aesop's Fables) in diverse conditions. Hidden beneath this fascinating paradigm is a fundamental question: by cumulatively interacting with different objects, how can an agent abstract the underlying cause-effect relations to predict and creatively exploit potential affordances of novel objects in the context of sought goals? Re-enacting this Aesop's Fable task on a humanoid within an open-ended 'learning-prediction-abstraction' loop, we address this problem and (i) present a brain-guided neural framework that emulates rapid one-shot encoding of ongoing experiences into a long-term memory and (ii) propose four task-agnostic learning rules (elimination, growth, uncertainty and status quo) that correlate predictions from remembered past experiences with the unfolding present situation to gradually abstract the underlying causal relations. Driven by the proposed architecture, the ensuing robot behaviours illustrated causal learning and anticipation similar to natural agents. Results further demonstrate that by cumulatively interacting with few objects, the predictions of the robot in case of novel objects converge close to the physical law, i.e. the Archimedes principle: this being independent of both the objects explored during learning and the order of their cumulative exploration.},
	number = {120},
	journal = {Journal of the Royal Society Interface},
	author = {Bhat, Ajaz Ahmad and Mohan, Vishwanathan and Sandini, Giulio and Morasso, Pietro},
	year = {2016},
	pmid = {27466440},
	keywords = {Affordances, Animal cognition, Cause-effect relations, Developmental robotics, Episodic memory, Object, Robotics},
}

@article{jamone_autonomous_2012,
	title = {Autonomous online learning of reaching behavior in a humanoid robot},
	volume = {9},
	issn = {02198436},
	doi = {10.1142/S021984361250017X},
	abstract = {In this paper we describe an autonomous strategy which enables a humanoid robot to learn how to reach for a visually identified object in the 3D space. The robot is a 22-DOF upper-body humanoid with moving eyes, neck, arm and hand. The robot is bootstrapped with limited a-priori knowledge, sufficient to start the interaction with the environment; this interaction allows the robot to learn different sensorimotor mappings, required for reaching. The arm-head forward kinematic model and a visuo-motor inverse model are learned from sensory experience. Learning is performed purely online (without any separation between training and execution) through a goal-directed exploration of the environment. During the learning the robot is also able to build an internal representation of its reachable space. © 2012 World Scientific Publishing Company. © World Scientic Publishing Company.},
	number = {3},
	journal = {International Journal of Humanoid Robotics},
	author = {Jamone, Lorenzo and Natale, Lorenzo and Nori, Francesco and Metta, Giorgio and Sandini, Giulio},
	year = {2012},
	keywords = {Human Activity, Human Pose, Motion, Reaching, autonomous online learning, humanoids},
	pages = {1--26},
}

@article{mitrokhin_learning_2019,
	title = {Learning sensorimotor control with neuromorphic sensors: {Toward} hyperdimensional active perception},
	volume = {4},
	issn = {24709476},
	doi = {10.1126/scirobotics.aaw6736},
	abstract = {The hallmark of modern robotics is the ability to directly fuse the platform’s perception with its motoric ability—the concept often referred to as “active perception.” Nevertheless, we find that action and perception are often kept in separated spaces, which is a consequence of traditional vision being frame based and only existing in the moment and motion being a continuous entity. This bridge is crossed by the dynamic vision sensor (DVS), a neuromorphic camera that can see the motion. We propose a method of encoding actions and perceptions together into a single space that is meaningful, semantically informed, and consistent by using hyperdimensional binary vectors (HBVs). We used DVS for visual perception and showed that the visual component can be bound with the system velocity to enable dynamic world perception, which creates an opportunity for real-time navigation and obstacle avoidance. Actions performed by an agent are directly bound to the perceptions experienced to form its own “memory.” Furthermore, because HBVs can encode entire histories of actions and perceptions—from atomic to arbitrary sequences—as constant-sized vectors, autoassociative memory was combined with deep learning paradigms for controls. We demonstrate these properties on a quadcopter drone ego-motion inference task and the MVSEC (multivehicle stereo event camera) dataset.},
	number = {30},
	journal = {Science Robotics},
	author = {Mitrokhin, A. and Sutor, P. and Fermüller, C. and Aloimonos, Y.},
	year = {2019},
	keywords = {Action Recognition, Real-Time, Vision},
}

@article{music_control_2017,
	title = {Control sharing in human-robot team interaction},
	volume = {44},
	issn = {13675788},
	doi = {10.1016/j.arcontrol.2017.09.017},
	abstract = {The interaction between humans and robot teams is highly relevant in many application domains, for example in collaborative manufacturing, search and rescue, and logistics. It is well-known that humans and robots have complementary capabilities: Humans are excellent in reasoning and planning in unstructured environments, while robots are very good in performing tasks repetitively and precisely. In consequence, one of the key research questions is how to combine human and robot team decision making and task execution capabilities in order to exploit their complementary skills. From a controls perspective this question boils down to how control should be shared among them. This article surveys advances in human-robot team interaction with special attention devoted to control sharing methodologies. Additionally, aspects affecting the control sharing design, such as human behavior modeling, level of autonomy and human-machine interfaces are identified. Open problems and future research directions towards joint decision making and task execution in human-robot teams are discussed.},
	journal = {Annual Reviews in Control},
	author = {Musić, Selma and Hirche, Sandra},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Human behavior modeling, Human-Robot Interaction(HRI), Human-robot team interaction, Review Paper, Robot team, Robotics, Shared control},
	pages = {342--354},
}

@article{keemink_admittance_2018,
	title = {Admittance control for physical human–robot interaction},
	volume = {37},
	issn = {17413176},
	doi = {10.1177/0278364918768950},
	abstract = {This paper presents an overview of admittance control as a method of physical interaction control between machines and humans. We present an admittance controller framework and elaborate control scheme that can be used for controller design and development. Within this framework, we analyze the influence of feed-forward control, post-sensor inertia compensation, force signal filtering, additional phase lead on the motion reference, internal robot flexibility, which also relates to series elastic control, motion loop bandwidth, and the addition of virtual damping on the stability, passivity, and performance of minimal inertia rendering admittance control. We present seven design guidelines for achieving high-performance admittance controlled devices that can render low inertia, while aspiring coupled stability and proper disturbance rejection.},
	number = {11},
	journal = {International Journal of Robotics Research},
	author = {Keemink, Arvid Q.L. and van der Kooij, Herman and Stienen, Arno H.A.},
	year = {2018},
	note = {ISBN: 0278364918},
	keywords = {Admittance control, Human-Robot Interaction(HRI), Motion, Review Paper, haptics, pHRI, passivity, robotics},
	pages = {1421--1444},
}

@article{rea_human_2019,
	title = {Human {Motion} {Understanding} for {Selecting} {Action} {Timing} in {Collaborative} {Human}-{Robot} {Interaction}},
	volume = {6},
	issn = {2296-9144},
	doi = {10.3389/frobt.2019.00058},
	abstract = {In the industry of the future, so as in healthcare and at home, robots will be a familiar presence. Since they will be working closely with human operators not always properly trained for human-machine interaction tasks, robots will need the ability of automatically adapting to changes in the task to be performed or to cope with variations in how the human partner completes the task. The goal of this work is to make a further step toward endowing robot with such capability. To this purpose, we focus on the identification of relevant time instants in an observed action, called dynamic instants, informative on the partner’s movement timing, and marking instants where an action starts or ends, or changes to another actions. The time instants are temporal locations where the motion can be ideally segmented, providing a set of primitives that can be used to build a temporal signature of the action and finally support the understanding of the dynamics and coordination in time. We validate our approach in two contexts, considering first a situation in which the human partner can perform multiple different activities, and then moving to settings where an action is already recognized and shows a certain degree of periodicity. In the two contexts we address different challenges. In the first one, working in batch on a dataset collecting videos of a variety of cooking activities, we investigate whether the action signature we compute could facilitate the understanding of which type of action is occurring in front of the observer, with tolerance to viewpoint changes. In the second context, we evaluate online on the robot iCub the capability of the action signature in providing hints to establish an actual temporal coordination during the interaction with human participants. In both cases, we show promising results that speak in favour of the potentiality of our approach.},
	number = {July},
	journal = {Frontiers in Robotics and AI},
	author = {Rea, Francesco and Vignolo, Alessia and Sciutti, Alessandra and Noceti, Nicoletta},
	year = {2019},
	keywords = {Action understanding, Human Activity, Motion, action synchronization, human motion understanding, human motion understanding, action synchronization, human-robot, interaction, motion signature, optical flow, view-invariance},
	pages = {1--16},
}

@article{maffongelli_ontogenesis_2019,
	title = {The ontogenesis of action syntax},
	volume = {5},
	issn = {24747394},
	doi = {10.1525/collabra.215},
	abstract = {Language and action share similar organizational principles. Both are thought to be hierarchical and recursive in nature. Here we address the relationship between language and action from developmental and neurophysiological perspectives. We discuss three major aspects: The extent of the analogy between language and action; the necessity to extend research on the yet largely neglected aspect of action syntax; the positive contribution of a developmental approach to this topic. We elaborate on the claim that adding an ontogenetic approach will help to obtain a comprehensive picture about both the interplay between language and action and its development, and to answer the question whether the underlying mechanisms of detecting syntactic violations of action sequences are similar to or different from the processing of language syntactic violations.},
	number = {1},
	journal = {Collabra: Psychology},
	author = {Maffongelli, Laura and D’Ausilio, Alessandro and Fadiga, Luciano and Daum, Moritz M.},
	year = {2019},
	keywords = {Action, Action understanding, Development, Infants, Language, Motion, Neuroscience, Syntax},
	pages = {1--11},
}

@article{tsarouchi_humanrobot_2016,
	title = {Human–robot interaction review and challenges on task planning and programming},
	volume = {29},
	issn = {13623052},
	url = {http://dx.doi.org/10.1080/0951192X.2015.1130251},
	doi = {10.1080/0951192X.2015.1130251},
	abstract = {The wide interest of research and industry in the human–robot interaction (HRI) related topics is proportional to the increased productivity and flexibility of the production lines, as it combines human and robot capabilities. This paper presents a review of recent research and progress on HRI, related to task planning/coordination and programming with emphasis on the manufacturing/production environment. Human–robot task allocation and scheduling, metrics for HRI, as well as the social aspects are reviewed. The role of digital human modelling systems for human–robot task planning related issues is also discussed. The process of learning by demonstration as well as the instructive systems is reviewed, focussing mainly on programming through visual guidance and imitation, voice commands and haptic interaction. The aspect of physical HRI as well as the safety related issues are also discussed. Additionally, a survey on multimodal communication frameworks is presented. Challenges encountered and directions for future research are discussed.},
	number = {8},
	journal = {International Journal of Computer Integrated Manufacturing},
	author = {Tsarouchi, Panagiota and Makris, Sotiris and Chryssolouris, George},
	year = {2016},
	note = {Publisher: Taylor \& Francis},
	keywords = {Human-Robot Interaction(HRI), Planning, Review Paper, human–computer interaction, human–robot interaction, learning, programming by demonstration, robot applications, robotic cells},
	pages = {916--931},
}

@article{goyal_role_2019,
	title = {The role of ego vision in view-invariant action recognition},
	url = {http://arxiv.org/abs/1906.03918},
	abstract = {Analysis and interpretation of egocentric video data is becoming more and more important with the increasing availability and use of wearable cameras. Exploring and fully understanding affinities and differences between ego and allo (or third-person) vision is paramount for the design of effective methods to process, analyse and interpret egocentric data. In addition, a deeper understanding of ego-vision and its peculiarities may enable new research perspectives in which first person viewpoints can act either as a mean for easily acquiring large amounts of data to be employed in general-purpose recognition systems, and as a challenging test-bed to assess the usability of techniques specifically tailored to deal with allocentric vision on more challenging settings. Our work, with an eye to cognitive science findings, leverages transfer learning in Convolutional Neural Networks to demonstrate capabilities and limitations of an implicitly learnt view-invariant representation in the specific case of action recognition.},
	author = {Goyal, Gaurvi and Noceti, Nicoletta and Odone, Francesca and Sciutti, Alessandra},
	year = {2019},
	note = {arXiv: 1906.03918},
	keywords = {Action Recognition, Human Activity, Vision},
}

@article{unhelkar_human-aware_2018,
	title = {Human-{Aware} {Robotic} {Assistant} for {Collaborative} {Assembly}: {Integrating} {Human} {Motion} {Prediction} with {Planning} in {Time}},
	volume = {3},
	issn = {23773766},
	doi = {10.1109/LRA.2018.2812906},
	abstract = {Introducing mobile robots into the collaborative assembly process poses unique challenges for ensuring efficient and safe human-robot interaction. Current human-robot work cells require the robot to cease operating completely whenever a human enters a shared region of the given cell, and the robots do not explicitly model or adapt to the behavior of the human. In this work, we present a human-aware robotic system with single-axis mobility that incorporates both predictions of human motion and planning in time to execute efficient and safe motions during automotive final assembly. We evaluate our system in simulation against three alternative methods, including a baseline approach emulating the behavior of standard safety systems in factories today. We also assess the system within a factory test environment. Through both live demonstration and results from simulated experiments, we show that our approach produces statistically significant improvements in quantitative measures of safety and fluency of interaction.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Unhelkar, Vaibhav V. and Lasota, Przemyslaw A. and Tyroller, Quirin and Buhai, Rares Darius and Marceau, Laurie and Deml, Barbara and Shah, Julie A.},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Human Pose, Human-Robot Interaction(HRI), Physical human-robot interaction, Prediction, assembly, collaborative robots},
	pages = {2394--2401},
}

@article{bohg_interactive_2017,
	title = {Interactive perception: {Leveraging} action in perception and perception in action},
	volume = {33},
	issn = {15523098},
	doi = {10.1109/TRO.2017.2721939},
	abstract = {Recent approaches in robot perception follow the insight that perception is facilitated by interaction with the environment. These approaches are subsumed under the term Interactive Perception (IP). This view of perception provides the following benefits. First, interaction with the environment creates a rich sensory signal that would otherwise not be present. Second, knowledge of the regularity in the combined space of sensory data and action parameters facilitates the prediction and interpretation of the sensory signal. In this survey, we postulate this as a principle for robot perception and collect evidence in its support by analyzing and categorizing existing work in this area. We also provide an overview of the most important applications of IP. We close this survey by discussing remaining open questions. With this survey, we hope to help define the field of Interactive Perception and to provide a valuable resource for future research.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Bohg, Jeannette and Hausman, Karol and Sankaran, Bharath and Brock, Oliver and Kragic, Danica and Schaal, Stefan and Sukhatme, Gaurav S.},
	year = {2017},
	note = {arXiv: 1604.03670},
	keywords = {Action understanding, Autonomous systems, Cognitive robotics, Perception, Review Paper, Robot learning, Robot vision systems},
	pages = {1273--1291},
}

@article{lemaignan_artificial_2017,
	title = {Artificial cognition for social human–robot interaction: {An} implementation},
	volume = {247},
	issn = {00043702},
	url = {http://dx.doi.org/10.1016/j.artint.2016.07.002},
	doi = {10.1016/j.artint.2016.07.002},
	abstract = {Human–Robot Interaction challenges Artificial Intelligence in many regards: dynamic, partially unknown environments that were not originally designed for robots; a broad variety of situations with rich semantics to understand and interpret; physical interactions with humans that requires fine, low-latency yet socially acceptable control strategies; natural and multi-modal communication which mandates common-sense knowledge and the representation of possibly divergent mental models. This article is an attempt to characterise these challenges and to exhibit a set of key decisional issues that need to be addressed for a cognitive robot to successfully share space and tasks with a human. We identify first the needed individual and collaborative cognitive skills: geometric reasoning and situation assessment based on perspective-taking and affordance analysis; acquisition and representation of knowledge models for multiple agents (humans and robots, with their specificities); situated, natural and multi-modal dialogue; human-aware task planning; human–robot joint task achievement. The article discusses each of these abilities, presents working implementations, and shows how they combine in a coherent and original deliberative architecture for human–robot interaction. Supported by experimental results, we eventually show how explicit knowledge management, both symbolic and geometric, proves to be instrumental to richer and more natural human–robot interactions by pushing for pervasive, human-level semantics within the robot's deliberative system.},
	journal = {Artificial Intelligence},
	author = {Lemaignan, Séverin and Warnier, Mathieu and Sisbot, E. Akin and Clodic, Aurélie and Alami, Rachid},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Action understanding, Cognition, Cognitive architecture, Cognitive robotics, Human-Robot Interaction(HRI), Human–robot interaction, Knowledge representation and reasoning, Perspective taking},
	pages = {45--69},
}

@article{tanwani_generative_2017,
	title = {A generative model for intention recognition and manipulation assistance in teleoperation},
	volume = {2017-Septe},
	issn = {21530866},
	doi = {10.1109/IROS.2017.8202136},
	abstract = {Performing remote manipulation tasks by tele-operation with limited bandwidth, communication delays and environmental differences is a challenging problem. In this paper, we learn a task-parameterized generative model from the teleoperator demonstrations using a hidden semi-Markov model that provides assistance in performing remote manipulation tasks. We present a probabilistic formulation to capture the intention of the teleoperator, and subsequently assist the teleoperator by time-independent shared control and/or time-dependent autonomous control formulations of the model. In the shared control mode, the model corrects the remote arm movement based on the current state of the teleoperator; whereas in the autonomous control mode, the model generates the movement of the remote arm for autonomous task execution. We show the formulation of the model with virtual fixtures and provide comparisons to benchmark our approach. Teleoperation experiments with the Baxter robot for reaching a movable target and opening a valve reveal that the proposed methodology improves the performance of the teleoperator and caters for environmental differences in performing remote manipulation tasks.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Tanwani, Ajay Kumar and Calinon, Sylvain},
	year = {2017},
	note = {ISBN: 9781538626825},
	keywords = {Action Recognition, Automation, Human-Robot Interaction(HRI), Learning and Adaptive Systems, Learning from Demonstration, Telerobotics and Teleoperation},
	pages = {43--50},
}

@article{nikolaidis_human-robot_2017-1,
	title = {Human-robot mutual adaptation in collaborative tasks: {Models} and experiments},
	issn = {17413176},
	doi = {10.1177/0278364917690593},
	abstract = {Adaptation is critical for effective team collaboration. This paper introduces a computational formalism for mutual adaptation between a robot and a human in collaborative tasks. We propose the Bounded-Memory Adaptation Model, which is a probabilistic finite-state controller that captures human adaptive behaviors under a bounded-memory assumption. We integrate the Bounded-Memory Adaptation Model into a probabilistic decision process, enabling the robot to guide adaptable participants towards a better way of completing the task. Human subject experiments suggest that the proposed formalism improves the effectiveness of human-robot teams in collaborative tasks, when compared with one-way adaptations of the robot to the human, while maintaining the human’s trust in the robot.},
	journal = {International Journal of Robotics Research},
	author = {Nikolaidis, Stefanos and Hsu, David and Srinivasa, Siddhartha},
	year = {2017},
	keywords = {Human-Robot Interaction(HRI), Human-robot collaboration, mutual-adaptation, planning under uncertainty},
}

@article{kappler_real-time_2018,
	title = {Real-time perception meets reactive motion generation},
	volume = {3},
	issn = {23773766},
	doi = {10.1109/LRA.2018.2795645},
	abstract = {We address the challenging problem of robotic grasping and manipulation in the presence of uncertainty. This uncertainty is due to noisy sensing, inaccurate models, and hard-to-predict environment dynamics. We quantify the importance of continuous, real-time perception and its tight integration with reactive motion generation methods in dynamic manipulation scenarios. We compare three different systems that are instantiations of the most common architectures in the field: 1) a traditional sense-plan-act approach that is still widely used; 2) a myopic controller that only reacts to local environment dynamics; and 3) a reactive planner that integrates feedback control and motion optimization. All architectures rely on the same components for real-time perception and reactive motion generation to allow a quantitative evaluation. We extensively evaluate the systems on a real robotic platform in four scenarios that exhibit either a challenging workspace geometry or a dynamic environment. We quantify the robustness and accuracy that is due to integrating real-time feedback at different time scales in a reactive motion generation system. We also report on the lessons learned for system building.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kappler, Daniel and Meier, Franziska and Issac, Jan and Mainprice, Jim and Cifuentes, Cristina Garcia and Wuthrich, Manuel and Berenz, Vincent and Schaal, Stefan and Ratliff, Nathan and Bohg, Jeannette},
	year = {2018},
	note = {arXiv: 1703.03512
Publisher: IEEE},
	keywords = {Motion, Planning, Reactive and sensor-based planning, Real-Time, perception for grasping and manipulation, sensor-based control},
	pages = {1864--1871},
}

@article{rudenko_human_2019,
	title = {Human {Motion} {Trajectory} {Prediction}: {A} {Survey}},
	doi = {10.1177/ToBeAssigned},
	abstract = {We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.},
	journal = {arXiv},
	author = {Rudenko, Andrey and Palmieri, Luigi and Herman, Michael and Kitani, Kris M. and Gavrila, Dariu M. and Arras, Kai O.},
	year = {2019},
	note = {arXiv: 1905.06113v3},
	keywords = {Human Experiment, Motion, Review Paper, bit-rock stochastic interaction model, drillstring dynamics, experimental identification, hysteretic friction},
	pages = {107754631982824},
}

@article{butepage_anticipating_2018,
	title = {Anticipating {Many} {Futures}: {Online} {Human} {Motion} {Prediction} and {Generation} for {Human}-{Robot} {Interaction}},
	issn = {10504729},
	doi = {10.1109/ICRA.2018.8460651},
	abstract = {Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. The bottleneck of most methods is the lack of an accurate model of natural human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motion patterns. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Butepage, Judith and Kjellstrom, Hedvig and Kragic, Danica},
	year = {2018},
	note = {arXiv: 1702.08212
ISBN: 9781538630815},
	keywords = {Anticipation, Human Activity, Motion},
	pages = {4563--4570},
}

@article{nogueira-campos_anticipatory_2019,
	title = {Anticipatory postural adjustments during joint action coordination},
	volume = {9},
	issn = {20452322},
	doi = {10.1038/s41598-019-48758-1},
	abstract = {There is a current claim that humans are able to effortlessly detect others’ hidden mental state by simply observing their movements and transforming the visual input into motor knowledge to predict behaviour. Using a classical paradigm quantifying motor predictions, we tested the role of vision feedback during a reach and load-lifting task performed either alone or with the help of a partner. Wrist flexor and extensor muscle activities were recorded on the supporting hand. Early muscle changes preventing limb instabilities when participants performed the task by themselves revealed the contribution of the visual input in postural anticipation. When the partner performed the unloading, a condition mimicking a split-brain situation, motor prediction followed a pattern evolving along the task course and changing with the integration of successive somatosensory feedback. Our findings demonstrate that during social behaviour, in addition to self-motor representations, individuals cooperate by continuously integrating sensory signals from various sources.},
	number = {1},
	journal = {Scientific Reports},
	author = {Nogueira-Campos, A. A. and Hilt, P. M. and Fadiga, L. and Veronesi, C. and D’Ausilio, A. and Pozzo, T.},
	year = {2019},
	pages = {1--9},
}

@article{ingrand_deliberation_2017,
	title = {Deliberation for autonomous robots: {A} survey},
	volume = {247},
	issn = {00043702},
	url = {http://dx.doi.org/10.1016/j.artint.2014.11.003},
	doi = {10.1016/j.artint.2014.11.003},
	abstract = {Autonomous robots facing a diversity of open environments and performing a variety of tasks and interactions need explicit deliberation in order to fulfill their missions. Deliberation is meant to endow a robotic system with extended, more adaptable and robust functionalities, as well as reduce its deployment cost. The ambition of this survey is to present a global overview of deliberation functions in robotics and to discuss the state of the art in this area. The following five deliberation functions are identified and analyzed: planning, acting, monitoring, observing, and learning. The paper introduces a global perspective on these deliberation functions and discusses their main characteristics, design choices and constraints. The reviewed contributions are discussed with respect to this perspective. The survey focuses as much as possible on papers with a clear robotics content and with a concern on integrating several deliberation functions.},
	journal = {Artificial Intelligence},
	author = {Ingrand, Félix and Ghallab, Malik},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Acting, Deliberation, Learning, Monitoring, Observing, Planning, Robotics},
	pages = {10--44},
}

@article{robla-gomez_working_2017,
	title = {Working {Together}: {A} {Review} on {Safe} {Human}-{Robot} {Collaboration} in {Industrial} {Environments}},
	volume = {5},
	issn = {21693536},
	doi = {10.1109/ACCESS.2017.2773127},
	abstract = {After many years of rigid conventional procedures of production, industrial manufacturing is going through a process of change toward flexible and intelligent manufacturing, the so-called Industry 4.0. In this paper, human-robot collaboration has an important role in smart factories since it contributes to the achievement of higher productivity and greater efficiency. However, this evolution means breaking with the established safety procedures as the separation of workspaces between robot and human is removed. These changes are reflected in safety standards related to industrial robotics since the last decade, and have led to the development of a wide field of research focusing on the prevention of human-robot impacts and/or the minimization of related risks or their consequences. This paper presents a review of the main safety systems that have been proposed and applied in industrial robotic environments that contribute to the achievement of safe collaborative human-robot work. Additionally, a review is provided of the current regulations along with new concepts that have been introduced in them. The discussion presented in this paper includes multi-disciplinary approaches, such as techniques for estimation and the evaluation of injuries in human-robot collisions, mechanical and software devices designed to minimize the consequences of human-robot impact, impact detection systems, and strategies to prevent collisions or minimize their consequences when they occur.},
	journal = {IEEE Access},
	author = {Robla-Gomez, S. and Becerra, Victor M. and Llata, J. R. and Gonzalez-Sarabia, E. and Torre-Ferrero, C. and Perez-Oria, J.},
	year = {2017},
	keywords = {Human-Robot Interaction(HRI), Industry 4.0, Motion, Review Paper, Safety, Vision, human-robot collaboration, industrial robot, industrial standards},
	pages = {26754--26773},
}

@article{shervin_javdani_henny_admoni_stefania_pellegrinelli_siddhartha_s_srinivasa_shared_2019,
	title = {Shared {Autonomy} via {HindsightOptimization} for {Teleoperation} {andTeaming}},
	doi = {10.1177/ToBeAssigned},
	number = {X},
	journal = {Journal of Vibration and Control},
	author = {Shervin Javdani, Henny Admoni, Stefania Pellegrinelli, Siddhartha S. Srinivasa, {and} J. Andrew Bagnell},
	year = {2019},
	note = {arXiv: 1706.00155v1},
	keywords = {Human  PerformanceAugmentation, Human Experiment, Human-Robot Interaction(HRI), Personal  Robots, Physical  Human-Robot  Interaction, Prediction, Rehabilitation  Robotics, Telerobotics},
	pages = {107754631982824},
}

@article{li_framework_2016,
	title = {A {Framework} of {Human}-{Robot} {Coordination} {Based} on {Game} {Theory} and {Policy} {Iteration}},
	volume = {32},
	issn = {15523098},
	doi = {10.1109/TRO.2016.2597322},
	abstract = {In this paper, we propose a framework to analyze the interactive behaviors of humans and robots in physical interactions. Game theory is employed to describe the system under study, and policy iteration is adopted to provide a solution of Nash equilibrium. The human's control objective is estimated based on the measured interaction force, and it is used to adapt the robot's objective such that human-robot coordination can be achieved. The validity of the proposed method is verified through a rigorous proof and experimental studies.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Li, Yanan and Tee, Keng Peng and Yan, Rui and Chan, Wei Liang and Wu, Yan},
	year = {2016},
	keywords = {Adaptive control, Human-Robot Interaction(HRI), Motion, Robotics, human-robot interaction, robot control},
	pages = {1408--1418},
}

@article{hiatt_human_2017,
	title = {Human modeling for human–robot collaboration},
	issn = {17413176},
	doi = {10.1177/0278364917690592},
	abstract = {Teamwork is best achieved when members of the team understand one another. Human–robot collaboration poses a particular challenge to this goal due to the differences between individual team members, both mentally/computationally and physically. One way in which this challenge can be addressed is by developing explicit models of human teammates. Here, we discuss, compare and contrast the many techniques available for modeling human cognition and behavior, and evaluate their benefits and drawbacks in the context of human–robot collaboration.},
	number = {March 2018},
	journal = {International Journal of Robotics Research},
	author = {Hiatt, Laura M. and Narber, Cody and Bekele, Esube and Khemlani, Sangeet S. and Trafton, J. Gregory},
	year = {2017},
	keywords = {Cognitive human–robot interaction, Human-Robot Interaction(HRI), Modelling, Review Paper, cognitive robotics, human-centered and life-like robotics},
}

@article{beckerle_human-robot_2017,
	title = {A human-robot interaction perspective on assistive and rehabilitation robotics},
	volume = {11},
	issn = {16625218},
	doi = {10.3389/fnbot.2017.00024},
	abstract = {Assistive and rehabilitation devices are a promising and challenging field of recent robotics research. Motivated by societal needs such as aging populations, such devices can support motor functionality and subject training. The design, control, sensing, and assessment of the devices become more sophisticated due to a human in the loop. This paper gives a human-robot interaction perspective on current issues and opportunities in the field. On the topic of control and machine learning, approaches that support but do not distract subjects are reviewed. Options to provide sensory user feedback that are currently missing from robotic devices are outlined. Parallels between device acceptance and affective computing are made. Furthermore, requirements for functional assessment protocols that relate to real-world tasks are discussed. In all topic areas, the design of human-oriented frameworks and methods is dominated by challenges related to the close interaction between the human and robotic device. This paper discusses the aforementioned aspects in order to open up new perspectives for future robotic solutions.},
	number = {MAY},
	journal = {Frontiers in Neurorobotics},
	author = {Beckerle, Philipp and Salvietti, Gionata and Unal, Ramazan and Prattichizzo, Domenico and Rossi, Simone and Castellini, Claudio and Hirche, Sandra and Endo, Satoshi and Amor, Heni Ben and Ciocarlie, Matei and Mastrogiovanni, Fulvio and Argall, Brenna D. and Bianchi, Matteo},
	year = {2017},
	keywords = {Affective computing, Assistive and rehabilitation robotics, Functional assessment, Human Experiment, Human-Robot Interaction(HRI), Human-oriented design, Human-robot interaction, Learning and control, Review Paper, Sensory feedback},
	pages = {1--6},
}

@article{thomaschke_visuomotor_2018,
	title = {Visuomotor and motorvisual priming with different types of set-level congruency: evidence in support of ideomotor theory, and the planning and control model ({PCM})},
	volume = {82},
	issn = {14302772},
	doi = {10.1007/s00426-017-0885-3},
	abstract = {Perception can prime action (visuomotor priming), and action can prime perception (motorvisual priming). According to ideomotor theory both effects rely on the overlap of mental representations between perception and action. This implies that both effects get more pronounced the more features they share. We tested this hypothesis by employing in a motorvisual (Exp. 1) and in a visuomotor (Exp. 2) setting, three different pairs of left/right target stimuli (hand pictures, arrows, and words) varying in how strongly they overlap with the pair of left/right responses. For two stimulus pairs (hands and words) the hypothesis was confirmed: hand pictures share more features with the responses than words, consequently hand pictures produced a stronger visuomotor and a stronger motorvisual priming effect than words. However, arrow stimuli showed a different pattern: the temporal dynamics of both priming effects, as well as the direction of the effect seen in motorvisual priming, were significant but opposite to that of the hand and word stimuli. This suggests that the arrows’ representations were not involved in ideomotor processes, and we propose instead that they were represented in a spatial or scalar fashion, outside the representations assumed in ideomotor theory. The results are discussed in the context of ideomotor theory, and the planning and control model of motorvisual priming.},
	number = {6},
	journal = {Psychological Research},
	author = {Thomaschke, Roland and Miall, R. Christopher and Rueß, Miriam and Mehta, Puja R. and Hopkins, Brian},
	year = {2018},
	note = {Publisher: Springer Berlin Heidelberg},
	keywords = {Brain, Motion, Vision},
	pages = {1073--1090},
}

@article{javdani_minimizing_2016,
	title = {Minimizing user cost for shared autonomy},
	volume = {2016-April},
	issn = {21672148},
	doi = {10.1109/HRI.2016.7451886},
	abstract = {In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. One often used strategy considers the user and autonomy as independent decision makers, with the system blending these decisions. However, this independence leads to suboptimal, and often frustrating, behavior. Instead, we propose a system that explicitly models the interplay between the user and assistance. Our approach centers around the idea of learning how users respond to assistance. We then propose a cost minimization framework for assisting while utilizing this learned model.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Javdani, Shervin and Bagnell, J. Andrew and Srinivasa, Siddhartha S.},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781467383707},
	keywords = {Markov, Robotics},
	pages = {621--622},
}

@article{johnson_tomorrows_2018,
	title = {Tomorrow’s {Human}–{Machine} {Design} {Tools}: {From} {Levels} of {Automation} to {Interdependencies}},
	volume = {12},
	issn = {21695032},
	doi = {10.1177/1555343417736462},
	abstract = {The growth of sophistication in machine capabilities must go hand in hand with growth of sophistication in human–machine interaction capabilities. To continue advancement as we build today’s intelligent machines, designers need formative tools for creating sociotechnical systems. In this article, we will briefly assess the appropriateness of “levels of automation” as a tool for designing human–machine systems. Additionally, we present coactive design and interdependence analysis as a viable alternative tool moving forward into more advanced and sophisticated human–machine systems.},
	number = {1},
	journal = {Journal of Cognitive Engineering and Decision Making},
	author = {Johnson, Matthew and Bradshaw, Jeffrey M. and Feltovich, Paul J.},
	year = {2018},
	note = {ISBN: 1555343417736},
	keywords = {Automation, Human, cognitive engineering, collaboration, design methods, human–automation interaction, human–computer interaction, human–robot interaction, level of automation, team design},
	pages = {77--82},
}

@article{knoblich_evolving_2008,
	title = {Evolving intentions for social interaction: {From} entrainment to joint action},
	volume = {363},
	issn = {09628452},
	doi = {10.1098/rstb.2008.0006},
	abstract = {This article discusses four different scenarios to specify increasingly complex mechanisms that enable increasingly flexible social interactions. The key dimension on which these mechanisms differ is the extent to which organisms are able to process other organisms' intentions and to keep them apart from their own. Drawing on findings from ecological psychology, scenario 1 focuses on entrainment and simultaneous affordance in 'intentionally blind' individuals. Scenario 2 discusses how an interface between perception and action allows observers to simulate intentional action in others. Scenario 3 is concerned with shared perceptions, arising through joint attention and the ability to distinguish between self and other. Scenario 4 illustrates how people could form intentions to act together while simultaneously distinguishing between their own and the other's part of a joint action. The final part focuses on how combining the functionality of the four mechanisms can explain different forms of social interactions. It is proposed that basic interpersonal processes are put to service by more advanced functions that support the type of intentionality required to engage in joint action, cultural learning, and communication.},
	number = {1499},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Knoblich, Günther and Sebanz, Natalie},
	year = {2008},
	keywords = {Action understanding, Communication, Evolution of social interaction, Human-Human Interaction (HHI), Imitation, Intention, Joint action, Joint-action, Social cognitive neuroscience, Tool use},
	pages = {2021--2031},
}

@article{kuniyoshi_learning_1994,
	title = {Learning by {Watching}: {Extracting} {Reusable} {Task} {Knowledge} from {Visual} {Observation} of {Human} {Performance}},
	volume = {10},
	issn = {1042296X},
	doi = {10.1109/70.338535},
	abstract = {A novel task instruction method for future intelligent robots is presented, In our method, a robot learns reusable task plans by watching a human perform assembly tasks. Functional units and working algorithms for visual recognition and analysis of human action sequences are presented. The overall system is model based and integrated at the symbolic level. Temporal segmentation of a continuous task performance into meaningful units and identification of each operation is processed in real time by concurrent recognition processes under active attention control. Dependency among assembly operations in the recognized action sequence is analyzed, which results in a hierarchical task plan describing the higher level structure of the task. In another workspace with a different initial state, the system re-instantiates and executes the task plan to accomplish an equivalent goal. The effectiveness of our method is supported by experimental results with block assembly tasks},
	number = {6},
	journal = {IEEE Transactions on Robotics and Automation},
	author = {Kuniyoshi, Yasuo and Inaba, Masayuki and Inoue, Hirochika},
	year = {1994},
	keywords = {Human Demonstration, Human Experiment, Robotics, Vision},
	pages = {799--822},
}

@article{frith_role_2012,
	title = {The role of metacognition in human social interactions},
	volume = {367},
	issn = {14712970},
	doi = {10.1098/rstb.2012.0123},
	abstract = {Metacognition concerns the processes by which we monitor and control our own cognitive processes. It can also be applied to others, in which case it is known as mentalizing. Both kinds of metacognition have implicit and explicit forms, where implicit means automatic and without awareness. Implicit metacognition enables us to adopt a we-mode, through which we automatically take account of the knowledge and intentions of others. Adoption of this mode enhances joint action. Explicit metacognition enables us to reflect on and justify our behaviour to others. However, access to the underlying processes is very limited for both self and others and our reports on our own and others' intentions can be very inaccurate. On the other hand, recent experiments have shown that, through discussions of our perceptual experiences with others, we can detect sensory signals more accurately, even in the absence of objective feedback. Through our willingness to discuss with others the reasons for our actions and perceptions, we overcome our lack of direct access to the underlying cognitive processes. This creates the potential for us to build more accurate accounts of the world and of ourselves. I suggest, therefore, that explicit metacognition is a uniquely human ability that has evolved through its enhancement of collaborative decision-making. © 2012 The Royal Society.},
	number = {1599},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Frith, Chris D.},
	year = {2012},
	keywords = {Action understanding, Control, Human Activity, Mentalizing, Metacognition, Monitoring, Prefrontal cortex, Robotics, We-mode},
	pages = {2213--2223},
}

@article{viviani_minimum-jerk_1995,
	title = {Minimum-{Jerk}, {Two}-{Thirds} {Power} {Law}, and {Isochrony}: {Converging} {Approaches} to {Movement} {Planning}},
	volume = {21},
	issn = {00961523},
	doi = {10.1037/0096-1523.21.1.32},
	abstract = {Two approaches to the study of movement planning were contrasted. Data on the drawing of complex two-dimensional trajectories were used to test whether the covariations of the kinematic and geometrical parameters of the movement formalized by the two-thirds power law and by the isochrony principle (P. Viviani \& R. Schneider, 1991) can be derived from the minimum-jerk model hypothesis (T. Flash \& N. Hogan, 1985). The convergence of the 2 approaches was satisfactory insofar as the relation between tangential velocity and curvature is concerned (two-thirds power law). Global isochrony could not be deduced from the optimal control hypothesis. Scaling of velocity within movement subunits can instead be derived from the minimum-jerk hypothesis. The implications vis-à-vis the issue of movement planning are discussed with an emphasis on the representation used by the motor control system for coding the intended trajectories.},
	number = {1},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Viviani, Paolo and Flash, Tamar},
	year = {1995},
	keywords = {Human Demonstration, Human Experiment, Modelling, Motion},
	pages = {32--53},
}

@article{sciutti_humanizing_2018,
	title = {Humanizing human-robot interaction: {On} the importance of mutual understanding},
	volume = {37},
	issn = {02780097},
	doi = {10.1109/MTS.2018.2795095},
	abstract = {© 1982-2012 IEEE. In conjunction with what is often called the industry 4.0, the new machine age, or the rise of the robots, the authors of this paper have each experienced the following phenomenon. At public events and roundtable discussions, among our circles of friends, or during interviews with the media, we are asked on a surprisingly regular basis: How must humankind adapt to the imminent process of technological change? What do we have to learn in order to keep pace with the smart new machines? What new skills do we need to understand the robots?.},
	number = {1},
	journal = {IEEE Technology and Society Magazine},
	author = {Sciutti, Alessandra and Mara, Martina and Tagliasco, Vincenzo and Sandini, Giulio},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Human Experiment, Human-Robot Interaction(HRI), Robotics},
	pages = {22--29},
}

@article{ewerton_anticipative_2016,
	title = {Anticipative {Interaction} {Primitives} for {Human}-{Robot} {Collaboration}},
	abstract = {This paper introduces our initial investigation on the prob-lem of providing a semi-autonomous robot collaborator with anticipative capabilities to predict human actions. Anticipa-tive robot behavior is a desired characteristic of robot collab-orators that lead to fluid, proactive interactions. We are par-ticularly interested in improving reactive methods that rely on human action recognition to activate the corresponding robot action. Action recognition invariably causes delay in the robot's response, and the goal of our method is to elimi-nate this delay by predicting the next human action. Predic-tion is achieved by using a lookup table containing variations of assembly sequences, previously demonstrated by differ-ent users. The method uses the nearest neighbor sequence in the table that matches the actual sequence of human ac-tions. At the movement level, our method uses a probabilis-tic representation of interaction primitives to generate robot trajectories. The method is demonstrated using a 7 degree-of-freedom lightweight arm equipped with a 5-finger hand on an assembly task consisting of 17 steps.},
	journal = {2016 AAAI Fall Symposium Series: Shared Autonomy in Research and Practice},
	author = {Ewerton, Marco and Maeda, Guilherme and Rother, David and Weimar, Jakob and Kollegger, Gerrit and Wiemeyer, Josef and Peters, Jan},
	year = {2016},
	note = {ISBN: 9781577357759},
	keywords = {Action Recognition, Anticipation, Human-Robot Interaction(HRI), Primitives, Technical Report FS-16-05},
	pages = {325--330},
}

@article{elsner_infants_2014,
	title = {Infants' online perception of give-and-take interactions},
	volume = {126},
	issn = {00220965},
	url = {http://dx.doi.org/10.1016/j.jecp.2014.05.007},
	doi = {10.1016/j.jecp.2014.05.007},
	abstract = {This research investigated infants' online perception of give-me gestures during observation of a social interaction. In the first experiment, goal-directed eye movements of 12-month-olds were recorded as they observed a give-and-take interaction in which an object is passed from one individual to another. Infants' gaze shifts from the passing hand to the receiving hand were significantly faster when the receiving hand formed a give-me gesture relative to when it was presented as an inverted hand shape. Experiment 2 revealed that infants' goal-directed gaze shifts were not based on different affordances of the two receiving hands. Two additional control experiments further demonstrated that differences in infants' online gaze behavior were not mediated by an attentional preference for the give-me gesture. Together, our findings provide evidence that properties of social action goals influence infants' online gaze during action observation. The current studies demonstrate that infants have expectations about well-formed object transfer actions between social agents. We suggest that 12-month-olds are sensitive to social goals within the context of give-and-take interactions while observing from a third-party perspective. © 2014 The Authors.},
	journal = {Journal of Experimental Child Psychology},
	author = {Elsner, Claudia and Bakker, Marta and Rohlfing, Katharina and Gredebäck, Gustaf},
	year = {2014},
	note = {Publisher: Elsevier Inc.},
	keywords = {Action understanding, Affordances, Anticipation, Eye movement, Gaze, Gesture, Give-me gesture, Infant, Infants, Social interaction},
	pages = {280--294},
}

@article{menoret_neural_2014,
	title = {Neural correlates of non-verbal social interactions: {A} dual-{EEG} study},
	volume = {55},
	issn = {18733514},
	url = {http://dx.doi.org/10.1016/j.neuropsychologia.2013.10.001},
	doi = {10.1016/j.neuropsychologia.2013.10.001},
	abstract = {Successful non-verbal social interaction between human beings requires dynamic and efficient encoding of others' gestures. Our study aimed at identifying neural markers of social interaction and goal variations in a non-verbal task. For this, we recorded simultaneously the electroencephalogram from two participants (dual-EEG), an actor and an observer, and their arm/hand kinematics in a real face-to-face paradigm. The observer watched "biological actions" performed by the human actor and "non-biological actions" performed by a robot. All actions occurred within an interactive or non-interactive context depending on whether the observer had to perform a complementary action or not (e.g., the actor presents a saucer and the observer either places the corresponding cup or does nothing). We analysed the EEG signals of both participants (i.e., beta ({\textasciitilde}20. Hz) oscillations as an index of cortical motor activity and motor related potentials (MRPs)). We identified markers of social interactions by synchronising EEG to the onset of the actor's movement. Movement kinematics did not differ in the two context conditions and the MRPs of the actor were similar in the two conditions. For the observer, however, an observation-related MRP was measured in all conditions but was more negative in the interactive context over fronto-central electrodes. Moreover, this feature was specific to biological actions. Concurrently, the suppression of beta oscillations was observed in the actor's EEG and the observer's EEG rapidly after the onset of the actor's movement. Critically, this suppression was stronger in the interactive than in the non-interactive context despite the fact that movement kinematics did not differ in the two context conditions. For the observer, this modulation was observed independently of whether the actor was a human or a robot. Our results suggest that acting in a social context induced analogous modulations of motor and sensorimotor regions in observer and actor. Sharing a common goal during an interaction seems thus to evoke a common representation of the global action that includes both actor and observer movements. © 2013 Elsevier Ltd.},
	number = {1},
	journal = {Neuropsychologia},
	author = {Ménoret, Mathilde and Varnet, Léo and Fargier, Raphaël and Cheylus, Anne and Curie, Aurore and des Portes, Vincent and Nazir, Tatjana A. and Paulignan, Yves},
	year = {2014},
	note = {Publisher: Elsevier},
	keywords = {Dual-EEG, EEG, Human-Human Interaction (HHI), Kinematics, Motor system, Mu rhythm, Object Placement, Observation, Social interaction},
	pages = {85--97},
}

@article{konvalinka_two-brain_2012,
	title = {The two-brain approach: how can mutually interacting brains teach us something about social interaction?},
	volume = {6},
	doi = {10.3389/fnhum.2012.00215},
	abstract = {Measuring brain activity simultaneously from two people interacting is intuitively appealing if one is interested in putative neural markers of social interaction. However, given the complex nature of interactions, it has proven difficult to carry out two-person brain imaging experiments in a methodologically feasible and conceptually relevant way. Only a small number of recent studies have put this into practice, using fMRI, EEG, or NIRS. Here, we review two main two-brain methodological approaches, each with two conceptual strategies. The first group has employed two-brain fMRI recordings, studying (1) turn-based interactions on the order of seconds, or (2) pseudo-interactive scenarios, where only one person is scanned at a time, investigating the flow of information between brains. The second group of studies has recorded dual EEG/NIRS from two people interacting, in (1) face-to-face turn-based interactions, investigating functional connectivity between theory-of-mind regions of interacting partners, or in (2) continuous mutual interactions on millisecond timescales, to measure coupling between the activity in one person's brain and the activity in the other's brain. We discuss the questions these approaches have addressed, and consider scenarios when simultaneous two-brain recordings are needed. Furthermore, we suggest that (1) quantification of inter-personal neural effects via measures of emergence, and (2) multivariate decoding models that generalize source-specific features of interaction, may provide novel tools to study brains in interaction. This may allow for a better understanding of social cognition as both representation and participation.},
	number = {July},
	journal = {Frontiers in Human Neuroscience},
	author = {Konvalinka, Ivana and Roepstorff, Andreas},
	year = {2012},
	keywords = {Brain, EEG, Human-Human Interaction (HHI), Neuroscience, decoding, dual EEG, dual eeg, hyperscanning, interpersonal analysis, social interaction},
	pages = {1--10},
}

@article{kurniawan_choosing_2010,
	title = {Choosing to {Make} an {Effort}: {The} {Role} of {Striatum} in {Signaling} {Physical} {Effort} of a {Chosen} {Action}},
	volume = {104},
	issn = {0022-3077},
	url = {http://www.physiology.org/doi/10.1152/jn.00027.2010},
	doi = {10.1152/jn.00027.2010},
	abstract = {The possibility that we will have to invest effort influences our future choice behavior. Indeed deciding whether an action is actually worth taking is a key element in the expression of human apathy or inertia. There is a well developed literature on brain activity related to the anticipation of effort, but how effort affects actual choice is less well understood. Furthermore, prior work is largely restricted to mental as opposed to physical effort or has confounded temporal with effortful costs. Here we investigated choice behavior and brain activity, using functional magnetic resonance imaging, in a study where healthy participants are required to make decisions between effortful gripping, where the factors of force (high and low) and reward (high and low) were varied, and a choice of merely holding a grip device for minimal monetary reward. Behaviorally, we show that force level influences the likelihood of choosing an effortful grip. We observed greater activity in the putamen when participants opt to grip an option with low effort compared with when they opt to grip an option with high effort. The results suggest that, over and above a nonspecific role in movement anticipation and salience, the putamen plays a crucial role in computations for choice that involves effort costs.},
	number = {1},
	journal = {Journal of Neurophysiology},
	author = {Kurniawan, I. T. and Seymour, B. and Talmi, D. and Yoshida, W. and Chater, N. and Dolan, R. J.},
	year = {2010},
	keywords = {Action understanding, Brain, Neurophysiological, Neuroscience},
	pages = {313--321},
}

@article{Rizzolatti2001,
	title = {Neurophysiological mechanisms underlying the understanding and imitation of action {Giacomo} {Rizzolatti}, {Leonardo} {Fogassi} and {V}},
	volume = {2},
	issn = {1471-003X},
	doi = {10.1038/35090060},
	abstract = {What are the neural bases of action understanding? Although this capacity could merely involve visual analysis of the action, it has been argued that we actually map this visual information onto its motor representation in our nervous system. Here we discuss evidence for the existence of a system, the 'mirror system', that seems to serve this mapping function in primates and humans, and explore its implications for the understanding and imitation of action.},
	number = {September},
	journal = {Nature Neuroscience},
	author = {Rizzolatti, Giacomo and Fogassi, Leonardo and Gallese, Vittorio},
	year = {2001},
	keywords = {Action understanding, Imitation, Mirror neurons, Neurophysiological},
	pages = {1--10},
}

@article{grigore_joint_2013,
	title = {Joint action understanding improves robot-to-human object handover},
	issn = {21530858},
	doi = {10.1109/IROS.2013.6697021},
	abstract = {The development of trustworthy human-assistive robots is a challenge that goes beyond the traditional boundaries of engineering. Essential components of trustworthiness are safety, predictability and usefulness. In this paper we demonstrate that the integration of joint action understanding from human-human interaction into the human-robot context can significantly improve the success rate of robot-to-human object handover tasks. We take a two layer approach. The first layer handles the physical aspects of the handover. The robot's decision to release the object is informed by a Hidden Markov Model that estimates the state of the handover. Inspired by human-human handover observations, we then introduce a higher-level cognitive layer that models behaviour characteristic for a human user in a handover situation. In particular, we focus on the inclusion of eye gaze / head orientation into the robot's decision making. Our results demonstrate that by integrating these non-verbal cues the success rate of robot-to-human handovers can be significantly improved, resulting in a more robust and therefore safer system.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Grigore, Elena Corina and Eder, Kerstin and Pipe, Anthony G. and Melhuish, Chris and Leonards, Ute},
	year = {2013},
	note = {Publisher: IEEE
ISBN: 9781467363587},
	keywords = {Gaze, Handovers, Head-Orientation, Human-Robot Interaction(HRI)},
	pages = {4622--4629},
}

@article{lachat_oscillatory_2012,
	title = {Oscillatory {Brain} {Correlates} of {Live} {Joint} {Attention}: {A} {Dual}-{EEG} {Study}},
	volume = {6},
	doi = {10.3389/fnhum.2012.00156},
	abstract = {Joint attention consists in following another's gaze onto an environmental object, which leads to the alignment of both subjects' attention onto this object. It is a fundamental mechanism of non-verbal communication, and it is essential for dynamic, online, interindividual synchronization during interactions. Here we aimed at investigating the oscillatory brain correlates of joint attention in a face-to-face paradigm where dyads of participants dynamically oriented their attention toward the same or different objects during joint and no-joint attention periods respectively. We also manipulated task instruction: in socially driven instructions, the participants had to follow explicitly their partner's gaze, while in color-driven instructions, the objects to be looked at were designated at by their color so that no explicit gaze following was required. We focused on oscillatory activities in the 10 Hz frequency range, where parieto-occipital alpha and the centro-parietal mu rhythms have been described, as these rhythms have been associated with attention and social coordination processes respectively. We tested the hypothesis of a modulation of these oscillatory activities by joint attention. We used dual-EEG to record simultaneously the brain activities of the participant dyads during our live, face-to-face joint attention paradigm. We showed that joint attention periods - as compared to the no-joint attention periods - were associated with a decrease of signal power between 11 and 13 Hz over a large set of left centro-parieto-occipital electrodes, encompassing the scalp regions where alpha and mu rhythms have been described. This 11-13 Hz signal power decrease was observed independently of the task instruction: it was similar when joint versus no-joint attention situations were socially driven and when they were color-driven. These results are interpreted in terms of the processes of attention mirroring, social coordination, and mutual attentiveness associated with joint attention state.},
	number = {June},
	journal = {Frontiers in Human Neuroscience},
	author = {Lachat, Fanny and Hugueville, Laurent and Lemaréchal, Jean-Didier and Conty, Laurence and George, Nathalie},
	year = {2012},
	keywords = {Brain, EEG, Joint-action, Neurophysiological, alpha, dual-EEG, dual-eeg, joint attention, mu, social coordination, time-frequency analysis},
	pages = {1--12},
}

@article{magrelli_social_2013,
	title = {Social orienting of children with autism to facial expressions and speech: {A} study with a wearable eye-tracker in naturalistic settings},
	volume = {4},
	issn = {16641078},
	doi = {10.3389/fpsyg.2013.00840},
	abstract = {This study investigates attention orienting to social stimuli in children with Autism Spectrum Conditions (ASC) during dyadic social interactions taking place in real-life settings. We study the effect of social cues that differ in complexity and distinguish between social cues produced by facial expressions of emotion and those produced during speech. We record the children's gazes using a head-mounted eye-tracking device and report on a detailed and quantitative analysis of the motion of the gaze in response to the social cues. The study encompasses a group of children with ASC from 2 to 11-years old (n = 14) and a group of typically developing (TD) children (n = 17) between 3 and 6-years old. While the two groups orient overtly to facial expressions, children with ASC do so to a lesser extent. Children with ASC differ importantly from TD children in the way they respond to speech cues, displaying little overt shifting of attention to speaking faces. When children with ASC orient to facial expressions, they show reaction times and first fixation lengths similar to those presented by TD children. However, children with ASC orient to speaking faces slower than TD children. These results support the hypothesis that individuals affected by ASC have difficulties processing complex social sounds and detecting intermodal correspondence between facial and vocal information. It also corroborates evidence that people with ASC show reduced overt attention toward social stimuli.},
	number = {NOV},
	journal = {Frontiers in Psychology},
	author = {Magrelli, Silvia and Jermann, Patrick and Noris, Basilio and Ansermet, François and Hentsch, François and Nadel, Jacqueline and Billard, Aude},
	year = {2013},
	keywords = {Autism spectrum conditions, Eye tracker, Eye-tracking, Facial expressions of emotion, Gaze, Human-Human Interaction (HHI), Overt attention, Social orienting, Speech},
	pages = {1--16},
}

@article{kourtis_predictive_2013,
	title = {Predictive representation of other people's actions in joint action planning: {An} {EEG} study},
	volume = {8},
	issn = {17470919},
	doi = {10.1080/17470919.2012.694823},
	abstract = {It has been postulated that when people engage in joint actions they form internal representations not only of their part of the joint task but of their co-actors' parts of the task as well. However, empirical evidence for this claim is scarce. By means of high-density electroencephalography, this study investigated whether one represents and simulates the action of an interaction partner when planning to perform a joint action. The results showed that joint action planning compared with individual action planning resulted in amplitude modulations of the frontal P3a and parietal P3b event-related potentials, which are associated with stimulus classification, updating of representations, and decision-making. Moreover, there was evidence for anticipatory motor simulation of the partner's action in the amplitude and peak latency of the late, motor part of the Contingent Negative Variation, which was correlated with joint action performance. Our results provide evidence that when people engage in joint tasks, they represent in advance each other's actions in order to facilitate coordination.},
	number = {1},
	journal = {Social Neuroscience},
	author = {Kourtis, D. and Sebanz, N. and Knoblich, G.},
	year = {2013},
	keywords = {Action prediction, EEG, Event-related potentials, Human-Human Interaction (HHI), Joint action, Joint-action, Motor control, Parietal cortex, Prediction},
	pages = {31--42},
}

@article{KOKAL20092046,
	title = {Acting together in and beyond the mirror neuron system},
	volume = {47},
	issn = {10538119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811909006211},
	doi = {https://doi.org/10.1016/j.neuroimage.2009.06.010},
	abstract = {Moving a set dinner table often takes two people, and doing so without spilling the glasses requires the close coordination of the two agents' actions. It has been argued that the mirror neuron system may be the key neural locus of such coordination. Instead, here we show that such coordination recruits two separable sets of areas: one that could translate between motor and visual codes and one that could integrate these information to achieve common goals. The former includes regions of the putative mirror neuron system, the latter, regions of the prefrontal, posterior parietal and temporal lobe adjacent to the putative mirror neuron system. Both networks were more active while participants cooperated with a human agent, responding to their actions, compared to a computer that did not, evidencing their social dimension. This finding shows that although the putative mirror neuron system can play a critical role in joint actions by translating both agents' actions into a common code, the flexible remapping of our own actions with those of others required during joint actions seems to be performed outside of the putative mirror neuron system. © 2009 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {NeuroImage},
	author = {Kokal, Idil and Gazzola, Valeria and Keysers, Christian},
	year = {2009},
	pmid = {19524043},
	note = {Publisher: Elsevier Inc.
ISBN: 1095-9572 (Electronic){\textbackslash}n1053-8119 (Linking)},
	keywords = {Action Recognition, Neuroscience},
	pages = {2046--2056},
}

@article{zhao_people_2016,
	title = {Do people spontaneously take a robot's visual perspective?},
	volume = {2016-April},
	issn = {21672148},
	doi = {10.1109/HRI.2016.7451770},
	abstract = {This study takes a novel approach to the topic of perspective taking in HRI. In a human behavioral experiment, we examined whether and in what circumstances people spontaneously take a humanoid robot’s visual perspective. We found that specific nonverbal behaviors displayed by a robot—namely, referential gaze and goal-directed reaching—led human viewers to take the robot’s visual perspective, though marginally less frequently than when they encounter the same behaviors displayed by another human. This project identifies specific features of robot behavior that trigger spontaneous social-cognitive processes in human viewers and informs the design of interactive robots in the future.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Zhao, Xuan and Cusimano, Corey and Malle, Bertram F.},
	year = {2016},
	note = {ISBN: 9781467383707},
	keywords = {Action understanding, Communication, Human-Robot Interaction(HRI), Human-robot interaction (HRI), Humanoid robot, Mind perception, Nonverbal behaviors, Perspective taking, Robotics, Vision},
	pages = {335--342},
}

@article{wang_p300_2015,
	title = {P300 and {Decision} {Making} under {Risk} and {Ambiguity}},
	volume = {2015},
	issn = {1687-5265},
	doi = {10.1155/2015/108417},
	abstract = {Our study aims to contrast the neural temporal features of early stage of decision making in the context of risk and ambiguity. In monetary gambles under ambiguous or risky conditions, 12 participants were asked to make a decision to bet or not, with the event-related potentials (ERPs) recorded meantime. The proportion of choosing to bet in ambiguous condition was significantly lower than that in risky condition. An ERP component identified as P300 was found. The P300 amplitude elicited in risky condition was significantly larger than that in ambiguous condition. The lower bet rate in ambiguous condition and the smaller P300 amplitude elicited by ambiguous stimuli revealed that people showed much more aversion in the ambiguous condition than in the risky condition. The ERP results may suggest that decision making under ambiguity occupies higher working memory and recalls more past experience while decision making under risk mainly mobilizes attentional resources to calculate current information. These findings extended the current understanding of underlying mechanism for early assessment stage of decision making and explored the difference between the decision making under risk and ambiguity.},
	journal = {Computational Intelligence and Neuroscience},
	author = {Wang, Lei and Zheng, Jiehui and Huang, Shenwei and Sun, Haoye},
	year = {2015},
	note = {Publisher: Hindawi Publishing Corporation},
	keywords = {Brain, EEG, Human Activity, Human-Computer Interaction, Neuroscience},
	pages = {1--7},
}

@article{chen_decentralized_2017,
	title = {Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989037},
	abstract = {Finding feasible, collision-free paths for multiagent systems can be challenging, particularly in non-communicating scenarios where each agent's intent (e.g. goal) is unobservable to the others. In particular, finding time efficient paths often requires anticipating interaction with neighboring agents, the process of which can be computationally prohibitive. This work presents a decentralized multiagent collision avoidance algorithm based on a novel application of deep reinforcement learning, which effectively offloads the online computation (for predicting interaction patterns) to an offline learning procedure. Specifically, the proposed approach develops a value network that encodes the estimated time to the goal given an agent's joint configuration (positions and velocities) with its neighbors. Use of the value network not only admits efficient (i.e., real-time implementable) queries for finding a collision-free velocity vector, but also considers the uncertainty in the other agents' motion. Simulation results show more than 26 percent improvement in paths quality (i.e., time to reach the goal) when compared with optimal reciprocal collision avoidance (ORCA), a state-of-the-art collision avoidance strategy.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Chen, Yu Fan and Liu, Miao and Everett, Michael and How, Jonathan P.},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509046331},
	keywords = {Inverse reinforcement Learning, Joint-action, Markov, Motion, Robotics},
	pages = {285--292},
}

@article{pedemonte_visual-based_2017,
	title = {Visual-based shared control for remote telemanipulation with integral haptic feedback},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989628},
	abstract = {Nowadays, one of the largest environmental challenges that European countries must face consists in dealing with the past half century of nuclear waste. In order to optimize maintenance costs, nuclear waste must be sorted, segregated and stored according to its radiation level. Towards this end, in [1] we have recently proposed a visual-based shared control architecture meant to facilitate a human operator in controlling two remote robotic arms (one equipped with a gripper and another with a camera) during remote manipulation tasks of nuclear waste via a master device. The operator could then receive force cues informative of the feasibility of her/his motion commands during the task execution. The strategy presented in [1], albeit effective, suffers however from a locality issue since the operator can only provide instantaneous velocity commands (in a suitable task space), and receive instantaneous force feedback cues. On the other hand, the ability to `steer' a whole future trajectory in task space, and to receive a corresponding integral force feedback along the whole planned trajectory (because of any constraint of the considered system), could significantly enhance the operator's performance, especially when dealing with complex manipulation tasks. The aim of this work is to then extend [1] towards a planning-based shared control architecture able to take into account the mentioned requirements. A human/hardware-in-the-loop experiment with simulated slave robots and a real master device is reported for demonstrating the feasibility and effectiveness of the proposed approach.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Pedemonte, Nicolo and Abi-Farraj, Firas and Giordano, Paolo Robuffo},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509046331},
	keywords = {Human Demonstration, Motion, Pose, Robotics, Vision},
	pages = {5342--5349},
}

@article{abi-farraj_learning-based_2017,
	title = {A learning-based shared control architecture for interactive task execution},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989042},
	abstract = {Shared control is a key technology for various robotic applications in which a robotic system and a human operator are meant to collaborate efficiently. In order to achieve efficient task execution in shared control, it is essential to predict the desired behavior for a given situation or context to simplify the control task for the human operator. To do this prediction, we use Learning from Demonstration (LfD), which is a popular approach for transferring human skills to robots.We encode the demonstrated behavior as trajectory distributions and generalize the learned distributions to new situations. The goal of this paper is to present a shared control framework that uses learned expert distributions to gain more autonomy. Our approach controls the balance between the controller’s autonomy and the human preference based on the distribu- tions of the demonstrated trajectories. Moreover, the learned distributions are autonomously refined from collaborative task executions, resulting in a master-slave system with increasing autonomy that requires less user input with an increasing number of task executions. We experimentally validated that our shared control approach enables efficient task executions. Moreover, the conducted experiments demonstrated that the developed system improves its performances through interactive task executions with our shared control},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Abi-Farraj, Firas and Osa, Takayuki and Pedemonte, Nicolo and Peters, Jan and Neumann, Gerhard and Giordano, Paolo Robuffo},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509046331},
	keywords = {Human Demonstration, Motion, Robotics},
	pages = {329--335},
}

@article{herlant_assistive_2016,
	title = {Assistive teleoperation of robot arms via automatic time-optimal mode switching},
	volume = {2016-April},
	issn = {21672148},
	doi = {10.1109/HRI.2016.7451731},
	abstract = {Assistive robotic arms are increasingly enabling users with upper extremity disabilities to perform activities of daily living on their own. However, the increased capability and dexterity of the arms also makes them harder to control with simple, low-dimensional interfaces like joysticks and sip-and-puff interfaces. A common technique to control a high-dimensional system like an arm with a low-dimensional input like a joystick is through switching between multiple control modes. However, our interviews with daily users of the Kinova JACO arm identified mode switching as a key problem, both in terms of time and cognitive load. We further confirmed objectively that mode switching consumes about 17.4\% of execution time even for able-bodied users controlling the JACO. Our key insight is that using even a simple model of mode switching, like time optimality, and a simple intervention, like automatically switching modes, significantly improves user satisfaction.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Herlant, Laura V. and Holladay, Rachel M. and Srinivasa, Siddhartha S.},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781467383707},
	keywords = {Human Activity, Human Demonstration, Robotics},
	pages = {35--42},
}

@article{pesce_ibarra_synchronization_2017,
	title = {Synchronization matters for motor coordination},
	volume = {119},
	issn = {0022-3077},
	doi = {10.1152/jn.00182.2017},
	abstract = {Using electroencephalography and electromyography recordings from healthy participants during a visual-depended bimanual coordination task, de Vries and colleagues showed that functional synchronization is important in motor coordination. The authors reported that higher coordination correlated positively with intermuscular synchrony, but correlated negatively with corticomuscular synchrony. They proposed that these two diverse motor systems operate differently depending on task demands. Similar experimental paradigms could identify motor mechanisms in patients with neurological disorders to design novel rehabilitation strategies.},
	number = {3},
	journal = {Journal of Neurophysiology},
	author = {Pesce Ibarra, Luigi S.},
	year = {2017},
	keywords = {Human-Human Interaction (HHI), Motion, Neuroscience},
	pages = {767--770},
}

@article{sanghvi_modeling_2019,
	title = {Modeling {Social} {Group} {Communication} with {Multi}-{Agent} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1903.01537},
	abstract = {In crowded social scenarios with a myriad of external stimuli, human brains exhibit a natural ability to filter out irrelevant information and narrowly focus their attention. In the midst of multiple groups of people, humans use such sensory gating to effectively further their own group's interactional goals. In this work, we consider the design of a policy network to model multi-group multi-person communication. Our policy takes as input the state of the world such as an agent's gaze direction, body pose of other agents or history of past actions, and outputs an optimal action such as speaking, listening or responding (communication modes). Inspired by humans' natural neurobiological filtering process, a central component of our policy network design is an information gating function, termed the Kinesic-Proxemic-Message Gate (KPM-Gate), that models the ability of an agent to selectively gather information from specific neighboring agents. The degree of influence of a neighbor is based on dynamic non-verbal cues such as body motion, head pose (kinesics) and interpersonal space (proxemics). We further show that the KPM-Gate can be used to discover social groups using its natural interpretation as a social attention mechanism. We pose the communication policy learning problem as a multi-agent imitation learning problem. We learn a single policy shared by all agents under the assumption of a decentralized Markov decision process. We term our policy network as the Multi-Agent Group Discovery and Communication Mode Network (MAGDAM network), as it learns social group structure in addition to the dynamics of group communication. Our experimental validation on both synthetic and real world data shows that our model is able to both discover social group structure and learn an accurate multi-agent communication policy.},
	number = {1},
	author = {Sanghvi, Navyata and Yonetani, Ryo and Kitani, Kris},
	year = {2019},
	note = {arXiv: 1903.01537},
	keywords = {Human-Human Interaction (HHI), Imitation, Markov},
}

@article{hu_brain--brain_2017,
	title = {Brain-to-brain synchronization across two persons predicts mutual prosociality},
	volume = {12},
	issn = {17495024},
	doi = {10.1093/scan/nsx118},
	abstract = {People tend to be more prosocial after synchronizing behaviors with others, yet the underlying neural mechanisms are rarely known. In this study, participant dyads performed either a coordination task or an independence task, with their brain activations recorded via the functional near-infrared spectroscopy hyperscanning technique. Participant dyads in the coordination group showed higher synchronized behaviors and greater subsequent inclination to help each other than those in the independence group, indicating the prosocial effect of interpersonal synchrony. Importantly, the coordination group demonstrated the significant task-related brain coherence, namely the interbrain synchronization, at the left middle frontal area. The detected interbrain synchronization was sensitive to shared intentionality between participants and was correlated with the mutual prosocial inclination. Further, the task-related brain coherence played a mediation role in the prosocial effect of interpersonal synchrony. This study reveals the relevance of brain-to-brain synchronization among individuals with subsequent mutual prosocial inclination and suggests the neural mechanism associating with shared cognition for the facilitation of interpersonal synchrony on prosociality.},
	number = {12},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Hu, Yi and Hu, Yinying and Li, Xianchun and Pan, Yafeng and Cheng, Xiaojun},
	year = {2017},
	keywords = {Brain, Cognition, EEG, Functional near-infrared spectroscopy, Human-Human Interaction (HHI), Interbrain synchronization, Interpersonal synchrony, Neuroscience, Prosociality, Shared intentionality},
	pages = {1835--1844},
}

@article{chan_characterization_2015,
	title = {Characterization of {Handover} {Orientations} used by {Humans} for {Efficient} {Robot} to {Human} {Handovers}},
	doi = {10.1109/IROS.2015.7353106},
	journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Chan, Wesley P and Pan, Matthew K X J and Croft, Elizabeth A and Inaba, Masayuki},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781479999941},
	keywords = {Handovers, Human Experiment, Object, Robotics},
	pages = {1--6},
}

@article{peternel_robot_2018,
	title = {Robot adaptation to human physical fatigue in human – robot co-manipulation},
	volume = {42},
	issn = {1573-7527},
	doi = {10.1007/s10514-017-9678-1},
	number = {5},
	journal = {Autonomous Robots},
	author = {Peternel, Luka and Tsagarakis, Nikos and Caldwell, Darwin and Ajoudani, Arash},
	year = {2018},
	note = {Publisher: Springer US},
	keywords = {EMG, Human Activity, Human-Human Interaction (HHI), Physical human–robot collaboration,Human fatigue,R, Primitives, fatigue, human, physical human, robot collaboration, robot interface, robot learning},
	pages = {1011--1021},
}

@article{bambach_egocentric_nodate,
	title = {An {Egocentric} {Perspective} on {Active} {Vision} and {Visual} {Object} {Learning} in {Toddlers}},
	author = {Bambach, Sven and Crandall, David J and Smith, Linda B and Yu, Chen},
	keywords = {Eye tracker, Eyes, Human-Human Interaction (HHI), Vision},
}

@article{nikolaidis_game-theoretic_2017,
	title = {Game-{Theoretic} {Modeling} of {Human} {Adaptation} in {Human}-{Robot} {Collaboration}},
	url = {http://arxiv.org/abs/1701.07790%0Ahttp://dx.doi.org/10.1145/2909824.3020253},
	doi = {10.1145/2909824.3020253},
	abstract = {In human-robot teams, humans often start with an inaccurate model of the robot capabilities. As they interact with the robot, they infer the robot's capabilities and partially adapt to the robot, i.e., they might change their actions based on the observed outcomes and the robot's actions, without replicating the robot's policy. We present a game-theoretic model of human partial adaptation to the robot, where the human responds to the robot's actions by maximizing a reward function that changes stochastically over time, capturing the evolution of their expectations of the robot's capabilities. The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has. We prove that under certain observability assumptions, the optimal policy can be computed efficiently. We demonstrate through a human subject experiment that the proposed model significantly improves human-robot team performance, compared to policies that assume complete adaptation of the human to the robot.},
	author = {Nikolaidis, Stefanos and Nath, Swaprava and Procaccia, Ariel D. and Srinivasa, Siddhartha},
	year = {2017},
	note = {arXiv: 1701.07790
ISBN: 9781450343367},
	keywords = {Human Activity, Human-Robot Interaction(HRI), Modelling},
	pages = {323--331},
}

@article{keeffe_improving_2016,
	title = {Improving {Task} {Performance} through {High} {Level} {Shared} {Control} of {Multiple} {Robots} with a {Context} {Aware} {Human}-{Robot} {Interface}},
	doi = {10.1109/ICARSC.2016.45},
	journal = {2016 International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
	author = {Keeffe, Simon O and Ward, Tomas E and Villing, Rudi},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509022557},
	keywords = {Human-Robot Interaction(HRI), Robotics, allocation authority, attend to other robots, for a time before, human-in-the-loop, human-robot interface, interacting with each robot, multi-robot, neglecting it to, shared control},
	pages = {277--282},
}

@article{chandrasekaran_human-robot_2015,
	title = {Human-robot collaboration: {A} survey},
	volume = {2015-June},
	issn = {07347502},
	doi = {10.1109/SECON.2015.7132964},
	abstract = {Human-Machine collaboration is a vastly developing field in the area of Robotics. This paper introduces the concept of such collaboration and describes its use in various facets of our society. The various kinds of interaction between humans and robots in applications such as elderly care, schools and education, medicine, military and space exploration have been reviewed in this paper. Also, the learning process used by the robot for interacting with the human and environment is presented.},
	number = {June},
	journal = {Conference Proceedings - IEEE SOUTHEASTCON},
	author = {Chandrasekaran, Balasubramaniyan and Conrad, James M.},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781467373005},
	keywords = {Human-Robot Interaction(HRI), Review Paper, Robotics, human robot interaction (HRI), man-machine cooperation/collaboration, mobile robotics, tele robotics},
	pages = {1--8},
}

@article{sheridan_human-robot_2016,
	title = {Human-{Robot} {Interaction}},
	volume = {58},
	issn = {15478181},
	doi = {10.1177/0018720816644364},
	abstract = {Robots are poised to fill a growing number of roles in today's society, from factory automation to service applications to medical care and entertainment. While robots were initially used in repetitive tasks where all human direction is given a priori, they are becoming involved in increasingly more complex and less structured tasks and activities, including interaction with people required to complete those tasks. This complexity has prompted the entirely new endeavor of Human-Robot Interaction (HRI), the study of how humans interact with robots, and how best to design and implement robot systems capable of accomplishing interactive tasks in human environments. The fundamental goal of HRI is to develop the principles and algorithms for robot systems that make them capable of direct, safe and effective interaction with humans. Many facets of HRI research relate to and draw from insights and principles from psychology, communication, anthropology, philosophy, and ethics, making HRI an inherently interdisciplinary endeavor.},
	number = {4},
	journal = {Human Factors},
	author = {Sheridan, Thomas B.},
	year = {2016},
	keywords = {Human-Robot Interaction(HRI), Modelling, Review Paper, human interaction, research needs, robot, supervisory control, teleoperator, telerobot},
	pages = {525--532},
}

@article{manchester_harmonic:_2018,
	title = {Harmonic: {A} {Multimodal} dataset of {Assistive} {Human}-{Robot} {Collaboration}},
	url = {www.sagepub.com/},
	doi = {10.1177/ToBeAssigned},
	abstract = {Contact constraints arise naturally in many robot planning problems. In recent years, a variety of contact-implicit trajectory optimization algorithms have been developed that avoid the pitfalls of mode pre-specification by simultaneously optimizing state, input, and contact force trajectories. However, their reliance on first-order integrators leads to a linear tradeoff between optimization problem size and plan accuracy. To address this limitation, we propose a new family of trajectory optimization algorithms that leverage ideas from discrete variational mechanics to derive higher-order generalizations of the classic time-stepping method of Stewart and Trinkle. By using these dynamics formulations as constraints in direct trajectory optimization algorithms, it is possible to perform contact-implicit trajectory optimization with significantly higher accuracy. For concreteness, we derive a second-order method and evaluate it using several simulated rigid body systems, including an underactuated biped and a quadruped. In addition, we use this second-order method to plan locomotion trajectories for a complex quadrupedal microrobot. The planned trajectories are evaluated on the physical platform and result in a number of performance improvements.},
	author = {Manchester, Zachary and Doshi, Neel and Wood, Robert J and Kuindersma, Scott},
	year = {2018},
	note = {arXiv: 1807.11154v1},
	keywords = {Contact, Discrete Mechanics, Eyes, Gaze, Human-Robot Interaction(HRI), Microrobots, Motion Planning, Trajectory Optimization},
	pages = {1--6},
}

@article{imre_affordance-based_2019,
	title = {Affordance-based altruistic robotic architecture for human–robot collaboration},
	issn = {17412633},
	doi = {10.1177/1059712318824697},
	abstract = {This article proposes a computational model for altruistic behavior, shows its implementation on a physical robot, and presents the results of human–robot interaction experiments conducted with the implemented system. Inspired from the sensorimotor mechanisms of the primate brain, object affordances are utilized for both intention estimation and action execution, in particular, to generate altruistic behavior. At the core of the model is the notion that sensorimotor systems developed for movement generation can be used to process the visual stimuli generated by actions of the others, infer the goals behind, and take the necessary actions to help achieving these goals, potentially leading to the emergence of altruistic behavior. Therefore, we argue that altruistic behavior is not necessarily a consequence of deliberate cognitive processing but may emerge through basic sensorimotor processes such as error minimization, that is, minimizing the difference between the observed and expected outcomes. In the model, affordances also play a key role by constraining the possible set of actions that an observed actor might be engaged in, enabling a fast and accurate intention inference. The model components are implemented on an upper-body humanoid robot. A set of experiments are conducted validating the workings of the components of the model, such as affordance extraction and task execution. Significantly, to assess how human partners interact with our altruistic model deployed robot, extensive experiments with naïve subjects are conducted. Our results indicate that the proposed computational model can explain emergent altruistic behavior in reference to its biological counterpart and moreover engage human partners to exploit this behavior when implemented on an anthropomorphic robot. © The Author(s) 2019.},
	journal = {Adaptive Behavior},
	author = {Imre, Mert and Oztop, Erhan and Nagai, Yukie and Ugur, Emre},
	year = {2019},
	note = {ISBN: 1059712318},
	keywords = {Affordances, Altruistic behavior, Human-Robot Interaction(HRI), Infants, Robotics, affordances, brain-inspired robotics, computational modeling, goal inference, human–robot interaction},
}

@article{baraglia_initiative_2016,
	title = {Initiative in robot assistance during collaborative task execution},
	volume = {2016-April},
	issn = {21672148},
	doi = {10.1109/HRI.2016.7451735},
	abstract = {It is increasingly apparent that the financial value of a firm depends on off-balance-sheet intangible assets. In this article, the authors focus on the most critical aspect of a firm: its customers. Specifically, they demon- strate how valuing customers makes it feasible to value firms, including firms with negative earnings. The authors define the value of a customer as the expected sum of discounted future earnings. They demonstrate their valuation method by using publicly available data for five firms. They find that a 1\% improvement in retention, margin, or acqui- sition cost improves firm value by 5\%, 1\%, and .1\%, respectively. They also find that a 1\% improvement in retention has almost five times greater high-growth impact on firm value than a 1\% change in discount rate or cost of capital. The results show that the linking of marketing concepts to shareholder value is both possible and insightful},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Baraglia, Jimmy and Cakmak, Maya and Nagai, Yukie and Rao, Rajesh and Asada, Minoru},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781467383707},
	keywords = {Bayesian Network, Human-Robot Interaction(HRI), Kinect, Robotics},
	pages = {67--74},
}

@article{Ivaldi2017,
	title = {Towards {Engagement} {Models} that {Consider} {Individual} {Factors} in {HRI}: {On} the {Relation} of {Extroversion} and {Negative} {Attitude} {Towards} {Robots} to {Gaze} and {Speech} {During} a {Human}--{Robot} {Assembly} {Task}},
	volume = {9},
	issn = {1875-4805},
	url = {https://doi.org/10.1007/s12369-016-0357-8},
	doi = {10.1007/s12369-016-0357-8},
	abstract = {Estimating the engagement is critical for human--robot interaction. Engagement measures typically rely on the dynamics of the social signals exchanged by the partners, especially speech and gaze. However, the dynamics of these signals are likely to be influenced by individual and social factors, such as personality traits, as it is well documented that they critically influence how two humans interact with each other. Here, we assess the influence of two factors, namely extroversion and negative attitude toward robots, on speech and gaze during a cooperative task, where a human must physically manipulate a robot to assemble an object. We evaluate if the score of extroversion and negative attitude towards robots co-variate with the duration and frequency of gaze and speech cues. The experiments were carried out with the humanoid robot iCub and N = 56 adult participants. We found that the more people are extrovert, the more and longer they tend to talk with the robot; and the more people have a negative attitude towards robots, the less they will look at the robot face and the more they will look at the robot hands where the assembly and the contacts occur. Our results confirm and provide evidence that the engagement models classically used in human--robot interaction should take into account attitudes and personality traits.},
	number = {1},
	journal = {International Journal of Social Robotics},
	author = {Ivaldi, Serena and Lefort, Sebastien and Peters, Jan and Chetouani, Mohamed and Provasi, Joelle and Zibetti, Elisabetta},
	month = jan,
	year = {2017},
	note = {arXiv: 1508.04603
Publisher: Springer Netherlands
ISBN: 9781457710056},
	keywords = {Engagement, Gaze, Human-Robot Interaction(HRI), Human–robot interaction, Personality, Robotics, Social signals, Speech},
	pages = {63--86},
}

@article{hansen_humanhuman_2017,
	title = {Human–{Human} {Handover} {Tasks} and {How} {Distance} and {Object} {Mass} {Matter}},
	volume = {124},
	issn = {1558688X},
	doi = {10.1177/0031512516682668},
	abstract = {We investigated the coordination between two individuals during object handovers. Ten participants (eight males, two females; 26.0 AE 5.0 years, 72.7 AE 13.5 kg, 1.73 AE 0.8 m) arranged in pairs (a giver and a receiver), passed an object from the giver to the receiver at a self-selected speed. A motion capture system quantified the giver and the receiver's motion simultaneously. Three interpersonal distances and three object masses were chosen to study the handover. We hypothesized that (a) the handover occurs at half of the interpersonal distance between the giver and receiver and (b) the handover height depends on the objects' mass. Taken together, our results show that the handover strongly depends on the interpersonal distance between the giver and receiver, while object mass related only to handover duration.},
	number = {1},
	journal = {Perceptual and Motor Skills},
	author = {Hansen, Clint and Arambel, Paula and Ben Mansour, Khalil and Perdereau, Véronique and Marin, Frédéric},
	year = {2017},
	keywords = {Handovers, Human-Human Interaction (HHI), Motion, Object, Objects Weights, biomechanics, distances, handover task, human machine interaction, kinematics, motor control, proprioception},
	pages = {182--199},
}

@article{wykowska_embodied_2016,
	title = {Embodied artificial agents for understanding human social cognition},
	volume = {371},
	issn = {0962-8436},
	doi = {10.1098/rstb.2015.0375},
	abstract = {In this paper, we propose that experimental protocols involving artificial agents, in particular the embodied humanoid robots, provide insightful infor- mation regarding social cognitive mechanisms in the human brain. Using artificial agents allows for manipulation and control of various parameters of behaviour, appearance and expressiveness in one of the interaction partners (the artificial agent), and for examining effect of these parameters on the other interaction partner (the human). At the same time, using artificial agents means introducing the presence of artificial, yet human-like, systems into the human social sphere. This allows for testing in a controlled, but ecologi- cally valid, manner human fundamental mechanisms of social cognition both at the behavioural and at the neural level. This paper will reviewexisting literature that reports studies in which artificial embodied agents have been used to study social cognition and will address the question of whether var- ious mechanisms of social cognition (ranging from lower- to higher-order cognitive processes) are evoked by artificial agents to the same extent as by natural agents,humans in particular. Increasing the understanding of howbe- havioural and neural mechanisms of social cognition respond to artificial anthropomorphic agents provides empirical answers to the conundrum ‘What is a social agent?’},
	number = {1693},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Wykowska, Agnieszka and Chaminade, Thierry and Cheng, Gordon},
	year = {2016},
	pmid = {27069052},
	keywords = {Artificial agents, Brain, Gaze, Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Humanoid robots, Human–robot interaction, Mirror neurons, Neuroscience, Robotics, Social cognition, Social interaction, behaviour, cognition, neuroscience},
	pages = {20150375},
}

@article{moreau_hand_2016,
	title = {Hand and grasp selection in a preferential reaching task: {The} effects of object location, orientation, and task intention},
	volume = {7},
	issn = {16641078},
	url = {http://link.springer.com/article/10.1007/BF00227183},
	doi = {10.3389/fpsyg.2016.00360},
	abstract = {As numerous movement options are available in reaching and grasping, of particular interest are what factors influence an individual's choice of action. In the current study a preferential reaching task was used to assess the propensity for right handers to select their preferred hand and grasp a coffee mug by the handle in both independent and joint action object manipulation contexts. Mug location (right-space, midline, and left-space) and handle orientation (toward, away, to left, and to right of the participant) varied in four tasks that differed as a function of intention: (1) pick-up (unimanual, independent); (2) pick-up and pour (bimanual, independent); (3) pick-up and pass (unimanual, joint action); and (4) pick-up, pour and pass (bimanual, joint action). In line with previous reports, a right-hand preference for unimanual tasks was observed. Furthermore, extending existing literature to a preferential reaching task, role differentiation between the hands in bimanual tasks (i.e., preferred hand mobilizing, non-preferred hand stabilizing) was displayed. Finally, right-hand selection was greatest in right space, albeit lower in bimanual tasks compared to what is typically reported in unimanual tasks. Findings are attributed to the desire to maximize biomechanical efficiency in reaching. Grasp postures were also observed to reflect consideration of efficiency. More specifically, within independent object manipulation (pick-up; pick-up and pour) participants only grasped the mug by the handle when it afforded a comfortable posture. Furthermore, in joint action (pick-up and pass; pick-up, pour and pass), the confederate was only offered the handle if the intended action of the confederate was similar or required less effort than that of the participant. Together, findings from the current study add to our knowledge of hand and grasp selection in unimanual and bimanual object manipulation, within the context of both independent and joint action tasks.},
	number = {MAR},
	journal = {Frontiers in Psychology},
	author = {Moreau, Quentin and Galvan, Lucie and Nazir, Tatjana A. and Paulignan, Yves and Scharoun, Sara M. and Scanlan, Kelly A. and Bryden, Pamela J. and Moreau, Quentin and Galvan, Lucie and Nazir, Tatjana A. and Paulignan, Yves and Scharoun, Sara M. and Scanlan, Kelly A. and Bryden, Pamela J.},
	year = {2016},
	pmid = {27014165},
	note = {Publisher: Elsevier Ltd
ISBN: 1471-003X (Print){\textbackslash}r1471-003X (Linking)},
	keywords = {Grasp selection, Hand selection, Object location, Object orientation, Preferential reaching, Task intention, joint action, kinematics and dynamics, motor system, reach-to-grasp, social interactions},
	pages = {1--8},
}

@article{GENTILUCCI199271,
	title = {Temporal coupling between transport and grasp components during prehension movements: effects of visual perturbation},
	volume = {47},
	issn = {0166-4328},
	url = {http://www.sciencedirect.com/science/article/pii/S0166432805802530},
	doi = {https://doi.org/10.1016/S0166-4328(05)80253-0},
	abstract = {The temporal coupling between the transport and grasp components of prehension movements was investigated through two experiments. In Experiment 1, six normal subjects were required to reach and grasp each of three spheres located at three different distances (Blocked trials). In Experiment 2, a visual perturbation paradigm was used in which the location of the object to be reached and grasped could change at the beginning of arm movement (Perturbed trials). The same subjects participated in both experiments. Kinematics of wrist trajectory (transport component) and of distance between thumb and index finger (grasp component) were analyzed. The results of Experiment 1 showed that the two components could be temporally coupled during their time course. In Experiment 2, the visual perturbation affected both the components, but different times were required by each component to reorganize the movement towards the new target. These different times caused the decoupling of those events that appeared synchronized in Experiment 1. Finally, evidence was found to suggest that planning of grip formation takes into account not only the perceived characteristics of the object, but also the time planned by the transport component to reach the object. © 1992 Elsevier Science Publishers B.V. All Rights Reserved.},
	number = {1},
	journal = {Behavioural Brain Research},
	author = {Gentilucci, M. and Chieffi, S. and Scarpa, M. and Castiello, U.},
	year = {1992},
	pmid = {1571102},
	note = {ISBN: 0166-4328 (Print){\textbackslash}r0166-4328},
	keywords = {Arm movement, Grasp, Human, Human-Human Interaction (HHI), Prehension, Transport, Vision, Visual perturbation},
	pages = {71--82},
}

@article{Medina2016,
	title = {A human-inspired controller for fluid human-robot handovers},
	issn = {21640580},
	doi = {10.1109/HUMANOIDS.2016.7803296},
	abstract = {—Handovers are seamless events that occur fre- quently and naturally between people. Although previous works have focused on the design of robot handover controllers synthesizing one of the phases of a handover (approaching, passing, or retracting) in an isolated manner, we take a different approach and treat the handover as a single continuous entity. In this paper we present a novel bi-directional and human- inspired handover controller using insights we learned from human-human experiments. We model the dynamics of the handover in a continuous and time-independent way yielding robust and fluid behavior. We implemented our approach on a robot platform consisting of a 7-DoF robotic arm with a 16- DoF humanoid hand. Our results show that resulting human- robot handovers are smooth and reduce internal forces with the human compared to traditional switching approaches.},
	journal = {IEEE-RAS International Conference on Humanoid Robots},
	author = {Medina, José R. and Duvallet, Felix and Karnam, Murali and Billard, Aude},
	year = {2016},
	note = {ISBN: 9781509047185},
	keywords = {Handovers, Human Demonstration, Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Motion},
	pages = {324--331},
}

@article{sadigh_information_2016,
	title = {Information gathering actions over human internal state},
	volume = {2016-Novem},
	issn = {21530866},
	doi = {10.1109/IROS.2016.7759036},
	abstract = {Much of estimation of human internal state (goal, intentions, activities, preferences, etc.) is passive: an algorithm observes human actions and updates its estimate of human state. In this work, we embrace the fact that robot actions affect what humans do, and leverage it to improve state estimation. We enable robots to do active information gathering, by planning actions that probe the user in order to clarify their internal state. For instance, an autonomous car will plan to nudge into a human driver’s lane to test their driving style. Results in simulation and in a user study suggest that active information gathering significantly outperforms passive state estimation.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Sadigh, Dorsa and Sastry, S. Shankar and Seshia, Sanjit A. and Dragan, Anca},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509037629},
	keywords = {Anticipation, Human Activity, Human Experiment, Prediction},
	pages = {66--73},
}

@article{Sciutti2012,
	title = {Anticipatory gaze in human-robot interactions},
	abstract = {Our interactions with the world and with other individuals are strongly dependent on the way we move our eyes. A par- ticular phenomenon occurring during action observation is proactive gaze behavior: the observer’s gaze anticipates the forthcoming goal of the observed movement rather than re- actively tracking the demonstrator’s motion. Interestingly, the appearance of anticipatory saccades during action obser- vation seems to be tightly connected to a motor resonance mechanism, that is to a direct mapping between the ob- served action and the motor repertoire of the observer. In other words, gaze predictivity would indicate that we have recognized the agent as conspecific, or at least as an inter- action partner who could share our same goals and same ac- tions. With the aim of evaluating the naturalness of human- robot interactions and to test if a robotic model is able to induce motor resonance as a human demonstrator, we com- pared gaze behavior when participants observed a human and the humanoid robot iCub performing a simple object - transport task. Since participants showed a very simi- lar anticipatory gazing pattern when observing the human and the robotic stimuli, we concluded that proactivity in gazing is a very robust mechanism in our brain, which can be evoked also by a robotic model. Moreover, this kind of study introduces the measure of proactive gaze behavior as a powerful tool to understand which elements in the robotic implementation let the robot be perceived as an interactive agent rather than a mechanical tool.},
	number = {January 2014},
	journal = {Gaze in Human-Robot Interaction Workshop},
	author = {Sciutti, Alessandra and Bisio, Ambra and Nori, Francesco and Metta, Giorgio and Fadiga, Luciano and Sandini, Giulio},
	year = {2012},
	keywords = {Anticipation, Motor Resonance, Proactive Gaze},
}

@article{park_i-planner:_2019,
	title = {I-{Planner}: {Intention}-aware motion planning using learning-based human motion prediction},
	volume = {38},
	issn = {17413176},
	doi = {10.1177/0278364918812981},
	abstract = {We present a motion planning algorithm to compute collision-free and smooth trajectories for high-DOF robots interacting with humans in a shared workspace. Our approach uses offline learning of human actions along with temporal coherence to predict the human actions. Our intention-aware online planning algorithm uses the learned database to compute a reliable trajectory based on the predicted actions. We represent the predicted human motion using a Gaussian distribution and compute tight upper bounds on collision probabilities for safe motion planning. We also describe novel techniques to account for noise in human motion prediction. We highlight the performance of our planning algorithm in complex simulated scenarios and real world benchmarks with 7-DOF robot arms operating in a workspace with a human performing complex tasks. We demonstrate the benefits of our intention-aware planner in terms of computing safe trajectories in such uncertain environments.},
	number = {1},
	journal = {International Journal of Robotics Research},
	author = {Park, Jae Sung and Park, Chonhyon and Manocha, Dinesh},
	year = {2019},
	note = {ISBN: 0278364918},
	keywords = {Action Recognition, Human Activity, Motion, Prediction, Robot motion planning, Robotics, human motion prediction},
	pages = {23--39},
}

@article{kwon_expressing_2018,
	title = {Expressing {Robot} {Incapability}},
	doi = {10.1145/3171221.3171276},
	abstract = {Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it.We frame this as a trajectory optimization problem: maximize the similar- ity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot.We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.},
	author = {Kwon, Minae and Huang, Sandy H. and Dragan, Anca D.},
	year = {2018},
	note = {ISBN: 9781450349536},
	keywords = {2018, Grasp, Human Grasp, Motion, Robotics, acm reference format, and anca d, dragan, expressing, expressive robot motion, huang, incapability, minae kwon, sandy h, trajectory optimization},
	pages = {87--95},
}

@article{kaplan_toward_2016,
	title = {Toward human-robot collaboration in surgery: {Performance} assessment of human and robotic agents in an inclusion segmentation task},
	volume = {2016-June},
	issn = {10504729},
	doi = {10.1109/ICRA.2016.7487199},
	abstract = {Increasing the level of autonomy in robot-assisted surgery has the potential to improve the safety, speed, and applicability of robot-assisted surgical systems. To facilitate the development and incorporation of robot autonomy in clinical settings, human-robot collaboration models have been suggested in which human and robotic agents work together to accomplish a task. In this work, we measure performance of several human-robot collaboration models in two experiments based on the task of segmenting a stiff inclusion in soft tissue, which simulates a tumor. In the inclusion segmentation experiment, twelve participants explored an artificial tissue and identified the inclusion boundary under the collaboration models of (1) teleoperation, (2) supervised control, (3) traded control, and (4) full autonomy. In the boundary identification experiment, we isolate the performance of human and robotic agents in the boundary identification sub-task; participants and a robotic agent independently identified the boundary of four virtually palpated tissues. Results from the inclusion segmentation experiment indicate that human agents complete the task faster; teleoperation had the fastest task times. Results of both experiments indicate that the robotic agent identifies boundaries with higher sensitivity and less variance than human agents. This indicates that task accuracy increases when a robotic agent segments the boundary, while including a human agent can decrease the overall task time.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Kaplan, Kirsten E. and Nichols, Kirk A. and Okamura, Allison M.},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781467380263},
	keywords = {Human Demonstration, Human-Robot Interaction(HRI)},
	pages = {723--729},
}

@article{sheikholeslami_study_nodate,
	title = {A {Study} of {Reaching} {Motions} for {Collaborative} {Human}-{Robot} {Interaction}},
	author = {Sheikholeslami, Sara and Lee, Gilwoo and Hart, Justin W and Srinivasa, Siddhartha and Croft, Elizabeth A},
	keywords = {Handovers, Human Activity, Human-Robot Interaction(HRI), Motion},
	pages = {1--10},
}

@article{tseng_analysis_2018,
	title = {Analysis of {Coordination} {Patterns} between {Gaze} and {Control} in {Human} {Spatial} {Search}},
	issn = {24058963},
	doi = {10.1016/j.ifacol.2019.01.041},
	author = {Tseng, Kuo-shih},
	year = {2018},
	keywords = {Gaze, Human Experiment, Human-Human Interaction (HHI), Motion, autonomous mobile robots, human-machine interface, information analysis, machine learning, telerobotics},
}

@article{flash_models_2013,
	title = {Models of human movement: {Trajectory} planning and inverse kinematics studies},
	volume = {61},
	issn = {09218890},
	url = {http://dx.doi.org/10.1016/j.robot.2012.09.020},
	doi = {10.1016/j.robot.2012.09.020},
	abstract = {The seemingly simple everyday actions of moving limb and body to accomplish a motor task or interact with the environment are incredibly complex. To reach for a target we first need to sense the target's position with respect to an external coordinate system; we then need to plan a limb trajectory which is executed by issuing an appropriate series of neural commands to the muscles. These, in turn, exert appropriate forces and torques on the joints leading to the desired movement of the arm. Here we review some of the earlier work as well as more recent studies on the control of human movement, focusing on behavioral and modeling studies dealing with task space and joint-space movement planning. At the task level, we describe studies investigating trajectory planning and inverse kinematics problems during point-to-point reaching movements as well as two-dimensional (2D) and three-dimensional (3D) drawing movements. We discuss models dealing with the two-thirds power law, particularly differential geometrical approaches dealing with the relation between path geometry and movement velocity. We also discuss optimization principles such as the minimum-jerk model and the isochrony principle for point-to-point and curved movements. We next deal with joint-space movement planning and generation, discussing the inverse kinematics problem and common solutions to the problems of kinematic redundancy. We address the question of which reference frames are used by the nervous system and review studies examining the employment of kinematic constraints such as Donders' and Listing's laws. We also discuss optimization approaches based on Riemannian geometry. One principle of motor coordination during human locomotion emerging from this body of work is the intersegmental law of coordination. However, the nature of the coordinate systems underlying motion planning remains of interest as they are related to the principles governing the control of human arm movements. © 2012 Elsevier B.V. All rights reserved.},
	number = {4},
	journal = {Robotics and Autonomous Systems},
	author = {Flash, Tamar and Meirovitch, Yaron and Barliya, Avi},
	year = {2013},
	note = {Publisher: Elsevier B.V.},
	keywords = {Affine differential geometry modeling, Donder's and Listing's laws, Human Demonstration, Human trajectory planning, Intersegmental coordination, Kinematic redundancy, Motion, Planning, Trajectory formation, Two-thirds power law},
	pages = {330--339},
}

@article{donnarumma_sensorimotor_2018,
	title = {Sensorimotor {Communication} for {Humans} and {Robots}: {Improving} {Interactive} {Skills} by {Sending} {Coordination} {Signals}},
	volume = {10},
	issn = {23798939},
	doi = {10.1109/TCDS.2017.2756107},
	abstract = {IEEE During joint actions, humans continuously exchange coordination signals and use non-verbal, sensorimotor forms of communication. Here we discuss a specific example of sensorimotor communication \& \#x2013; \& \#x201C;signaling \& \#x201D; \& \#x2013; which consists in the intentional modification of one \& \#x2019;s own action plan (e.g., a plan for reaching a glass of wine) to make it more predictable or discriminable from alternative action plans that are contextually plausible (e.g., a plan for reaching another glass on the same table). We first review the existing evidence on signaling in human-human interactions, discussing under which conditions humans use signaling. Successively, we distill these insights into a computational theory of signaling during on-line interactions. Central to our approach are the following ideas: (1) signaling endows pragmatic plans with communicative goals; (2) signaling can be understood within a cost-benefit scheme, balancing the costs for the signaling agent against its benefits for interaction success; (3) signaling may be part of an interactive strategy that optimizes success when joint goals are uncertain. Finally, we exemplify the benefits of signaling in a series of simulations and discuss how endowing robots with signaling abilities can increase the quality of HRIs by making their behavior more predictable and \& \#x201C;legible \& \#x201D; for humans.},
	number = {4},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Donnarumma, Francesco and Dindo, Haris and Pezzulo, Giovanni},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Human-robot interaction, Joint-action, Motion, Prediction, joint action, sensorimotor communication, signaling},
	pages = {903--917},
}

@article{hogan_moving_1987,
	title = {Moving gracefully: quantitative theories of motor coordination},
	volume = {10},
	issn = {01662236},
	doi = {10.1016/0166-2236(87)90043-9},
	abstract = {Even simple movements require the coordination of a bewildering number of muscles. How does the brain cope with this task? How can we begin to understand this extraordinarily complex behavior? One effective way is to formulate a quantitative, mathematical theory of movement control. A theory simplifies matters because complex, detailed and experimentally testable predictions can be derived from basic concepts. This article will briefly review some attempts to develop a quantitative theory of motor coordination. © 1987.},
	number = {4},
	journal = {Trends in Neurosciences},
	author = {Hogan, Neville and Flash, Tamar},
	year = {1987},
	keywords = {Human Activity, Motion},
	pages = {170--174},
}

@article{talha_features_2018,
	title = {Features and {Classification} {Schemes} for {View}-{Invariant} and {Real}-{Time} {Human} {Action} {Recognition}},
	volume = {10},
	issn = {23798939},
	doi = {10.1109/TCDS.2018.2844279},
	abstract = {Human Action recognition (HAR) is largely used in the field of Ambient Assisted Living (AAL) to create an interaction between humans and computers. In these applications , it cannot be asked to people to act non-naturally. The algorithm has to adapt and the interaction has to be as quick as possible to make this interaction fluent. To improve the existing algorithms with regards to that points, we propose a novel method based on skeleton information provided by RGB-D cameras. This approach is able to carry out early action recognition and is more robust to viewpoint variability. To reach this goal, a new descriptor called Body Directional Velocity is proposed and a real-time classification is performed. Experimental results on four benchmarks show that our method competes with various skeleton-based HAR algorithms. We also show the suitability of our method for early recognition of human actions.},
	number = {4},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Talha, Sid Ahmed Walid and Hammouche, Mounir and Ghorbel, Enjie and Fleury, Anthony and Ambellouis, Sebastien},
	year = {2018},
	keywords = {Body-part directional velocity, Gaussian mixture model (GMM), hidden Markov model (HMM), human action recognition (HAR), human-robot interaction, skeleton analysis},
	pages = {894--902},
}

@article{modifications_trajectory_1985,
	title = {Trajectory {Modifications} {During} {Reaching} {Towards} {Visual} {Targets}},
	volume = {3},
	number = {3},
	author = {Modifications, Trajectory and Reaching, During and Targets, Towards Visual},
	year = {1985},
	keywords = {Human Activity, Human Experiment, Motion},
}

@article{ding_human_2011,
	title = {Human arm motion modeling and long-term prediction for safe and efficient human-robot-interaction},
	issn = {10504729},
	doi = {10.1109/ICRA.2011.5980248},
	abstract = {Modeling and predicting human behavior is indispensable when industrial robots interacting with human operators are to be manipulated safely and efficiently. One challenge is that human operators tend to follow different motion patterns, depending on their intention and the structure of the environment. This precludes the use of classical estimation techniques based on kinematic or dynamic models, especially for the purpose of long-term prediction. In this paper, we propose a method based on Hidden Markov Models to predict the region of the workspace that is possibly occupied by the human within a prediction horizon. In contrast to predictions in the form of single points such as most likely human positions as obtained from previous approaches, the regions obtained here may serve as safety constraints when the robot motion is planned or optimized. This way one avoids collisions with a probability not less than a predefined threshold. The practicability of our method is demonstrated by successfully and accurately predicting the motion of a human arm in two scenarios involving multiple motion patterns.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Ding, Hao and Reißig, Gunther and Wijaya, Kurniawan and Bortot, Dino and Bengler, Klaus and Stursberg, Olaf},
	year = {2011},
	note = {ISBN: 9781612843865},
	keywords = {Human-Robot Interaction(HRI), Modelling, Motion, Prediction},
	pages = {5875--5880},
}

@article{hasnain_synchrony-based_2012,
	title = {A {Synchrony}-{Based} {Perspective} for {Partner} {Selection} and {Attentional} {Mechanism} in {Human}-{Robot} {Interaction}},
	volume = {3},
	issn = {2081-4836},
	url = {http://www.degruyter.com/view/j/pjbr.2012.3.issue-3/s13230-013-0111-y/s13230-013-0111-y.xml},
	doi = {10.2478/s13230-013-0111-y},
	abstract = {Future robots must co-exist and directly interact with human beings. Designing these agents imply solving hard problems linked to human-robot interaction tasks. For instance, how a robot can choose an interacting partner among various agents and how a robot locates regions of interest in its visual field. Studies of neurobiology and psychology collectively named synchrony as an indispensable parameter for social interaction. We assumed that Human-Robot interaction could be initiated by synchrony detection. In this paper, we present a developmental approach for analyzing unintentional synchronization in human-robot interaction. Using our neural network model, the robot learns from a babbling step its inner dynamics by associating its own motor activities (oscillators) with the visual stimulus induced by its own motion. After learning the robot is capable of choosing an interacting agent and of localizing the spatial position of its preferred partner by synchrony detection.},
	number = {3},
	journal = {Paladyn, Journal of Behavioral Robotics},
	author = {Hasnain, Syed Khursheed and Mostafaoui, Ghiles and Gaussier, Philippe},
	year = {2012},
	note = {ISBN: 1323001301},
	keywords = {Brain, EEG, Human-Robot Interaction(HRI), Neuroscience, dynamical systems, focus of attention, human robot interaction, neural networks, partner selection, synchrony},
	pages = {156--171},
}

@article{gottwald_good_2015,
	title = {Good is up-spatial metaphors in action observation},
	volume = {6},
	issn = {16641078},
	doi = {10.3389/fpsyg.2015.01605},
	abstract = {Positive objects or actions are associated with physical highness, whereas negative objects or actions are related to physical lowness. Previous research suggests that metaphorical connection (“good is up” or “bad is down”) between spatial experience and evaluation of objects is grounded in actual experience with the body. Prior studies investigated effects of spatial metaphors with respect to verticality of either static objects or self-performed actions. By presenting videos of object placements, the current three experiments combined vertically-located stimuli with observation of vertically- directed actions. As expected, participants’ ratings of emotionally-neutral objects were systematically influenced by the observed vertical positioning, that is, ratings were more positive for objects that were observed being placed up as compared to down. Moreover, effects were slightly more pronounced for “bad is down,” because only the observed downward, but not the upward, action led to different ratings as compared to a medium-positioned action. Last, some ratings were even affected by observing only the upward/downward action, without seeing the final vertical placement of the object. Thus, both, a combination of observing a vertically-directed action and seeing a vertically- located object, and observing a vertically-directed action alone, affected participants’ evaluation of emotional valence of the involved object. The present findings expand the relevance of spatial metaphors to action observation, thereby giving new impetus to embodied-cognition.},
	number = {OCT},
	journal = {Frontiers in Psychology},
	author = {Gottwald, Janna M. and Elsner, Birgit and Pollatos, Olga},
	year = {2015},
	keywords = {Action observation, Action perception, Action understanding, Embodied cognition, Emotional valence, Object Placement, Spatial metaphors},
	pages = {1--10},
}

@article{sacheli_social_2015,
	title = {Social cues to joint actions: {The} role of shared goals},
	volume = {6},
	issn = {16641078},
	doi = {10.3389/fpsyg.2015.01034},
	abstract = {In daily life, we do not just move independently from how others move. Rather, the way we move conveys information about our cognitive and affective attitudes toward our conspecifics. However, the implicit social substrate of our movements is not easy to capture and isolate given the complexity of human interactive behaviors. In this perspective article we discuss the crucial conditions for exploring the impact of "interpersonal" cognitive/emotional dimensions on the motor behavior of individuals interacting in realistic contexts. We argue that testing interactions requires one to build up naturalistic and yet controlled scenarios where participants reciprocally adapt their movements in order to achieve an overarching "shared goal." We suggest that a shared goal is what singles out real interactions from situations where two or more individuals contingently but independently act next to each other, and that "interpersonal" socio-emotional dimensions might fail to affect co-agents' behaviors if real interactions are not at place. We report the results of a novel joint-grasping task suitable for exploring how individual sub-goals (i.e., correctly grasping an object) relate to, and depend from, the representation of "shared goals."},
	number = {JUL},
	journal = {Frontiers in Psychology},
	author = {Sacheli, Lucia M. and Aglioti, Salvatore M. and Candidi, Matteo},
	year = {2015},
	keywords = {Action understanding, Grasping, Human-Human Interaction (HHI), Interpersonal perception, Joint-action, Kinematics, Shared goals, Socio-emotional context},
	pages = {1--7},
}

@article{Sisbot2010,
	title = {Synthesizing {Robot} {Motions} {Adapted} to {Human} {Presence}},
	volume = {2},
	issn = {1875-4805},
	url = {https://doi.org/10.1007/s12369-010-0059-6},
	doi = {10.1007/s12369-010-0059-6},
	abstract = {With robotics hardware becoming more and more safe and compliant, robots are not far from entering our homes. The robot, that will share the same environment with humans, will be expected to consider the geometry of the interaction and to perform intelligent space sharing.},
	number = {3},
	journal = {International Journal of Social Robotics},
	author = {Sisbot, Emrah Akin and Marin-Urias, Luis F. and Broquère, Xavier and Sidobre, Daniel and Alami, Rachid},
	year = {2010},
	note = {ISBN: 1875-4791},
	keywords = {Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Motion, Robotics},
	pages = {329--343},
}

@article{dautenhahn_robots_2005,
	title = {Robots we like to live with?! - a developmental perspective on a personalized, life-long robot companion},
	issn = {0-7803-8570-5},
	doi = {10.1109/roman.2004.1374720},
	abstract = {This work addresses different possible social relationships between robots and humans, drawing on animal-human relationships. I argue that humans have been living in (generally peaceful) co-existence with a number of potentially dangerous species, such as some canines. Interestingly dogs are not born 'pet dogs', it's not completely 'predefined' in their genes whether they become friendly or dangerous. A critical period in a puppy's early life significantly shapes its socialization and behavioral conformation. I suggest that such a developmental model of socialization could be an interesting viewpoint on the design of future generations of robots that need to co-exist with humans, and that humans like to live with. I propose the challenge of developing 'personalized robot companions', machines that can serve as life-long companions. I argue that such individualized robots are necessary due to human nature: people have individual needs, likes and dislikes, preferences and personalities that a companion would have to adapt to: one and the same robot not fit all people. Cognitive robot companions above all need to be socialized and personalized in order to meet the social, emotional and cognitive needs of people they are 'living with'.},
	author = {Dautenhahn, K.},
	year = {2005},
	pmid = {17301026},
	note = {ISBN: 0-7803-8570-5},
	keywords = {Human Demonstration, Robotics},
	pages = {17--22},
}

@article{collier_look_2015,
	title = {Look together: analyzing gaze coordination with epistemic network analysis},
	volume = {6},
	doi = {10.3389/fpsyg.2015.01016},
	abstract = {When conversing and collaborating in everyday situations, people naturally and interactively align their behaviors with each other across various communication channels, including speech, gesture, posture, and gaze. Having access to a partner's referential gaze behavior has been shown to be particularly important in achieving collaborative outcomes, but the process in which people's gaze behaviors unfold over the course of an interaction and become tightly coordinated is not well understood. In this paper, we present work to develop a deeper and more nuanced understanding of coordinated referential gaze in collaborating dyads. We recruited 13 dyads to participate in a collaborative sandwich-making task and used dual mobile eye tracking to synchronously record each participant's gaze behavior. We used a relatively new analysis technique-epistemic network analysis-to jointly model the gaze behaviors of both conversational participants. In this analysis, network nodes represent gaze targets for each participant, and edge strengths convey the likelihood of simultaneous gaze to the connected target nodes during a given time-slice. We divided collaborative task sequences into discrete phases to examine how the networks of shared gaze evolved over longer time windows. We conducted three separate analyses of the data to reveal (1) properties and patterns of how gaze coordination unfolds throughout an interaction sequence, (2) optimal time lags of gaze alignment within a dyad at different phases of the interaction, and (3) differences in gaze coordination patterns for interaction sequences that lead to breakdowns and repairs. In addition to contributing to the growing body of knowledge on the coordination of gaze behaviors in joint activities, this work has implications for the design of future technologies that engage in situated interactions with human users. (PsycINFO Database Record (c) 2015 APA, all rights reserved) (journal abstract).},
	number = {July},
	journal = {Frontiers in Psychology},
	author = {Collier, Wesley and Gleicher, Michael and Andrist, Sean and Shaffer, David and Mutlu, Bilge},
	year = {2015},
	keywords = {Gaze, Human Activity, Human-Human Interaction (HHI), conversational repair, epistemic network analysis, gaze tracking, referential gaze, social signals},
	pages = {1--15},
}

@article{sebanz_joint_2006,
	title = {Joint action: {Bodies} and minds moving together},
	volume = {10},
	issn = {13646613},
	doi = {10.1016/j.tics.2005.12.009},
	abstract = {The ability to coordinate our actions with those of others is crucial for our success as individuals and as a species. Progress in understanding the cognitive and neural processes involved in joint action has been slow and sparse, because cognitive neuroscientists have predominantly studied individual minds and brains in isolation. However, in recent years, major advances have been made by investigating perception and action in social context. In this article we outline how studies on joint attention, action observation, task sharing, action coordination and agency contribute to the understanding of the cognitive and neural processes supporting joint action. Several mechanisms are proposed that allow individuals to share representations, to predict actions, and to integrate predicted effects of own and others' actions. ?? 2005 Elsevier Ltd. All rights reserved.},
	number = {2},
	journal = {Trends in Cognitive Sciences},
	author = {Sebanz, Natalie and Bekkering, Harold and Knoblich, Günther G??nther},
	year = {2006},
	pmid = {16406326},
	note = {arXiv: f
ISBN: 1364-6613, 1364-6613},
	keywords = {Human-Human Interaction (HHI), Joint-action, Motion, Vision},
	pages = {70--76},
}

@article{kompatsiari_bidding_2018,
	title = {Bidding for joint attention: {On} the role of eye contact in gaze cueing},
	issn = {2045-2322},
	url = {https://psyarxiv.com/mx28g/},
	doi = {10.17605/OSF.IO/MX28G},
	abstract = {Most experimental protocols examining joint attention with the gaze cueing paradigm are “observational” and “offline”, thereby not involving social interaction. We examined whether within a naturalistic online interaction, real-time eye contact influences the gaze cueing effect (GCE). We embedded gaze cueing in an interactive protocol with the iCub humanoid robot. This has the advantage of ecological validity combined with excellent experimental control. Critically, before averting gaze, iCub either established eye contact or not, a manipulation enabled by an algorithm detecting position of the human eyes. For non-predictive gaze cueing procedure (Experiment 1), only the eye contact condition elicited GCE, while for counter-predictive procedure (Experiment 2), only the condition with no eye contact induced GCE. These results reveal an interactive effect of strategic and social top-down components on the reflexive gaze-induced orienting of attention. More generally, we propose that naturalistic protocols cast a new light on mechanisms of social cognition.},
	number = {September},
	journal = {Preprint},
	author = {Kompatsiari, Kyveli and Ciardo, Francesca and Tikhanoff, Vadim and Metta, Giorgio and Wykowska, Agnieszka},
	year = {2018},
	note = {Publisher: Springer US},
	keywords = {Cognition and Perception, Cognitive Psychology, Eyes, Gaze, Human-Robot Interaction(HRI), Joint-action, Linguistics, Psychology, Social and Behavioral Sciences, Vision, eye contact, gaze cueing, human, interactive gaze, joint attention, robot interaction, social interaction},
	pages = {345--346},
}

@article{natale_predicting_2014,
	title = {Predicting others' intention involves motor resonance: {EMG} evidence from 6- and 9-month-old infants},
	volume = {7},
	issn = {18789293},
	url = {http://dx.doi.org/10.1016/j.dcn.2013.10.004},
	doi = {10.1016/j.dcn.2013.10.004},
	abstract = {The study explores infants' ability to generate on-line predictions about others' action goals through the recruitment of motor resonance mechanisms. To this aim, electromyography was recorded from mouth-opening suprahyoid muscles (SM) of 9-month-old infants while watching a video of an adult agent reaching-to-grasp an object and bringing it either to mouth or head. The results demonstrated, for the first time, that at the age of 9 months there is a dynamic mirror modulation of SM activity by action observation, with the infant's muscles responsible for the action final goal being recruited from the action outset. The comparison with the responses of 6-month-olds tested on the same task showed that in younger and older infants there is a different chronometry of the SM activation with respect to the different phases of the observed action (i.e., bringing vs. grasping, respectively). Results suggest that motor resonance mechanisms triggered within the infants' motor system by action observation undergo gradual development during the first year of life. They also indicate that motor resonance may reflect anticipation of the agent's intention based on the goal of the action.©2013 Elsevier Ltd. All rights reserved.},
	number = {November},
	journal = {Developmental Cognitive Neuroscience},
	author = {Natale, Elena and Senna, Irene and Bolognini, Nadia and Quadrelli, Ermanno and Addabbo, Margaret and Macchi Cassia, Viola and Turati, Chiara},
	year = {2014},
	pmid = {24270044},
	note = {Publisher: Elsevier Ltd
ISBN: 1878-9293},
	keywords = {Action understanding, Brain, Cognition, Cognitive development, Grasping skills, Infancy, Motor resonance, Prediction},
	pages = {23--29},
}

@article{levine_learning_2018,
	title = {Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
	volume = {37},
	issn = {17413176},
	doi = {10.1177/0278364917710318},
	abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
	number = {4-5},
	journal = {International Journal of Robotics Research},
	author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
	year = {2018},
	pmid = {21156984},
	note = {arXiv: 1603.02199
ISBN: 978-1-4503-3716-8},
	keywords = {Grasp, Robotics, Vision, deep learning, neural networks},
	pages = {421--436},
}

@article{Sciutti2018,
	title = {Guest {Editorial} {A} {Sense} of {Interaction} in {Humans} and {Robots}: {From} {Visual} {Perception} to {Social} {Cognition}},
	volume = {10},
	issn = {2379-8920},
	url = {https://ieeexplore.ieee.org/document/8567856/},
	doi = {10.1109/TCDS.2018.2883166},
	number = {4},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Sciutti, Alessandra and Noceti, Nicoletta},
	year = {2018},
	keywords = {Cognition, Human-Robot Interaction(HRI), Robotics, Vision},
	pages = {839--842},
}

@article{aroyo_can_2018,
	title = {Can a {Humanoid} {Robot} {Spot} a {Liar}?},
	number = {December},
	journal = {IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids)},
	author = {Aroyo, A.M. and Gonzalez-Billandon, J. and Tonelli, A. and Sciutti, A. and Gori, M. and Sandini, G. and Rea, F.},
	year = {2018},
	keywords = {Action understanding, Human-Robot Interaction(HRI), Robotics, Vision},
}

@article{ortigue_understanding_2010,
	title = {Understanding actions of others: {The} electrodynamics of the left and right hemispheres. {A} high-density {EEG} neuroimaging study},
	volume = {5},
	issn = {19326203},
	doi = {10.1371/journal.pone.0012160},
	abstract = {BACKGROUND: When we observe an individual performing a motor act (e.g. grasping a cup) we get two types of information on the basis of how the motor act is done and the context: what the agent is doing (i.e. grasping) and the intention underlying it (i.e. grasping for drinking). Here we examined the temporal dynamics of the brain activations that follow the observation of a motor act and underlie the observer's capacity to understand what the agent is doing and why.{\textbackslash}n{\textbackslash}nMETHODOLOGY/PRINCIPAL FINDINGS: Volunteers were presented with two-frame video-clips. The first frame (T0) showed an object with or without context; the second frame (T1) showed a hand interacting with the object. The volunteers were instructed to understand the intention of the observed actions while their brain activity was recorded with a high-density 128-channel EEG system. Visual event-related potentials (VEPs) were recorded time-locked with the frame showing the hand-object interaction (T1). The data were analyzed by using electrical neuroimaging, which combines a cluster analysis performed on the group-averaged VEPs with the localization of the cortical sources that give rise to different spatio-temporal states of the global electrical field. Electrical neuroimaging results revealed four major steps: 1) bilateral posterior cortical activations; 2) a strong activation of the left posterior temporal and inferior parietal cortices with almost a complete disappearance of activations in the right hemisphere; 3) a significant increase of the activations of the right temporo-parietal region with simultaneously co-active left hemispheric sources, and 4) a significant global decrease of cortical activity accompanied by the appearance of activation of the orbito-frontal cortex.{\textbackslash}n{\textbackslash}nCONCLUSIONS/SIGNIFICANCE: We conclude that the early striking left hemisphere involvement is due to the activation of a lateralized action-observation/action execution network. The activation of this lateralized network mediates the understanding of the goal of object-directed motor acts (mirror mechanism). The successive right hemisphere activation indicates that this hemisphere plays an important role in understanding the intention of others.},
	number = {8},
	journal = {PLoS ONE},
	author = {Ortigue, Stephanie and Sinigaglia, Corrado and Rizzolatti, Giacomo and Grafton, Scott T.},
	year = {2010},
	pmid = {20730095},
	note = {ISBN: 1932-6203 (Electronic){\textbackslash}r1932-6203 (Linking)},
	keywords = {Action understanding, EEG, Neuroscience},
}

@article{grzyb_childrens_2018,
	title = {Children's scale errors are a natural consequence of learning to associate objects with actions: a computational model},
	issn = {1363755X},
	url = {http://doi.wiley.com/10.1111/desc.12777},
	doi = {10.1111/desc.12777},
	number = {November},
	journal = {Developmental Science},
	author = {Grzyb, Beata J. and Nagai, Yukie and Asada, Minoru and Cattani, Allegra and Floccia, Caroline and Cangelosi, Angelo},
	year = {2018},
	keywords = {Action understanding, Brain, Modelling, and, communications technology, japan, national institute of information, osaka, school of computer science, university of},
	pages = {e12777},
}

@article{pan_evaluating_2018,
	title = {Evaluating {Social} {Perception} of {Human}-to-{Robot} {Handovers} {Using} the {Robot} {Social} {Attributes} {Scale} ({RoSAS})},
	issn = {21672148},
	url = {http://dl.acm.org/citation.cfm?doid=3171221.3171257},
	doi = {10.1145/3171221.3171257},
	abstract = {© 2018 ACM. This work explores social perceptions of robots within the domain of human-to-robot handovers. Using the Robotic Social Attributes Scale (RoSAS), we explore how users socially judge robot receivers as three factors are varied: Initial position of the robot arm prior to handover, grasp method employed by the robot when receiving a handover object trading off perceived object safety for time efficiency, and retraction speed of the arm following handover. Our results show that over multiple handover interactions with the robot, users gradually perceive the robot receiver as being less discomforting and having more emotional warmth. Additionally, we have found that by varying grasp method and retraction speed, users may hold significantly different judgments of robot competence and discomfort. With these results, we recognize empirically that users are able to develop social perceptions of robots which can change through modification of robot receiving behaviour and through repeated interaction with the robot. More widely, this work suggests that measurement of user social perceptions should play a larger role in the design and evaluation of human-robot interactions and that the RoSAS can serve as a standardized tool in this regard.},
	journal = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction - HRI '18},
	author = {Pan, Matthew K.X.J. and Croft, Elizabeth A. and Niemeyer, Günter},
	year = {2018},
	note = {ISBN: 9781450349536},
	keywords = {Handovers, Human Experiment, Human-Robot Handovers, Human-Robot Interaction, Human-Robot Interaction(HRI), Measurement, Psychometric Scale, Robotics, Robots, Social Perception, Social Robotics, Social Robots, dovers, han-, human-robot handovers, human-robot interaction, measurement, psychometric scale, robotics, robots, social perception, social robotics, social robots},
	pages = {443--451},
}

@article{baraglia_efficient_2017,
	title = {Efficient human-robot collaboration: when should a robot take initiative?},
	issn = {17413176},
	doi = {10.1177/0278364916688253},
	abstract = {The promise of robots assisting humans in everyday tasks has led to a variety of research questions and challenges in human-robot collaboration. Here, we address the question of whether and when a robot should take initiative during joint human-robot task execution. We designed a robotic system capable of autonomously performing table-top manipulation tasks while monitoring the environmental state. Our system is able to predict future environmental states and the robot's actions to reach them using a dynamic Bayesian network. To evaluate our system, we implemented three different initiative conditions to trigger the robot's actions. Human-initiated help gives control of the robot action timing to the user; robot-initiated reactive help triggers robot assistance when it detects that the human needs help; robot-initiated proactive help makes the robot help whenever it can. We performed a user study (N=18) to compare the trigger mechanisms in terms of quality of interaction, system performance and perceived sociality of the robot. We found that people collaborate best with a proactive robot, yielding better team fluency and high subjective ratings. However, they prefer having control of when the robot should help, rather than working with a reactive robot that only helps when needed. We also found that participants gazed at the robot's face more during the human-initiated help compared to the other conditions. This shows that asking for the robot's help may lead to a more "social" interaction, without improving the quality of interaction or the system performance.},
	journal = {International Journal of Robotics Research},
	author = {Baraglia, Jimmy and Cakmak, Maya and Nagai, Yukie and Rao, Rajesh P.N. and Asada, Minoru},
	year = {2017},
	note = {ISBN: 0278-3649},
	keywords = {Bayesian Network, Bayesian network, Human robot interaction, Human-Robot Interaction(HRI), Robotics, initiative assistive robotics, social robotics},
}

@article{strumwasser_relations_1994,
	title = {The relations between neuroscience and human behavioral science.},
	volume = {61},
	issn = {0022-5002},
	url = {http://www.pubmedcentral.gov/articlerender.fcgi?artid=1334418},
	doi = {10.1901/jeab.1994.61-307},
	abstract = {Neuroscience seeks to understand how the human brain, perhaps the most complex electrochemical machine in the universe, works, in terms of molecules, membranes, cells and cell assemblies, development, plasticity, learning, memory, cognition, and behavior. The human behavioral sciences, in particular psychiatry and clinical psychology, deal with disorders of human behavior and mentation. The gap between neuroscience and the human behavioral sciences is still large. However, some major advances in neuroscience over the last two decades have diminished the span. This article reviews the major advances of neuroscience in six areas with relevance to the behavioral sciences: (a) evolution of the nervous system; (b) visualizing activity in the human brain; (c) plasticity of the cerebral cortex; (d) receptors, ion channels, and second/third messengers; (e) molecular genetic approaches; and (f) understanding integrative systems with networks and circadian clocks as examples.},
	number = {2},
	journal = {Journal of the Experimental Analysis of Behavior},
	author = {Strumwasser, F},
	year = {1994},
	pmid = {7513347},
	note = {ISBN: 0022-5002},
	keywords = {Brain, Human Activity, Neuroscience, and one can rightfully, ask, behavior-physiology relations, brain, do, going on in, humans, neuroscience, the study of human, there are major revolutions, they have relevance for},
	pages = {307--317},
}

@article{Rasch2018,
	title = {A {Joint} {Motion} {Model} for {Human}-{Like} {Robot}-{Human} {Handover}},
	url = {http://arxiv.org/abs/1808.09280},
	abstract = {In future, robots will be present in everyday life. The development of these supporting robots is a challenge. A fundamental task for assistance robots is to pick up and hand over objects to humans. By interacting with users, soft factors such as predictability, safety and reliability become important factors for development. Previous works show that collaboration with robots is more acceptable when robots behave and move human-like. In this paper, we present a motion model based on the motion profiles of individual joints. These motion profiles are based on observations and measurements of joint movements in human-human handover. We implemented this joint motion model (JMM) on a humanoid and a non-humanoidal industrial robot to show the movements to subjects. Particular attention was paid to the recognizability and human similarity of the movements. The results show that people are able to recognize human-like movements and perceive the movements of the JMM as more human-like compared to a traditional model. Furthermore, it turns out that the differences between a linear joint space trajectory and JMM are more noticeable in an industrial robot than in a humanoid robot.},
	author = {Rasch, Robin and Wachsmuth, Sven and König, Matthias},
	year = {2018},
	note = {arXiv: 1808.09280v1
ISBN: 9781538672822},
	keywords = {Handovers, Human Experiment, Human-Robot Interaction(HRI), Motion},
}

@article{lapenta_motor_2018,
	title = {Motor system recruitment during action observation: {No} correlation between mu-rhythm desynchronization and corticospinal excitability},
	volume = {13},
	issn = {19326203},
	doi = {10.1371/journal.pone.0207476},
	abstract = {Observing others' actions desynchronizes electroencephalographic (EEG) rhythms and modulates corticospinal excitability as assessed by transcranial magnetic stimulation (TMS). However, it remains unclear if these measures reflect similar neurofunctional mechanisms at the individual level. In the present study, a within-subject experiment was designed to assess these two neurophysiological indexes and to quantify their mutual correlation. Participants observed reach-to-grasp actions directed towards a small (precision grip) or a large object (power grip). We focused on two specific time points for both EEG and TMS. The first time point (t1) coincided with the maximum hand aperture, i.e. the moment at which a significant modulation of corticospinal excitability is expected. The second (t2), coincided with the EEG resynchronization occurring at the end of the action, i.e. the moment at which a hypothetic minimum for action observation effect is expected. Results showed a Mu rhythm bilateral desynchronization at t1 with differential resynchronization at t2 in the two hemispheres. Beta rhythm was more desynchronized in the left hemisphere at both time points. These EEG differences, however, were not influenced by grip type. Conversely, motor potentials evoked by TMS in an intrinsic hand muscle revealed an interaction effect of grip and time. No significant correlations between Mu/Beta rhythms and motor evoked potentials were found. These findings are discussed considering the spatial and temporal resolution of the two investigated techniques and argue over two alternative explanations: i. each technique provides different measures of the same process or ii. they describe complementary features of the action observation network in humans.},
	number = {11},
	journal = {PLoS ONE},
	author = {Lapenta, Olivia M. and Ferrari, Elisabetta and Boggio, Paulo S. and Fadiga, Luciano and D’Ausilio, Alessandro},
	year = {2018},
	pmid = {30440042},
	note = {ISBN: 1111111111},
	keywords = {Action understanding, EEG, Vision},
	pages = {1--15},
}

@article{ferro_reading_2010,
	title = {Reading as active sensing: {A} computational model of gaze planning in word recognition},
	volume = {4},
	issn = {16625218},
	doi = {10.3389/fnbot.2010.00006},
	abstract = {WE OFFER A COMPUTATIONAL MODEL OF GAZE PLANNING DURING READING THAT CONSISTS OF TWO MAIN COMPONENTS: a lexical representation network, acquiring lexical representations from input texts (a subset of the Italian CHILDES database), and a gaze planner, designed to recognize written words by mapping strings of characters onto lexical representations. The model implements an active sensing strategy that selects which characters of the input string are to be fixated, depending on the predictions dynamically made by the lexical representation network. We analyze the developmental trajectory of the system in performing the word recognition task as a function of both increasing lexical competence, and correspondingly increasing lexical prediction ability. We conclude by discussing how our approach can be scaled up in the context of an active sensing strategy applied to a robotic setting.},
	number = {JUN},
	journal = {Frontiers in Neurorobotics},
	author = {Ferro, Marcello and Ognibene, Dimitri and Pezzulo, Giovanni and Pirrelli, Vito},
	year = {2010},
	pmid = {20577589},
	keywords = {Active sensing, Gaze, Lexical representation network, Modelling, Prediction, Reading, SOM, Serial order encoding},
	pages = {1--16},
}

@article{rozzi_grasping_2015,
	title = {Grasping actions and social interaction: {Neural} bases and anatomical circuitry in the monkey},
	volume = {6},
	issn = {16641078},
	doi = {10.3389/fpsyg.2015.00973},
	abstract = {The study of the neural mechanisms underlying grasping actions showed that cognitive functions are deeply embedded in motor organization. In the first part of this review, we describe the anatomical structure of the motor cortex in the monkey and the cortical and sub-cortical connections of the different motor areas. In the second part, we review the neurophysiological literature showing that motor neurons are not only involved in movement execution, but also in the transformation of object physical features into motor programs appropriate to grasp them (through visuo-motor transformations). We also discuss evidence indicating that motor neurons can encode the goal of motor acts and the intention behind action execution. Then, we describe one of the mechanisms-the mirror mechanism-considered to be at the basis of action understanding and intention reading, and describe the anatomo-functional pathways through which information about the social context can reach the areas containing mirror neurons. Finally, we briefly show that a clear similarity exists between monkey and human in the organization of the motor and mirror systems. Based on monkey and human literature, we conclude that the mirror mechanism relies on a more extended network than previously thought, and possibly subserves basic social functions. We propose that this mechanism is also involved in preparing appropriate complementary response to observed actions, allowing two individuals to become attuned and cooperate in joint actions.},
	number = {July},
	journal = {Frontiers in Psychology},
	author = {Rozzi, Stefano and Coudé, Gino},
	year = {2015},
	pmid = {26236258},
	note = {arXiv: 1011.1669v3
ISBN: 1664-1078},
	keywords = {Grasp, Grasping, Human-Human Interaction (HHI), Intention, Macaque, Mirror neurons, Monkey Experiment, Motor, Motor goal, Parietal},
	pages = {1--19},
}

@article{huang_forecasting_nodate,
	title = {Forecasting the {Dynamics} of {Human} {Interaction}},
	author = {Huang, De-an and Kitani, Kris M},
	note = {ISBN: 9783319105840},
	keywords = {Action Recognition, Human Activity, Prediction},
}

@article{lasota_multiple-predictor_2017,
	title = {A multiple-predictor approach to human motion prediction},
	issn = {10504729},
	doi = {10.1109/ICRA.2017.7989265},
	abstract = {— The ability to accurately predict human motion is imperative for any human-robot interaction application in which the human and robot interact in close proximity to one another. Although a variety of human motion prediction approaches have already been developed, they are often de-signed for specific types of tasks or motions, and thus do not generalize well. Furthermore, it is not always obvious which of these methods is appropriate for a given task, making human motion prediction difficult to implement in practice. We address this problem by introducing a multiple-predictor system (MPS) for human motion prediction. In our approach, the system learns directly from task data in order to determine the most favorable parameters for each implemented prediction method and which combination of these predictors to use. Our implementation consists of three complementary methods: velocity-based position projection, time series classification, and sequence prediction. We describe the process of forming the MPS and our evaluation of its performance against the individual methods in terms of accuracy of predictions of human position over a range of look-ahead time values. We report that our method leads to a reduction in mean error of 18.5\%, 28.9\%, and 37.3\% when compared with the three individual methods, respectively.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Lasota, Przemyslaw A. and Shah, Julie A.},
	year = {2017},
	pmid = {1304422},
	note = {Publisher: IEEE
ISBN: 9781509046331},
	keywords = {Human Activity, Markov, Motion, Prediction},
	pages = {2300--2307},
}

@article{li_role_2015,
	title = {Role adaptation of human and robot in collaborative tasks},
	volume = {2015-June},
	issn = {10504729},
	doi = {10.1109/ICRA.2015.7139983},
	abstract = {In this paper, a role adaptation method is de- veloped for human-robot collaboration based on game theory. This role adaptation is engaged whenever the interaction force changes, causing the proportion of control sharing between human and robot to vary. In one boundary condition, the robot takes full control of the system when there is no human intervention. In the other boundary condition, it becomes a follower when the human exhibits strong intention to lead the task. Experimental results show that the proposed method yields better overall performance than fixed-role interactions.},
	number = {June},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Li, Yanan and Tee, Keng Peng and Chan, Wei Liang and Yan, Rui and Chua, Yuanwei and Limbu, Dilip Kumar},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781479969227},
	pages = {5602--5607},
}

@article{takagi_physically_2017,
	title = {Physically interacting individuals estimate the partner's goal to enhance their movements},
	volume = {1},
	issn = {23973374},
	doi = {10.1038/s41562-017-0054},
	abstract = {Takagi and colleagues present a model of how human pairs learn movements through touch. Participants learn in the same way when the model is applied to a robotic partner. This is important for the development of physical assistance robotics.},
	number = {3},
	journal = {Nature Human Behaviour},
	author = {Takagi, Atsushi and Ganesh, Gowrishankar and Yoshioka, Toshinori and Kawato, Mitsuo and Burdet, Etienne},
	year = {2017},
	note = {Publisher: Macmillan Publishers Limited, part of Springer Nature.
ISBN: 4156201600},
	keywords = {Human-Human Interaction (HHI), Motion, Prediction},
}

@article{njeri_mwangi_dyadic_2018,
	title = {Dyadic {Gaze} {Patterns} {During} {Child}-{Robot} {Collaborative} {Gameplay} in a {Tutoring} {Interaction}},
	doi = {10.0/Linux-x86_64},
	abstract = {This study examines patterns of coordinated gaze between a child and a robot (NAO) during a card matching game, ‘Memory’. Dyadic gaze behavior like mutual gaze, gaze following and joint attention are indications both of child’s engagement with the robot and of the quality of child-robot interaction. Eighteen children interacted with a robot tutor in two settings. In the first setting, the robot tutor gave clues to assist children in finding the matching cards, and in the other setting, the robot tutor only looked at the participants during the play. We investigated the coordination between child and robots’ gaze behaviors. We found that more occurrences of mutual gaze and gaze following made the children aware of the gaze hints given by the robot and improved the efficacy of the robot tutor as a helping agent. This study, therefore, provides guidelines for gaze behaviors design to enrich child-robot interaction in a tutoring context.},
	author = {Njeri Mwangi, Eunice and Barakova, Emilia I and Dã, Marta and CatalÃ, Andreu and Rauterberg, Matthias},
	year = {2018},
	note = {ISBN: 9781538679807},
	keywords = {Gaze, Human-Robot Interaction(HRI), Modelling, Robots in Education, Therapy and Rehabilitation},
}

@article{zunino_what_nodate,
	title = {What {Will} {I} {Do} {Next} ? {The} {Intention} from {Motion} {Experiment}},
	author = {Zunino, Andrea and Cavazza, Jacopo and Koul, Atesh and Cavallo, Andrea and Becchio, Cristina and Murino, Vittorio and Analysis, Pattern and Vision, Computer and Istituto, Pavis and Genova, Tecnologia},
	note = {arXiv: 1708.01034v1},
	keywords = {Action understanding, Human Activity, Motion},
}

@article{bugur_computational_nodate,
	title = {A {Computational} {Model} {For} {Action} {Prediction} {Development}},
	author = {Bugur, Serkan and Nagai, Yukie and Oztop, Erhan and Ugur, Emre},
	keywords = {Action Recognition, Infants, Prediction},
	pages = {8--12},
}

@article{macinnes_wearable_2018,
	title = {Wearable {Eye}-tracking for {Research} : comparisons across devices},
	doi = {https://doi.org/10.1101/299925},
	journal = {bioRxiv},
	author = {MacInnes, Jeff},
	year = {2018},
	keywords = {Eye tracker, Eyes, Gaze},
}

@article{sciutti_interacting_2017,
	title = {Interacting with robots to investigate the bases of social interaction},
	volume = {25},
	issn = {15344320},
	doi = {10.1109/TNSRE.2017.2753879},
	abstract = {Humans show a great natural ability at interacting with each other. Such efficiency in joint actions depends on a synergy between planned collaboration and emergent coordination, a subconscious mechanism based on a tight link between action execution and perception. This link supports phenomena as mutual adaptation, synchronization and anticipation, which cut drastically the delays in the interaction and the need of complex verbal instructions and result in the establishment of joint intentions, the backbone of social interaction. From a neurophysiological perspective this is possible because the same neural system supporting action execution is responsible of the understanding and the anticipation of the observed action of others. Defining which human motion features allow for such emergent coordination with another agent would be crucial to establish more natural and efficient interaction paradigms with artificial devices, ranging from assistive and rehabilitative technology to companion robots. However, investigating the behavioral and neural mechanisms supporting natural interaction poses substantial problems. In particular, the unconscious processes at the basis of emergent coordination (e.g., unintentional movements or gazing) are very difficult - if not impossible - to restrain or control in a quantitative way for a human agent. Moreover, during an interaction, participants influence each other continuously in a complex way, resulting in behaviors that go beyond experimental control. In this paper we propose robotics technology as a potential solution to this methodological problem. Robots indeed can establish an interaction with a human partner, contingently reacting to his actions without losing the controllability of the experiment or the naturalness of the interactive scenario. A robot could represent an "interactive probe" to assess the sensory and motor mechanisms underlying human-human interaction. We discuss this proposal with examples from our research with the humanoid robot iCub, showing how an interactive humanoid robot could be a key tool to serve the investigation of the psychological and neuroscientific bases of social interaction.},
	number = {12},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	author = {Sciutti, Alessandra and Sandini, Giulio},
	year = {2017},
	pmid = {29035218},
	note = {Publisher: IEEE},
	keywords = {Action understanding, Anticipation, Biological motion, Gaze, Human-Robot Interaction(HRI), Humanoid, Motor resonance, Neurophysiological, Second-person neuroscience.},
	pages = {2295--2304},
}

@article{huang_enabling_2018,
	title = {Enabling robots to communicate their objectives},
	issn = {15737527},
	doi = {10.1007/s10514-018-9771-0},
	abstract = {The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.},
	journal = {Autonomous Robots},
	author = {Huang, Sandy H. and Held, David and Abbeel, Pieter and Dragan, Anca D.},
	year = {2018},
	pmid = {9097023},
	note = {arXiv: 1702.03465
ISBN: 1234567245},
	keywords = {Action understanding, Explainable artificial intelligence, Human-Robot Interaction(HRI), Human-robot interaction, Inverse reinforcement learning, Robotics, Transparency},
	pages = {1--18},
}

@article{nguyen_merging_2018,
	title = {Merging physical and social interaction for effective human-robot collaboration},
	number = {Humanoids},
	author = {Nguyen, Phuong D H and Bottarel, Fabrizio and Pattacini, Ugo and Hoffmann, Matej and Natale, Lorenzo and Metta, Giorgio},
	year = {2018},
	note = {ISBN: 9781538672822},
	keywords = {Handovers, Human-Robot Interaction(HRI)},
}

@article{moetesum_socially_2018,
	title = {Socially {Believable} {Robots} {Provisional} chapter {Socially} {Believable} {Robots}},
	url = {http://dx.doi.org/10.5772/intechopen.71375},
	doi = {10.5772/intechopen.71375},
	abstract = {Long-term companionship, emotional attachment and realistic interaction with robots have always been the ultimate sign of technological advancement projected by sci-fi literature and entertainment industry. With the advent of artificial intelligence, we have indeed stepped into an era of socially believable robots or humanoids. Affective computing has enabled the deployment of emotional or social robots to a certain level in social settings like informatics, customer services and health care. Nevertheless, social believability of a robot is communicated through its physical embodiment and natural expressiveness. With each passing year, innovations in chemical and mechanical engineering have facilitated lifelike embodiments of robotics; however, still much work is required for developing a "social intelligence" in a robot in order to maintain the illusion of dealing with a real human being. This chapter is a collection of research studies on the modeling of complex autonomous systems. It will further shed light on how different social settings require different levels of social intelligence and what are the implications of integrating a socially and emotionally believable machine in a society driven by behaviors and actions.},
	author = {Moetesum, Momina and Siddiqi, Imran},
	year = {2018},
	keywords = {Action understanding, Robotics, anthropomorphism, cognitive systems, human computer interaction, humanoids, roboethics, social intelligence, social robots},
	pages = {3--24},
}

@article{tekden_modeling_2018,
	title = {Modeling the {Development} of {Infant} {Imitation} using {Inverse} {Reinforcement} {Learning}},
	author = {Tekden, Ahmet E and Ugur, Emre and Nagai, Yukie and Oztop, Erhan},
	year = {2018},
	keywords = {Developmental Stages, Imitation, Infants, Inverse reinforcement Learning, Machine Learning methods for robot development, Modelling, Sensorimotor development},
}

@article{vinciarelli_social_2009,
	title = {Social signal processing: {Survey} of an emerging domain},
	volume = {27},
	issn = {02628856},
	url = {http://dx.doi.org/10.1016/j.imavis.2008.11.007},
	doi = {10.1016/j.imavis.2008.11.007},
	abstract = {The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence - the ability to recognize human social signals and social behaviours like turn taking, politeness, and disagreement - in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for social signal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially aware computing. © 2008 Elsevier B.V. All rights reserved.},
	number = {12},
	journal = {Image and Vision Computing},
	author = {Vinciarelli, Alessandro and Pantic, Maja and Bourlard, Hervé},
	year = {2009},
	pmid = {28464927},
	note = {arXiv: NIHMS150003
Publisher: Elsevier B.V.
ISBN: 9781605583037},
	keywords = {Computer vision, Human, Human Activity, Human behaviour analysis, Social interactions, Social signals, Speech processing},
	pages = {1743--1759},
}

@article{vinciarelli_new_2015,
	title = {New {Social} {Signals} in a {New} {Interaction} {World}},
	volume = {1},
	issn = {2333-942X},
	doi = {10.1109/MSMC.2015.2441992},
	number = {2},
	journal = {IEEE Systems, Man, and Cybernetics Magazine},
	author = {Vinciarelli, Alessandro and Pentland, Alex (Sandy)},
	year = {2015},
	keywords = {Human Activity, Human-Computer Interaction, Human-Human Interaction (HHI)},
	pages = {10--17},
}

@article{wang_real_2016,
	title = {Real {Time} {Eye} {Gaze} {Tracking} with {Kinect}},
	doi = {10.1109/ICPR.2016.7900052},
	abstract = {Traditional gaze tracking systems rely on explicit infrared lights and high resolution cameras to achieve high performance and robustness. These systems, however, require complex setup and thus are restricted in lab research and hard to apply in practice. In this paper, we propose to perform gaze tracking with a consumer level depth sensor (Kinect). Leveraging on Kinect’s capability to obtain 3D coordinates, we propose an efficient model-based gaze tracking system. We first build a unified 3D eye model to relate gaze directions and eye features (pupil center, eyeball center, cornea center) through subject- dependent eye parameters. A personal calibration framework is further proposed to estimate the subject-dependent eye pa- rameters. Finally we can perform real time gaze tracking given the 3D coordinates of eye features from Kinect and the subject- dependent eye parameters from personal calibration procedure. Experimental results with 6 subjects prove the effectiveness of the proposed 3D eye model and the personal calibration framework. Furthermore, the gaze tracking system is able to work in real time (20 fps) and with low resolution eye images.},
	journal = {International Conference on Pattern Recognition},
	author = {Wang, Kang},
	year = {2016},
	note = {ISBN: 9781509048465},
	keywords = {Gaze, Human Activity, Human Experiment},
	pages = {2753--2758},
}

@inproceedings{ogata_falling_2007,
	title = {Falling {Motion} {Control} for {Humanoid} {Robots} {While} {Walking}},
	isbn = {978-1-4244-1862-6},
	doi = {10.1039/c39830000171},
	abstract = {Humanoid robots are prone to fall caused by disturbances. If the disturbance is weak, a humanoid robot can avoid falling by using feedback control. If the disturbance is strong, humanoid robots can perform an Ukemi motion: an active shock-reducing motion. This research proposes a technique for selecting an optimal strategy to handle disturbances while walking, and describes a method of generating Ukemi motion. This technique detects disturbances using sensor data while walking steadily. Moreover, an experiment with a real robot was performed where fall detection was computed using discriminant analysis of walking data labelled as fall and non-fall. Furthermore, this study uses the three-dimensional linear inverted pendulum mode (3D-LIPM), which has been utilized in gait generation of humanoid robots, to generate a solution for the center of gravity for the contracting/expanding fall correction movement. The effectiveness of the proposed technique was confirmed by the verification experiments.},
	author = {Ogata, Kunishiro and Jerada, Koji and Kuniyoshi, Yasuo},
	year = {2007},
	note = {Issue: 4
ISSN: 00224936},
	keywords = {Motion, Robotics, [Electronic Manuscript]},
	pages = {171--172},
}

@article{Sidiropoulos2018,
	title = {A human inspired handover policy using {Gaussian} {Mixture} {Models} and haptic cues},
	issn = {15737527},
	url = {https://doi.org/10.1007/s10514-018-9705-x},
	doi = {10.1007/s10514-018-9705-x},
	abstract = {A handover strategy is proposed that aims at natural and fluent robot to human object handovers. For the approaching phase, a globally asymptotically stable dynamical system (DS) is utilized, trained from human demonstrations and exploiting the existence of mirroring in the human wrist motion. The DS operates in the robot task space thus achieving independence with respect to the robot platform, encapsulating the position and orientation of the human wrist within a single DS. It is proven that the motion generated by such a DS, having as target the current wrist pose of the receiver's hand, is bounded and converges to the previously unknown handover location. Haptic cues based on load estimates at the robot giver ensure full object load transfer before grip release. The proposed strategy is validated with simulations and experiments in real settings.},
	journal = {Autonomous Robots},
	author = {Sidiropoulos, Antonis and Psomopoulou, Efi and Doulgeri, Zoe},
	year = {2018},
	note = {Publisher: Springer US},
	keywords = {Gaussian Mixture Model, Haptic communication, Human Demonstration, Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Motion, Physical human-robot interaction, Programming by Demonstration},
	pages = {1--16},
}

@article{sesma-sanchez_gaze_2012,
	title = {Gaze {Estimation} {Interpolation} {Methods} {Based} on {Binocular} {Data}},
	volume = {59},
	number = {8},
	author = {Sesma-sanchez, Laura and Villanueva, Arantxa and Cabeza, Rafael},
	year = {2012},
	keywords = {Gaze, Human Activity, Human Experiment},
	pages = {2235--2243},
}

@article{hogans_coordination_1985,
	title = {The coordination of arm movements: {An} experimentally {Confirmed} {Mathematical} {Model}},
	volume = {5},
	issn = {0270-6474},
	doi = {4020415},
	number = {7},
	author = {Hogans, Neville and Flash, Tamar},
	year = {1985},
	pmid = {4020415},
	note = {ISBN: 0270-6474 (Print)},
	keywords = {Human-Human Interaction (HHI), Modelling, Motion},
	pages = {1688--1703},
}

@article{hoppe_eye_2018,
	title = {Eye {Movements} {During} {Everyday} {Behavior} {Predict} {Personality} {Traits}},
	volume = {12},
	issn = {1662-5161},
	url = {http://journal.frontiersin.org/article/10.3389/fnhum.2018.00105/full},
	doi = {10.3389/fnhum.2018.00105},
	abstract = {Besides allowing us to perceive our surroundings, eye movements are also a window into our mind and a rich source of information on who we are, how we feel, and what we do. Here we show that eye movements during an everyday task predict aspects of our personality. We tracked eye movements of 42 participants while they ran an errand on a university campus and subsequently assessed their personality traits using well-established questionnaires. Using a state-of-the-art machine learning method and a rich set of features encoding different eye movement characteristics, we were able to reliably predict four of the Big Five personality traits (neuroticism, extraversion, agreeableness, conscientiousness) as well as perceptual curiosity only from eye movements. Further analysis revealed new relations between previously neglected eye movement characteristics and personality. Our findings demonstrate a considerable influence of personality on everyday eye movement control, thereby complementing earlier studies in laboratory settings. Improving automatic recognition and interpretation of human social signals is an important endeavor, enabling innovative design of human-computer systems capable of sensing spontaneous natural user behavior to facilitate efficient interaction and personalization. Eye movements facilitate efficient sampling of visual information from the world around us. For example, in everyday social interactions, we often understand, predict, and explain the behavior and emotional states of others by how their eyes move (Emery, 2000). The exact mechanisms by which eye movement is controlled, and the range of factors that can influence it, are subject to intense research (Wolfe, 1994; Martinez-Conde et al., 2004; Foulsham et al., 2011; Rucci and Victor, 2015). Understanding the types of information eye movements convey is of current interest to a range of fields, from psychology and the social sciences to computer science (Henderson et al., 2013; Bulling et al., 2011; Bulling and Zander, 2014; Bixler and D'Mello, 2015; Steil and Bulling, 2015). One emerging body of research suggests that the way in which we move our eyes is modulated by who we are-by our personality (Isaacowitz, 2005; Rauthmann et al., 2012; Risko et al., 2012; Baranes et al., 2015; Hoppe et al., 2015). Personality traits characterize an individual's patterns of behavior, thinking, and feeling (Kazdin, 2000). Studies reporting relationships between personality traits and eye movements suggest that people with similar traits tend to move their eyes in similar ways. Optimists, for example, spend less time inspecting negative emotional stimuli (e.g., skin cancer images) than pessimists (Isaacowitz, 2005). Individuals high in openness spend a longer time fixating and dwelling on locations when watching abstract animations (Rauthmann et al., 2012), and perceptually curious individuals inspect more of the regions in a naturalistic scene (Risko et al., 2012). But pioneering studies on the association between personality and eye movements share two methodological limitations.},
	number = {April},
	journal = {Frontiers in Human Neuroscience},
	author = {Hoppe, Sabrina and Loetscher, Tobias and Morey, Stephanie A. and Bulling, Andreas},
	year = {2018},
	pmid = {29713270},
	note = {ISBN: 1662-5161},
	keywords = {2000, Eyes, Gaze, Human Activity, Human Experiment, and, and explain the behavior, by how their eyes, emery, emotional states of others, example, eye movements facilitate efficient, eye tracking, eye-based user modeling, for, from the world around, gaze behavior, in everyday social interactions, machine learning, move, personality, predict, real world, sampling of visual information, the exact mechanisms by, us, we often understand, which},
	pages = {1--8},
}

@article{mirrazavi_salehian_unified_2018,
	title = {A unified framework for coordinated multi-arm motion planning},
	volume = {424},
	issn = {17413176},
	doi = {10.1177/0278364918765952},
	abstract = {Coordination is essential in the design of dynamic control strategies for multi-arm robotic systems. Given the complexity of the task and dexterity of the system, coordination constraints can emerge from different levels of planning and control. Primarily, one must consider task-space coordination, where the robots must coordinate with each other, with an object or with a target of interest. Coordination is also necessary in joint space, as the robots should avoid self-collisions at any time. We provide such joint-space coordination by introducing a centralized inverse kinematics (IK) solver under self-collision avoidance constraints, formulated as a quadratic program and solved in real-time. The space of free motion is modeled through a sparse non-linear kernel classification method in a data-driven learning approach. Moreover, we provide multi-arm task-space coordination for both synchronous or asynchronous behaviors. We define a synchronous behavior as that in which the robot arms must coordinate with ...},
	journal = {International Journal of Robotics Research},
	author = {Mirrazavi Salehian, Seyed Sina and Figueroa, Nadia and Billard, Aude},
	year = {2018},
	note = {arXiv: 1508.04886v1
ISBN: 0037549716666},
	keywords = {Human-Robot Interaction(HRI), Motion, Multi-arm motion planning, coordination, dynamical systems, self-collision avoidance},
}

@article{li_minimized_2017,
	title = {A minimized falling damage method for humanoid robots},
	volume = {14},
	issn = {17298814},
	doi = {10.1177/1729881417728016},
	abstract = {One issue with air transportation and sustainability is that although aviation could be considered economically and socially sustainable, it does generate environmental concerns. The aim of this paper is to examine public attitudes towards air transportation and sustainability, in order to determine how individuals value sustainability in relation to air travel. This empirical paper is based on two large survey data sets, one from the East Midlands region of the United Kingdom and one from the East Coast of the United States. After an initial review of relevant literature and policy, a range of attitudinal statements from the surveys are examined. These statements cover the economic and social benefits of air transportation, the contribution of air travel to climate change, and environmental responses. The analysis demonstrates the high value individuals put on the economic and social sustainability aspects of air transportation. Although many acknowledge aviation's contribution to climate change, few are willing to respond in terms of paying more to offset the negative environmental effects of aviation or to fly less. When analysing the value of sustainability by population sub-group, flight frequency and gender are highlighted as key variables in terms of environmental attitudes. © 2013 Elsevier Ltd.},
	number = {5},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Li, Qingqing and Chen, Xuechao and Zhou, Yuhang and Yu, Zhangguo and Zhang, Weimin and Huang, Qiang},
	year = {2017},
	keywords = {Falling backward, Human Activity, Human Demonstration, Humanoid robots, Inverted pendulum, Motion, Optimization algorithm, Robotics},
	pages = {1--10},
}

@article{aronson_eye-hand_2018,
	title = {Eye-{Hand} {Behavior} in {Human}-{Robot} {Shared} {Manipulation}},
	issn = {21672148},
	url = {http://doi.acm.org/10.1145/3171221.3171287},
	doi = {10.1145/3171221.3171287},
	abstract = {Shared autonomy systems enhance people's abilities to perform activities of daily living using robotic manipulators. Recent systems succeed by first identifying their operators' intentions, typically by analyzing the user's joystick input. To enhance this recognition, it is useful to characterize people's behavior while performing such a task. Furthermore, eye gaze is a rich source of information for understanding operator intention. The goal of this paper is to provide novel insights into the dynamics of control behavior and eye gaze in human-robot shared manipulation tasks. To achieve this goal, we conduct a data collection study that uses an eye tracker to record eye gaze during a human-robot shared manipulation activity, both with and without shared autonomy assistance. We process the gaze signals from the study to extract gaze features like saccades, fixations, smooth pursuits, and scan paths. We analyze those features to identify novel patterns of gaze behaviors and highlight where these patterns are similar to and different from previous findings about eye gaze in human-only manipulation tasks. The work described in this paper lays a foundation for a model of natural human eye gaze in human-robot shared manipulation.},
	journal = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Aronson, Reuben M and Santini, Thiago and Kübler, Thomas C and Kasneci, Enkelejda and Srinivasa, Siddhartha and Admoni, Henny},
	year = {2018},
	note = {ISBN: 9781450349536},
	keywords = {Eyes, Gaze, Human-Robot Interaction(HRI), eye gaze, eye tracking, human-robot interaction, nonverbal communication, shared autonomy},
	pages = {4--13},
}

@article{nadel_toward_2004,
	title = {Toward communication: {First} imitations in infants, low-functioning children with autism and robots},
	volume = {5},
	issn = {1572-0373},
	url = {http://www.jbe-platform.com/content/journals/10.1075/is.5.1.04nad},
	doi = {10.1075/is.5.1.04nad},
	abstract = {Adopting a functionalist perspective, we emphasize the interest of considering imitation as a single capacity with two functions: communication and learning. These two functions both imply such capacities as detection of novelty, attraction toward moving stimuli and perception-action coupling. We propose that the main difference between the processes involved in the two functions is that, in the case of learning, the dynamics is internal to the system constituted by an individual whereas in the case of communication, the dynamics concerns the system composed by the perception of one individual coupled with the action of the other.},
	number = {1},
	journal = {Interaction Studies},
	author = {Nadel, Jacqueline and Revel, Arnaud and Andry, Pierre and Gaussier, Philippe},
	year = {2004},
	keywords = {Autonomous, Children with autism, Communication, Development, Imitation, Infant, Robotics, robot/s},
	pages = {45--74},
}

@article{chen_planning_2018,
	title = {Planning with {Trust} for {Human}-{Robot} {Collaboration}},
	issn = {21672148},
	url = {http://dl.acm.org/citation.cfm?doid=3171221.3171264},
	doi = {10.1145/3171221.3171264},
	abstract = {Trust is essential for human-robot collaboration and user adoption of autonomous systems, such as robot assistants. This paper introduces a computational model which integrates trust into robot decision-making. Specifically, we learn from data a partially observable Markov decision process (POMDP) with human trust as a latent variable. The trust-POMDP model provides a principled approach for the robot to (i) infer the trust of a human teammate through interaction, (ii) reason about the effect of its own actions on human behaviors, and (iii) choose actions that maximize team performance over the long term. We validated the model through human subject experiments on a table-clearing task in simulation (201 participants) and with a real robot (20 participants). The results show that the trust-POMDP improves human-robot team performance in this task. They further suggest that maximizing trust in itself may not improve team performance.},
	journal = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction - HRI '18},
	author = {Chen, Min and Nikolaidis, Stefanos and Soh, Harold and Hsu, David and Srinivasa, Siddhartha},
	year = {2018},
	note = {arXiv: 1801.04099
ISBN: 9781450349536},
	keywords = {Human-Robot Interaction(HRI), Markov, human-robot collaboration, partially observable markov decision process (pomd, trust models},
	pages = {307--315},
}

@article{mainprice_predicting_2015,
	title = {Predicting human reaching motion in collaborative tasks using {Inverse} {Optimal} {Control} and iterative re-planning},
	issn = {1050-4729},
	doi = {10.1109/ICRA.2015.7139282},
	abstract = {To enable safe and efficient human-robot collaboration in shared workspaces, it is important for the robot to predict how a human will move when performing a task. While predicting human motion for tasks not known a priori is very challenging, we argue that single-arm reaching motions for known tasks in collaborative settings (which are especially relevant for manufacturing) are indeed predictable. Two hypotheses underlie our approach for predicting such motions: First, that the trajectory the human performs is optimal with respect to an unknown cost function, and second, that human adaptation to their partner's motion can be captured well through iterative replanning with the above cost function. The key to our approach is thus to learn a cost function which “explains” the motion of the human. To do this, we gather example trajectories from two participants performing a collaborative assembly task using motion capture. We then use Inverse Optimal Control to learn a cost function from these trajectories. Finally, we predict a human's motion for a given task by iteratively replanning a trajectory for a 23 DoF human kinematic model using the STOMP algorithm with the learned cost function in the presence of a moving collaborator. Our results suggest that our method outperforms baseline methods and generalizes well for tasks similar to those that were demonstrated.},
	journal = {Robotics and Automation (ICRA), 2015 IEEE International Conference on},
	author = {Mainprice, J and Hayne, R and Berenson, D},
	year = {2015},
	pmid = {25373136},
	note = {arXiv: 1606.02111
ISBN: VO -},
	keywords = {Collaboration, Cost function, Hidden Markov models, Human Activity, Motion, Optimal control, Planning, Prediction algorithms, STOMP algorithm, Trajectory, collaborative tasks, cost function, human kinematic model, human reaching motion prediction, human-robot collaboration, human-robot interaction, inverse optimal control, iterative methods, iterative re-planning, motion capture, optimal control, path planning, stochastic trajectory optimization for motion plan},
	pages = {885--892},
}

@article{huang_adaptive_2015,
	title = {Adaptive {Coordination} {Strategies} for {Human}-{Robot} {Handovers}},
	issn = {2330765X},
	doi = {10.15607/RSS.2015.XI.031},
	abstract = {Handovers of objects are critical interactions that frequently arise in physical collaborations. In such interactions, humans naturally monitor the pace and workload of their partners and adapt their handovers accordingly. In this paper, we investigate how robots designed to engage in physical collaborations may achieve similar adaptivity in performing handovers. To that end, we collected and analyzed data from human dyads performing a common household task unloading a dish rack where receivers had different levels of task demands. We identified two coordination strategies that enabled givers to adapt to receivers task demands. We then formulated and implemented these strategies on a robotic manipulator. The implemented autonomous system was evaluated in a human-robot interaction study against two baselines that use proactive and reactive coordination methods. The results show a tradeoff between team performance and user experience when human receivers had greater task demands. In particular, the proactive method provided the greatest levels of team performance but offered the poorest user experience compared to the reactive and adaptive methods. The reactive method, while improving user experience over the proactive method, resulted in the poorest team performance. Our adaptive method maintained this improved user experience while offering an improved team performance compared to the reactive method. Our findings offer insights into the tradeoffs involved in the use of these methods and inform the future design of handover interactions for robots.},
	journal = {Robotics: Science and Systems XI},
	author = {Huang, Chien-Ming and Cakmak, Maya and Mutlu, Bilge},
	year = {2015},
	note = {ISBN: 9780992374716},
	keywords = {Handovers, Human-Robot Interaction(HRI)},
}

@article{parastegari_modeling_2017,
	title = {Modeling human reaching phase in human-human object handover with application in robot-human handover},
	volume = {2017-Septe},
	issn = {21530866},
	doi = {10.1109/IROS.2017.8206205},
	abstract = {© 2017 IEEE. In robot to human object handover, the configuration (position and orientation) in which the object is transferred should be selected so that the handover is safe and comfortable for the human. The trajectory on which the robot moves the object to the point of transfer should be also selected so that the robot intention is clear and the handover feels natural to the human. In this paper, we propose to select the configuration for the transfer and the trajectory to reach this configuration based on what humans do in human-human handovers. We describe a human study designed to investigate the human-human handover and propose an ergonomic model that can predict object transfer position observed in the study. A human-robot experiment is then conducted that shows that the proposed model generates transfer positions that match the preferred height and distance relative to the human.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Parastegari, Sina and Abbasi, Bahareh and Noohi, Ehsan and Zefran, Milos},
	year = {2017},
	note = {ISBN: 9781538626825},
	keywords = {Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Modelling},
	pages = {3597--3602},
}

@article{revel_emergence_2009,
	title = {Emergence of structured interactions: {From} a theoretical model to pragmatic robotics},
	volume = {22},
	issn = {08936080},
	url = {http://dx.doi.org/10.1016/j.neunet.2009.01.005},
	doi = {10.1016/j.neunet.2009.01.005},
	abstract = {In this article, we present two neural architectures for the control of socially interacting robots. Beginning with a theoretical model of interaction inspired by developmental psychology, biology and physics, we present two sub-cases of the model that can be interpreted as "turn-taking" and "synchrony" at the behavioral level. These neural architectures are both detailed and tested in simulation. A robotic experiment is even presented for the "turn-taking" case. We then discuss the interest of such behaviors for the development of further social abilities in robots. © 2009 Elsevier Ltd. All rights reserved.},
	number = {2},
	journal = {Neural Networks},
	author = {Revel, A. and Andry, P.},
	year = {2009},
	pmid = {19243912},
	note = {arXiv: 1707.03042
Publisher: Elsevier Ltd},
	keywords = {Development, Modelling, Neural models, Robotics, Social interaction},
	pages = {116--125},
}

@article{mortl_modeling_2012,
	title = {Modeling inter-human movement coordination: {Synchronization} governs joint task dynamics},
	volume = {106},
	issn = {03401200},
	doi = {10.1007/s00422-012-0492-8},
	abstract = {Human interaction partners tend to synchronize their movements during repetitive actions such as walking. Research of inter-human coordination in purely rhythmic action tasks reveals that the observed patterns of interaction are dominated by synchronization effects. Initiated by our finding that human dyads synchronize their arm movements even in a goal-directed action task, we present a step-wise approach to a model of inter-human movement coordination. In an experiment, the hand trajectories of ten human dyads are recorded. Governed by a dynamical process of phase synchronization, the participants establish in-phase as well as anti-phase relations. The emerging relations are successfully reproduced by the attractor dynamics of coupled phase oscillators inspired by the Kuramoto model. Three different methods on transforming the motion trajectories into instantaneous phases are investigated and their influence on the model fit to the experimental data is evaluated. System identification technique allows us to estimate the model parameters, which are the coupling strength and the frequency detuning among the dyad. The stability properties of the identified model match the relations observed in the experimental data. In short, our model predicts the dynamics of inter-human movement coordination. It can directly be implemented to enrich human-robot interaction.},
	number = {4-5},
	journal = {Biological Cybernetics},
	author = {Mörtl, Alexander and Lorenz, Tamara and Vlaskamp, Björn N.S. and Gusrialdi, Azwirman and Schubö, Anna and Hirche, Sandra},
	year = {2012},
	pmid = {22648567},
	note = {ISBN: 0340-1200},
	keywords = {Coupled oscillators, Dynamical model, Human movement coordination, Human-Human Interaction (HHI), Joint-action, Motion, Phase estimation, Phase synchronization},
	pages = {241--259},
}

@article{lorenz_synchronization_2011,
	title = {Synchronization in a goal-directed task: {Human} {Movement} {Coordination} with each other and robotic partners},
	journal = {RO-MAN 2011 - The 20th IEEE International Symposium on Robot and Human Interactive Communication},
	author = {Lorenz, Tamara and Mörtl, Alexander and Vlaskamp, Björn and Schubö, Anna and Hirche, Sandra},
	year = {2011},
	note = {ISBN: 9781457715723},
	keywords = {Human Activity, Robotics},
}

@article{prepin_human-machine_2007,
	title = {Human-machine interaction as a model of machine-machine interaction: {How} to make machines interact as humans do},
	volume = {21},
	issn = {01691864},
	doi = {10.1163/156855307782506192},
	abstract = {Turn-taking is one of the main features of communicative systems.{\textbackslash}nIn particular, it is one of the bases allowing robust interactions{\textbackslash}nin imitation, thanks to its two linked aspects, i.e., communication{\textbackslash}nand learning. In this article, we propose a simple model based on{\textbackslash}nthe interaction of two neural oscillators inhibiting each other which{\textbackslash}nexplain how 'turn-taking' may emerge dynamically between two agents.{\textbackslash}nAn implementation of the model on a simple robotic platform made{\textbackslash}nof one CCD camera and one simple arm (ADRIANA platform) is detailed.{\textbackslash}nResults showing the emergence of a 'turn-taking' dynamics on this{\textbackslash}nplatform are discussed and an extension in simulation for a larger{\textbackslash}nscale of parameters in order to validate robustness is given.},
	number = {15},
	journal = {Advanced Robotics},
	author = {Prepin, Ken and Revel, Arnaud},
	year = {2007},
	keywords = {Coupled oscillators, Human Experiment, Imitation, Synchrony, Turn-taking},
	pages = {1709--1723},
}

@article{zhang_computational_2005,
	title = {{COMPUTATIONAL} {MODELLING} {OF}{\textbackslash}{nVISUAL} {ATTENTION}},
	volume = {3},
	issn = {1471-003X},
	doi = {10.1038/35058500},
	number = {March},
	author = {Zhang, L.},
	year = {2005},
	pmid = {11256080},
	note = {arXiv: 1011.1669v3
ISBN: 1471-0048},
	keywords = {Modelling, Vision},
	pages = {194--203},
}

@article{hasnain_intuitive_2013,
	title = {Intuitive human robot interaction based on unintentional synchrony: {A} psycho-experimental study},
	issn = {978-1-4799-1036-6},
	doi = {10.1109/DevLrn.2013.6652569},
	abstract = {Inspired by studies of interpersonal coordinations, we assumed that unintentional synchrony is a fundamental parameter to initiate and maintain Human Machine interactions. We developed, in our previous works, a neural model allowing a robot to synchronize its behavior depending on the human movement frequency, and thus to choose this interacting partner on the basis of synchrony detection between its own learned dynamics and the visual stimuli induced by the human motion. To confirm or deny our assumptions we present here a psychological study to measure unintentional synchronization during Unidirectional and Bidirectional Human Robot Interaction using our previously proposed model for initiating the interaction and focusing the robot attention on a selected partner. The experimental results demonstrated that bidirectional intuitive interaction leading to possible unintentional synchronization is primordial to obtain natural human robot interactions using a minimal cognitive load (unintentional behavior).},
	journal = {2013 IEEE 3rd Joint International Conference on Development and Learning and Epigenetic Robotics, ICDL 2013 - Electronic Conference Proceedings},
	author = {Hasnain, Syed Khursheed and Mostafaoui, Ghiles and Salesse, Robin and Marin, Ludovic and Gaussier, Philippe},
	year = {2013},
	note = {ISBN: 9781479910366},
	keywords = {Human-Robot Interaction(HRI), Motion},
}

@article{gaussier_reaching_2017,
	title = {Reaching and {Grasping} : what we can learn from psychology and robotics},
	journal = {Hal},
	author = {Gaussier, Philippe and Pitti, Alexandre},
	year = {2017},
	keywords = {Grasp, Human-Human Interaction (HHI), Psychology, Robotics},
	pages = {1--11},
}

@article{issartel_interpersonal_2009,
	title = {Interpersonal motor coordination: {From} human–human to human–robot interactions},
	volume = {10},
	issn = {1572-0373},
	url = {https://benjamins.com/catalog/is.10.3.09mar},
	doi = {10.1075/is.10.3.09mar},
	abstract = {Here, we propose that bidirectionality in implicit motor coordination between humanoid robots and humans could enhance the social competence of human–robot interactions. We first detail some questions pertaining to human–robot interactions, introducing the Uncanny Valley hypothesis. After introducing a framework pertinent for the understanding of natural social interactions, motor resonance, we examine two behaviors derived from this framework: motor coordination, investigated in and informative about human–human interaction, and motor interference, which demonstrate the relevance of the motor resonance framework to describe human perception of humanoid robots. These two lines of investigation are then put together to “close the loop” by proposing to implement a key feature of motor coordination, bidirectionality, in robots’ behavior. Finally, we discuss the feasibility of implementing motor coordination between humanoid robots and humans, and the consequences of this implementation in enhancing the social competence of robots interacting with humans. Keywords: interpersonal interaction, motor resonance, motor coordination, motor interference, social robotics, anthropomorphism},
	number = {3},
	journal = {Interaction Studies},
	author = {Issartel, Johann},
	year = {2009},
	keywords = {Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Motion, Robotics},
	pages = {479--504},
}

@article{nowak_functional_2017,
	title = {Functional synchronization: {The} emergence of coordinated activity in human systems},
	volume = {8},
	issn = {16641078},
	doi = {10.3389/fpsyg.2017.00945},
	abstract = {The topical landscape of psychology is highly compartmentalized, with distinct phenomena explained and investigated with recourse to theories and methods that have little in common. Our aim in this article is to identify a basic set of principles that underlie otherwise diverse aspects of human experience at all levels of psychological reality, from neural processes to group dynamics. The core idea is that neural, behavioral, mental, and social structures emerge through the synchronization of lower-level elements (e.g., neurons, muscle movements, thoughts and feelings, individuals) into a functional unit-a coherent structure that functions to accomplish tasks. The coherence provided by the formation of functional units may be transient, persisting only as long as necessary to perform the task at hand. This creates the potential for the repeated assembly and disassembly of functional units in accordance with changing task demands. This perspective is rooted in principles of complexity science and non-linear dynamical systems and is supported by recent discoveries in neuroscience and recent models in cognitive and social psychology. We offer guidelines for investigating the emergence of functional units in different domains, thereby honoring the topical differentiation of psychology while providing an integrative foundation for the field.},
	number = {JUN},
	journal = {Frontiers in Psychology},
	author = {Nowak, Andrzej and Vallacher, Robin R. and Zochowski, Michal and Rychwalska, Agnieszka},
	year = {2017},
	pmid = {28659842},
	keywords = {Brain, Function, Human Activity, Human-Human Interaction (HHI), Mind, Neuroscience, Self-organization, Social systems, Synchronization},
	pages = {1--15},
}

@article{feix_analysis_2014,
	title = {Analysis of human grasping behavior: {Correlating} tasks, objects and grasps},
	volume = {7},
	issn = {19391412},
	doi = {10.1109/TOH.2014.2326867},
	abstract = {—This paper is the second in a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. It investigates the tasks performed during the daily work of two housekeepers and two machinists and correlates grasp type and object properties with the attributes of the tasks being performed. The task or activity is classified according to the force required, the degrees of freedom, and the functional task type. We found that 46 percent of tasks are constrained, where the manipulated object is not allowed to move in a full six degrees of freedom. Analyzing the interrelationships between the grasp, object, and task data show that the best predictors of the grasp type are object size, task constraints, and object mass. Using these attributes, the grasp type can be predicted with 47 percent accuracy. Those parameters likely make useful heuristics for grasp planning systems. The results further suggest the common sub-categorization of grasps into power, intermediate, and precision categories may not be appropriate, indicating that grasps are generally more multi-functional than previously thought. We find large and heavy objects are grasped with a power grasp, but small and lightweight objects are not necessarily grasped with precision grasps—even with grasped object size less than 2 cm and mass less than 20 g, precision grasps are only used 61 percent of the time. These results have important implications for robotic hand design and grasp planners, since it appears while power grasps are frequently used for heavy objects, they can still be quite practical for small, lightweight objects.},
	number = {4},
	journal = {IEEE Transactions on Haptics},
	author = {Feix, Thomas and Bullock, Ian M. and Dollar, Aaron M.},
	year = {2014},
	pmid = {25248214},
	note = {ISBN: 1939-1412},
	keywords = {Human Grasp, Human grasping, activities of daily living, manipulation, prosthetics, robotic hands},
	pages = {430--441},
}

@article{pinto_supersizing_2016,
	title = {Supersizing self-supervision: {Learning} to grasp from {50K} tries and 700 robot hours},
	volume = {2016-June},
	issn = {10504729},
	doi = {10.1109/ICRA.2016.7487517},
	abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Pinto, Lerrel and Gupta, Abhinav},
	year = {2016},
	pmid = {25373136},
	note = {arXiv: 1509.06825
ISBN: 9781467380263},
	keywords = {Grasp, Robotics},
	pages = {3406--3413},
}

@article{jarrasse_slaves_2014,
	title = {Slaves no longer: {Review} on role assignment for human-robot joint motor action},
	volume = {22},
	issn = {10597123},
	doi = {10.1177/1059712313481044},
	abstract = {This paper summarizes findings on the growing field of role assignment policies for human-robot motor interaction. This topic has been investigated by researchers in the psychological theory of joint action, in human intention detection, force control, human-human physical interaction, as well as roboticists interested in developing robots with capabilities for efficient motor interaction with humans. Our goal is to promote fruitful interaction between these distinct communities by: (i) examining the role assignment policies for human-robot joint motor action in experimental psychology and robotics studies; and (ii) informing researchers in human-human interaction on existing work in the robotic field. After an overview of roles assignment in current robotic assistants, this paper examines key results about shared control between a robot and a human performing interactive motor tasks. Research on motor interaction between two humans has inspired recent developments that may extend the use of robots to applications requiring continuous mechanical interaction with humans.},
	number = {1},
	journal = {Adaptive Behavior},
	author = {Jarrassé, Nathanaël and Sanguineti, Vittorio and Burdet, Etienne},
	year = {2014},
	keywords = {Human-Robot Interaction(HRI), Joint-action, Motion, Motor joint action, education, human-human interaction, master-slave, physical human-robot interaction (pHRI), role assignment policies},
	pages = {70--82},
}

@article{feix_analysis_2014-1,
	title = {Analysis of human grasping behavior: {Object} characteristics and grasp type},
	volume = {7},
	issn = {19391412},
	doi = {10.1109/TOH.2014.2326871},
	abstract = {—This paper is the second in a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. It investigates the tasks performed during the daily work of two housekeepers and two machinists and correlates grasp type and object properties with the attributes of the tasks being performed. The task or activity is classified according to the force required, the degrees of freedom, and the functional task type. We found that 46 percent of tasks are constrained, where the manipulated object is not allowed to move in a full six degrees of freedom. Analyzing the interrelationships between the grasp, object, and task data show that the best predictors of the grasp type are object size, task constraints, and object mass. Using these attributes, the grasp type can be predicted with 47 percent accuracy. Those parameters likely make useful heuristics for grasp planning systems. The results further suggest the common sub-categorization of grasps into power, intermediate, and precision categories may not be appropriate, indicating that grasps are generally more multi-functional than previously thought. We find large and heavy objects are grasped with a power grasp, but small and lightweight objects are not necessarily grasped with precision grasps—even with grasped object size less than 2 cm and mass less than 20 g, precision grasps are only used 61 percent of the time. These results have important implications for robotic hand design and grasp planners, since it appears while power grasps are frequently used for heavy objects, they can still be quite practical for small, lightweight objects.},
	number = {3},
	journal = {IEEE Transactions on Haptics},
	author = {Feix, Thomas and Bullock, Ian M. and Dollar, Aaron M.},
	year = {2014},
	pmid = {25248214},
	note = {ISBN: 1939-1412},
	keywords = {Activities of daily living, Human grasping, Manipulation, Prosthetics, Robotic hands},
	pages = {311--323},
}

@article{koike_neural_2016,
	title = {Neural substrates of shared attention as social memory: {A} hyperscanning functional magnetic resonance imaging study},
	volume = {125},
	issn = {10959572},
	url = {http://dx.doi.org/10.1016/j.neuroimage.2015.09.076},
	doi = {10.1016/j.neuroimage.2015.09.076},
	abstract = {During a dyadic social interaction, two individuals can share visual attention through gaze, directed to each other (mutual gaze) or to a third person or an object (joint attention). Shared attention is fundamental to dyadic face-to-face interaction, but how attention is shared, retained, and neutrally represented in a pair-specific manner has not been well studied. Here, we conducted a two-day hyperscanning functional magnetic resonance imaging study in which pairs of participants performed a real-time mutual gaze task followed by a joint attention task on the first day, and mutual gaze tasks several days later. The joint attention task enhanced eye-blink synchronization, which is believed to be a behavioral index of shared attention. When the same participant pairs underwent mutual gaze without joint attention on the second day, enhanced eye-blink synchronization persisted, and this was positively correlated with inter-individual neural synchronization within the right inferior frontal gyrus. Neural synchronization was also positively correlated with enhanced eye-blink synchronization during the previous joint attention task session. Consistent with the Hebbian association hypothesis, the right inferior frontal gyrus had been activated both by initiating and responding to joint attention. These results indicate that shared attention is represented and retained by pair-specific neural synchronization that cannot be reduced to the individual level.},
	journal = {NeuroImage},
	author = {Koike, Takahiko and Tanabe, Hiroki C. and Okazaki, Shuntaro and Nakagawa, Eri and Sasaki, Akihiro T. and Shimada, Koji and Sugawara, Sho K. and Takahashi, Haruka K. and Yoshihara, Kazufumi and Bosch-Bayard, Jorge and Sadato, Norihiro},
	year = {2016},
	pmid = {26514295},
	note = {Publisher: The Authors
ISBN: 1095-9572 (Electronic) 1053-8119 (Linking)},
	keywords = {Eye-blink synchronization, Gaze, Human-Human Interaction (HHI), Hyperscanning, Inter-individual neural synchronization, Joint attention, Mutual gaze, Neuroscience, Shared attention},
	pages = {401--412},
}

@article{moualla_readability_2017,
	title = {Readability of the gaze and expressions of a robot museum visitor: {Impact} of the low level sensory-motor control},
	volume = {2017-Janua},
	doi = {10.1109/ROMAN.2017.8172381},
	journal = {RO-MAN 2017 - 26th IEEE International Symposium on Robot and Human Interactive Communication},
	author = {Moualla, Aliaa and Karaouzene, Ali and Boucenna, Sofiane and Vidal, Denis and Gaussier, Philippe},
	year = {2017},
	note = {ISBN: 9781538635186},
	keywords = {Artificial intelligence, Gaze, Human robot interactions, Robotics, computer vision, neural networks},
	pages = {712--719},
}

@article{Vignolo2017,
	title = {Detecting {Biological} {Motion} for {Human}–{Robot} {Interaction}: {A} {Link} between {Perception} and {Action}},
	volume = {4},
	issn = {2296-9144},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2017.00014/full},
	doi = {10.3389/frobt.2017.00014},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	number = {June},
	journal = {Frontiers in Robotics and AI},
	author = {Vignolo, Alessia and Noceti, Nicoletta and Rea, Francesco and Sciutti, Alessandra and Odone, Francesca and Sandini, Giulio},
	year = {2017},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
	keywords = {Action understanding, HRI, Human-Robot Interaction(HRI), Motion, biological motion, hri, robot, robot attention, temporal multi-resolution motion descriptor, two-thirds power law},
}

@article{lewkowicz_reading_2013,
	title = {Reading motor intention through mental imagery},
	volume = {21},
	issn = {10597123},
	doi = {10.1177/1059712313501347},
	abstract = {Motor imagery is defined as a dynamic state during which the representation of a given motor act is internally rehearsed without overt motor output. Some evidence in experimental psychology has suggested that imagery ability is crucial for the correct understanding of social intention. The present study was conducted first to confirm that the nature of the motor intention leads to early modulations of movement kinematics. Secondly, we tested whether humans use imagery to read an agent's intention when observing the very first element of a complex action sequence. Results revealed early variations in movement kinematics between three different social actions and further showed that human agents can use these early deviants to anticipate above chance level the end-result before seeing the second half of the sequence. Response times in the observation task were similar in duration to those measured in the true production task, suggesting the use of motor imagery for trial categorization. Nevertheless, in a third study, the use of an artificial (neural network) classifier demonstrated that classification within the first 500 ms is possible without cognitive imagery processing. Hence, our results suggest that low-level motor indices afford intention reading without need for motor imagery but that human agents may use imaging beyond simulation to create an embodied sense of interactivity. (PsycINFO Database Record (c) 2013 APA, all rights reserved) (journal abstract)},
	number = {5},
	journal = {Adaptive Behavior},
	author = {Lewkowicz, Daniel and Delevoye-Turrell, Yvonne and Bailly, David and Andry, Pierre and Gaussier, Philippe},
	year = {2013},
	note = {ISBN: 1059-7123},
	keywords = {Action understanding, Motion, Motor sequence, Neuroscience, biological movement, classifier, intentionality, kinematics, optimal control},
	pages = {315--327},
}

@article{andry_importance_2000,
	title = {Importance of {Dynamical} {Interactions} for {Social} {Learning}},
	journal = {Dynamical Neural Networks DYNN00},
	author = {Andry, P and {P Gaussier} and Nadel, J},
	year = {2000},
	keywords = {Human-Human Interaction (HHI), Imitation},
	pages = {60--62},
}

@article{Busch2017,
	title = {Learning {Legible} {Motion} from {Human}--{Robot} {Interactions}},
	volume = {9},
	issn = {1875-4805},
	url = {https://doi.org/10.1007/s12369-017-0400-4},
	doi = {10.1007/s12369-017-0400-4},
	abstract = {In collaborative tasks, displaying legible behavior enables other members of the team to anticipate intentions and to thus coordinate their actions accordingly. Behavior is therefore considered to be legible when an observer is able to quickly and correctly infer the intention of the agent generating the behavior. In previous work, legible robot behavior has been generated by using model-based methods to optimize task-specific models of legibility. In our work, we rather use model-free reinforcement learning with a generic, task-independent cost function. In the context of experiments involving a joint task between (thirty) human subjects and a humanoid robot, we show that: (1) legible behavior arises when rewarding the efficiency of joint task completion during human--robot interactions (2) behavior that has been optimized for one subject is also more legible for other subjects (3) the universal legibility of behavior is influenced by the choice of the policy representation.},
	number = {5},
	journal = {International Journal of Social Robotics},
	author = {Busch, Baptiste and Grizou, Jonathan and Lopes, Manuel and Stulp, Freek},
	month = nov,
	year = {2017},
	keywords = {Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Motion},
	pages = {765--779},
}

@article{boucenna_development_2011,
	title = {Development of joint attention and social referencing},
	issn = {2161-9476},
	doi = {10.1109/DEVLRN.2011.6037317},
	abstract = {In this work, we are interested in understanding how emotional interactions with a social partner can bootstrap increasingly complex behaviors such as social referencing. Our idea is that social referencing, facial expression recognition and the joint attention can emerge from a simple sensori-motor architecture. Without knowing that the other is an agent, we show our robot is able to learn some complex tasks if the human partner has a low level emotional resonance with the robot head. Hence we advocate the idea that social referencing can be bootstrapped from a simple sensori-motor system not dedicated to social interactions.},
	journal = {2011 IEEE International Conference on Development and Learning, ICDL 2011},
	author = {Boucenna, Sofiane and Gaussier, Philippe and Hafemeister, Laurence},
	year = {2011},
	note = {ISBN: 9781612849904},
	keywords = {Experiment, Human-Human Interaction (HHI)},
	pages = {1--6},
}

@article{melnyk_adaptive_2011,
	title = {Adaptive behavior of an electromechanical arm robot in a case of physical interaction with a human being},
	issn = {2159-6247},
	doi = {10.1109/AIM.2011.6027148},
	abstract = {The aim of this paper is to consider the adaptation behavior of an electromechanical arm manipulator to the physical interaction of humans. Preliminary experiments to explore the possibility of adaptive interactions between an arm robot and a human without knowledge of the forces are investigated. A simple and efficient control adaptation of the system is implemented at the level of the electrical drive.},
	number = {July},
	journal = {IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM},
	author = {Melnyk, A. A. and Henaff, P. and Razakarivony, S. and Borisenko, V. Ph and Gaussier, P.},
	year = {2011},
	note = {ISBN: 9781457708381},
	keywords = {Human-Robot Interaction(HRI)},
	pages = {689--694},
}

@article{hours_supersizing_nodate,
	title = {Supersizing {Self}-supervision : {Learning} to {Grasp}},
	author = {Hours, Robot},
	note = {arXiv: 1509.06825v1},
	keywords = {Grasp, Robotics},
}

@article{melnyk_robotics_2011,
	title = {Robotics {Research}},
	volume = {70},
	url = {http://link.springer.com/10.1007/978-3-642-19457-3},
	doi = {10.1007/978-3-642-19457-3},
	author = {Melnyk, A A and Henaff, P and Razakarivony, S and Gaussier, P},
	year = {2011},
	note = {ISBN: 978-3-642-19456-6},
	keywords = {Human-Robot Interaction(HRI)},
	pages = {1--2},
}

@article{andry_using_2012,
	title = {Using the rhythm of non-verbal human-robot interaction as a signal for learning {To} cite this version : {HAL} {Id} : hal-00669859},
	author = {Andry, Pierre and Blanchard, Arnaud and Gaussier, Philippe and Andry, Pierre and Blanchard, Arnaud and Gaussier, Philippe},
	year = {2012},
	keywords = {Cognition, Human-Human Interaction (HHI), Human-Robot Interaction(HRI)},
}

@incollection{Kelley10,
	address = {Rijeka},
	title = {Understanding {Activities} and {Intentions} for {Human}-{Robot} {Interaction}},
	url = {https://doi.org/10.5772/8127},
	booktitle = {Human-{Robot} {Interaction}},
	publisher = {IntechOpen},
	author = {Kelley, Richard and Tavakkoli, Alireza and King, Christopher and Nicolescu, Monica and Nicolescu, Mircea},
	editor = {Chugo, Daisuke},
	year = {2010},
	doi = {10.5772/8127},
	keywords = {Action understanding, Human-Robot Interaction(HRI)},
}

@article{denisa_learning_2016,
	title = {Learning {Compliant} {Movement} {Primitives} {Through} {Demonstration} and {Statistical} {Generalization}},
	volume = {21},
	issn = {10834435},
	doi = {10.1109/TMECH.2015.2510165},
	abstract = {In this paper, we address the problem of simultaneously achieving low trajectory tracking errors and compliant control without using explicit mathematical models of task dynamics. To achieve this goal, we propose a new movement representation called compliant movement primitives (CMPs), which encodes position trajectory and associated torque profiles and can be learned from a single user demonstration. With the proposed control framework, the robot can remain compliant and consequently safe for humans sharing its workspace, even if high trajectory tracking accuracy is required. We developed a statistical learning approach that can use a database of existing CMPs and compute new ones, adapted for novel task variations. The proposed approach was evaluated on a Kuka LWR-4 robot performing 1) a discrete pick-and-place task with objects of varying weight and 2) a periodic handle turning operation. The evaluation of the discrete task showed a 15-fold decrease of the tracking error while exhibiting compliant behavior compared to the standard feedback control approach. It also indicated no significant rise in the tracking error while using generalized primitives computed by the statistical learning method. With respect to unforeseen collisions, the proposed approach resulted in a 75\% drop of contact forces compared to standard feedback control. The periodic task demonstrated on-line use of the proposed approach to accomplish a task of handle turning.},
	number = {5},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Denisa, Miha and Gams, Andrej and Ude, Ales and Petric, Tadej},
	year = {2016},
	keywords = {Adaptive system, Human-Robot Interaction(HRI), Primitives, Robotics, intelligent robots, learning system, robot control},
	pages = {2581--2594},
}

@article{cao_realtime_2017,
	title = {Realtime multi-person {2D} pose estimation using part affinity fields},
	volume = {2017-Janua},
	issn = {10636919},
	doi = {10.1109/CVPR.2017.143},
	abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	author = {Cao, Zhe and Simon, Tomas and Wei, Shih En and Sheikh, Yaser},
	year = {2017},
	note = {arXiv: 1611.08050
ISBN: 9781538604571},
	keywords = {Action Recognition, Human-Human Interaction (HHI)},
	pages = {1302--1310},
}

@article{senft_supervised_2017,
	title = {Supervised autonomy for online learning in human-robot interaction},
	volume = {99},
	issn = {01678655},
	url = {https://doi.org/10.1016/j.patrec.2017.03.015},
	doi = {10.1016/j.patrec.2017.03.015},
	abstract = {When a robot is learning it needs to explore its environment and how its environment responds on its actions. When the environment is large and there are a large number of possible actions the robot can take, this exploration phase can take prohibitively long. However, exploration can often be optimised by letting a human expert guide the robot during its learning. Interactive machine learning, in which a human user interactively guides the robot as it learns, has been shown to be an effective way to teach a robot. It requires an intuitive control mechanism to allow the human expert to provide feedback on the robot's progress. This paper presents a novel method which combines Reinforcement Learning and Supervised Progressively Autonomous Robot Competencies (SPARC). By allowing the user to fully control the robot and by treating rewards as implicit, SPARC aims to learn an action policy while maintaining human supervisory oversight of the robot's behaviour. This method is evaluated and compared to Interactive Reinforcement Learning in a robot teaching task. Qualitative and quantitative results indicate that SPARC allows for safer and faster learning by the robot, whilst not placing a high workload on the human teacher.},
	journal = {Pattern Recognition Letters},
	author = {Senft, Emmanuel and Baxter, Paul and Kennedy, James and Lemaignan, Séverin and Belpaeme, Tony},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Human-Robot Interaction(HRI), Human-Robot interaction, Interactive machine learning, Markov, Progressive Autonomy, Reinforcement learning, Robotics, Supervised autonomy},
	pages = {77--86},
}

@article{luo_unsupervised_2018,
	title = {Unsupervised early prediction of human reaching for human–robot collaboration in shared workspaces},
	volume = {42},
	issn = {15737527},
	doi = {10.1007/s10514-017-9655-8},
	abstract = {This paper focuses on human–robot collaboration in industrial manipulation tasks that take place in a shared workspace. In this setting we wish to predict, as quickly as possible, the human's reaching motion so that the robot can avoid interference while performing a complimentary task. Given an observed part of a human's reaching motion, we thus wish to predict the remainder of the trajectory, and demonstrate that this is effective as a real-time input to the robot for human–robot collaboration tasks. We propose a two-layer framework of Gaussian Mixture Models and an unsupervised online learning algorithm that updates these models with newly-observed trajectories. Unlike previous work in this area which relies on supervised learning meth-ods to build models of human motion, our approach requires no offline training or manual labeling. The main advantage of this unsupervised approach is that it can build models on-the-fly and adapt to new people and new motion styles as they Electronic supplementary material The online version of this article (https://doi.org/10.1007/s10514-017-9655-8) contains supplementary material, which is available to authorized users. emerge. We test our method on motion capture data from a human-human collaboration experiment to show the early prediction performance. We also present two human–robot workspace sharing experiments of varying difficulty where the robot predicts the human's motion every 0.1 s. The exper-imental results suggest that our framework can use human motion predictions to decide on robot motions that avoid the human in real-time applications with high reliability.},
	number = {3},
	journal = {Autonomous Robots},
	author = {Luo, Ruikun and Hayne, Rafi and Berenson, Dmitry},
	year = {2018},
	note = {Publisher: Springer US
ISBN: 1051401796558},
	keywords = {GMM, Grasp, Human motion prediction, Human-Robot Interaction(HRI), Human–robot collaboration, Human–robot manipulation, Learning, Prediction},
	pages = {631--648},
}

@article{li_observer_2015,
	title = {Observer {Perception} of {Dominance} and {Mirroring} {Behavior} in {Human}-{Robot} {Relationships}},
	issn = {21672148},
	doi = {10.1145/2696454.2696459},
	abstract = {How people view relationships between humans and robots is an important consideration for the design and acceptance of social robots. Two studies investigated the effect of relational behavior in a human-robot dyad. In Study 1, participants watched videos of a human confederate discussing the Desert Survival Task with either another human confederate or a humanoid robot. Participants were less trusting of both the robot and the person in a human-robot relationship where the robot was dominant toward the person than when the person was dominant toward the robot; these differences were not found for a human pair. In Study 2, participants watched videos of a human confederate having an everyday conversation with either another human confederate or a humanoid robot. Participants who saw a confederate mirror the gestures of a robot found the robot less attractive than when the robot mirrored the confederate; the opposite effect was found for a human pair. Exploratory findings suggest that human-robot relationships are viewed differently than human dyads.},
	journal = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Li, Jamy and Ju, Wendy and Nass, Clifford},
	year = {2015},
	note = {ISBN: 9781450328838},
	keywords = {Design, Human Factors, Human-Robot Interaction(HRI), Observation, Perception},
	pages = {133--140},
}

@article{hayne_considering_2016,
	title = {Considering avoidance and consistency in motion planning for human-robot manipulation in a shared workspace},
	volume = {2016-June},
	issn = {10504729},
	doi = {10.1109/ICRA.2016.7487584},
	abstract = {— This paper presents an approach to formulat-ing the cost function for a motion planner intended for human-robot collaboration on manipulation tasks in a shared workspace. To be effective for human-robot collaboration a robot should plan its motion so that it is both safe and efficient. To achieve this, we propose two factors to consider in the cost function for the robot's motion planner: (1) Avoidance of the workspace previously-occupied by the human, so that the motion is as safe as possible, and (2) Consistency of the robot's motion, so that the motion is as predictable as possible for the human and they can perform their task without focusing undue attention on the robot. Our experiments in simulation and a human-robot workspace sharing study compare a cost function that uses only the first factor and a combined cost that uses both factors vs. a baseline method that is perfectly consistent but does not account for the human's previous motion. We find that using either cost function we outperform the baseline method in terms of task success rate without degrading the task completion time. The best task success rate is achieved with the cost function that includes both the avoidance and consistency terms.},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Hayne, Rafi and Luo, Ruikun and Berenson, Dmitry},
	year = {2016},
	note = {ISBN: 9781467380263},
	keywords = {Human-Robot Interaction(HRI), Motion, Planning},
	pages = {3948--3954},
}

@article{ajoudani_progress_2018,
	title = {Progress and prospects of the human–robot collaboration},
	volume = {42},
	issn = {15737527},
	doi = {10.1007/s10514-017-9677-2},
	abstract = {Recent technological advances in hardware design of the robotic platforms enabled the implementation of various control modalities for improved interactions with humans and unstructured environments. An important application area for the integration of robots with such advanced interaction capabilities is human–robot collaboration. This aspect represents high socio-economic impacts and maintains the sense of purpose of the involved people, as the robots do not completely replace the humans from the work process. The research community’s recent surge of interest in this area has been devoted to the implementation of various methodologies to achieve intuitive and seamless human–robot-environment interactions by incorporating the collaborative partners’ superior capabilities, e.g. human’s cognitive and robot’s physical power generation capacity. In fact, the main purpose of this paper is to review the state-of-the-art on intermediate human–robot interfaces (bi-directional), robot control modalities, system stability, benchmarking and relevant use cases, and to extend views on the required future developments in the realm of human–robot collaboration.},
	number = {5},
	journal = {Autonomous Robots},
	author = {Ajoudani, Arash and Zanchettin, Andrea Maria and Ivaldi, Serena and Albu-Schäffer, Alin and Kosuge, Kazuhiro and Khatib, Oussama},
	year = {2018},
	note = {Publisher: Springer US
ISBN: 0929-5593},
	keywords = {Human-Robot Interaction(HRI), Human-in-the-loop, Human–robot interaction, Human–robot interfaces, Physical human robot collaboration, Progress and prospects, Review Paper},
	pages = {957--975},
}

@article{erlhagen_goal-directed_2006,
	title = {Goal-directed imitation for robots: {A} bio-inspired approach to action understanding and skill learning},
	volume = {54},
	issn = {09218890},
	doi = {10.1016/j.robot.2006.01.004},
	abstract = {In this paper we present a robot control architecture for learning by imitation which takes inspiration from recent discoveries in action observation/execution experiments with humans and other primates. The architecture implements two basic processing principles: (1) imitation is primarily directed toward reproducing the outcome of an observed action sequence rather than reproducing the exact action means, and (2) the required capacity to understand the motor intention of another agent is based on motor simulation. The control architecture is validated in a robot system imitating in a goal-directed manner a grasping and placing sequence displayed by a human model. During imitation, skill transfer occurs by learning and representing appropriate goal-directed sequences of motor primitives. The robustness of the goal-directed organization of the controller is tested in the presence of incomplete visual information and changes in environmental constraints. © 2006 Elsevier Ltd. All rights reserved.},
	number = {5},
	journal = {Robotics and Autonomous Systems},
	author = {Erlhagen, W. and Mukovskiy, A. and Bicho, E. and Panin, G. and Kiss, C. and Knoll, A. and van Schie, H. and Bekkering, H.},
	year = {2006},
	note = {ISBN: 09218890},
	keywords = {Action sequence, Action understanding, Dynamic field, Human-Robot Interaction(HRI), Imitation, Imitation learning, Mirror neurons},
	pages = {353--360},
}

@article{bodden_evaluating_2016,
	title = {Evaluating intent-expressive robot arm motion},
	issn = {10709878},
	doi = {10.1109/ROMAN.2016.7745188},
	abstract = {Planning effective arm motions is integral to manipulation tasks. In general, motion synthesis methods have focused on functional objectives, such as minimizing time and maximizing efficiency. However, recent work in human-robot collaboration suggests that choices in motion design can influence collaboration performance and quality. Some motion designs are easier than others for human observers to interpret. In this paper, we explore the tradeoffs in robot arm movements designed to be observed by people. Through a series of human-subjects experiments, we compare collaboration performance between several motion-synthesis methods explored by prior work. We find that a number of factors, including the design of the robot arm and metric for success, affect the relative merits of different approaches.},
	journal = {25th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2016},
	author = {Bodden, Christopher and Rakita, Daniel and Mutlu, Bilge and Gleicher, Michael},
	year = {2016},
	pmid = {24335434},
	note = {arXiv: NIHMS150003
ISBN: 9781509039296},
	pages = {658--663},
}

@article{zhang_learning_2008,
	title = {Learning human activity containing sparse irrelevant events in long sequence},
	volume = {4},
	doi = {10.1109/CISP.2008.283},
	abstract = {In daily living, person often performs quite differently for finishing the same semantic task, because of her/his mood and the scene state of that time. A large amount of variations are caused by personal petty actions, aimless wandering and additive actions in special scenes, which we term irrelevant events. In this paper, we explore how activities containing sparse irrelevant events can be recognized. We introduce an irrelevant event state into hidden semi-Markov model to cover variations because of irrelevant events. The proposed model remains the partial order of sub-events of the activity of interest, and keeps its discriminability from others with the help of higher-order Markov setting. The experimental results demonstrate the efficiency of the proposed approach for human activity recognition.},
	journal = {Proceedings - 1st International Congress on Image and Signal Processing, CISP 2008},
	author = {Zhang, Weidong and Chen, Feng and Xu, Wenli and Du, Youtian},
	year = {2008},
	note = {ISBN: 9780769531199},
	keywords = {HMM, Markov},
	pages = {211--215},
}

@article{vignolo_complexity_2016,
	title = {The complexity of biological motion: {A} temporal multi-resolution motion descriptor for human detection in videos},
	abstract = {Human neonates show a natural predisposition towards biological motion: despite the limited visual information available, they can distinguish the movement of other living agents from object motion. This ability has been suggested to be the basis for identifying conspecifics from birth, hence representing a fundamental skill for the development of social interaction. Inspired by this, we propose a computational model to detect biological motion in video sequences acquired by the iCub humanoid robot, as a first step to improve its action understanding and interaction capabilities in a developmental perspective. We propose a temporal multi-resolution motion description which automatically copes with different dynamics and builds on top of low-level features that capture biological motion regularities. The starting point of the representation is the optical flow, a low-level measurement which simulates the limited amount of visual information available at birth. The description is evaluated in its capability to discriminate between biological and non-biological movements.We show how a temporal multi-resolution descriptor can effectively deal with dynamic events of variable temporal duration and complexity. Then we provide evidence that our approach guarantees a robust classification of biological versus non-biological video sequences, which generalizes to new scenarios, including new agents and new actions, also in presence of severe occlusions.},
	author = {Vignolo, Alessia and Noceti, Nicoletta and Sciutti, Alessandra and Rea, Francesco and Odone, Francesca and Sandini, Giulio},
	year = {2016},
	keywords = {Human, Human Pose, Vision},
}

@article{vernon_prospection_2015,
	title = {Prospection in {Cognition}: {The} {Case} for {Joint} {Episodic}-{Procedural} {Memory} in {Cognitive} {Robotics}},
	volume = {2},
	issn = {2296-9144},
	url = {http://journal.frontiersin.org/Article/10.3389/frobt.2015.00019/abstract},
	doi = {10.3389/frobt.2015.00019},
	abstract = {Prospection lies at the core of cognition: it is the means by which an agent — a person or a cognitive robot — shifts its perspective from immediate sensory experience to anticipate future events, be they the actions of other agents or the outcome of its own actions. Prospection, accomplished by internal simulation, requires mechanisms for both perceptual imagery and motor imagery. While it is known that these two forms of imagery are tightly entwined in the mirror neuron system, we do not yet have an effective model of the mentalizing network which would provide a framework to integrate declarative episodic and procedural memory systems and to combine experiential knowledge with skillful know-how. Such a framework would be founded on joint perceptuo-motor representations. In this paper we examine the case for this form of representation, contrasting sensory-motor theory with ideo-motor theory, and we discuss how such a framework could be realized by joint episodic-procedural memory. We argue that such a representation framework has several advantages for cognitive robotics. Since episodic memory operates by recombining imperfectly recalled past experience, this allows it to simulate new or unexpected events. Furthermore, by virtue of its associative nature, joint episodic-procedural memory allows the internal simulation to be conditioned by current context, semantic memory, and the agent’s value system. Context and semantics constrain the combinatorial explosion of potential perception-action associations and allow effective action selection in the pursuit of goals, while the value system provides the motives that underpin the agent’s autonomy and cognitive development. This joint episodic-procedural memory framework is neutral regarding the final implementation of these episodic and procedural memories, which can be configured sub-symbolically as associative networks or symbolically as content-addressable image databases and databases of motor-control scripts.},
	number = {July},
	journal = {Frontiers in Robotics and AI},
	author = {Vernon, David and Beetz, Michael and Sandini, Giulio},
	year = {2015},
	note = {ISBN: 2296-9144},
	keywords = {Anticipation, Human-Robot Interaction(HRI), Mirror neurons, Robotics, Vision, autonomy, cognitive system, development, episodic, episodic memory, ideo-motor theory, internal simulation, procedural memory, prospection},
	pages = {1--14},
}

@article{castelhano_i_2007,
	title = {I {See} {What} {You} {See}: {Eye} {Movements} in {Real}-{World} {Scenes} {Are} {Affected} by {Perceived} {Direction} of {Gaze}},
	volume = {4840},
	issn = {0302-9743},
	url = {http://www.springerlink.com/index/10.1007/978-3-540-77343-6},
	doi = {10.1007/978-3-540-77343-6},
	abstract = {In this chapter, we report an investigation the influence of the saliency of another person’s direction of gaze on an observer’s eye movements through real-world scenes. Participants’ eye movements were recorded while they viewed a sequence of scene photographs that told a story. A subset of the scenes contained an actor. The actor’s face was highly likely to be fixated, and when it was, the observer’s next saccade was more likely to be toward the object that was the focus of the ac- tor’s gaze than in any other direction. Furthermore, when eye movement patterns did not show an immediate saccade to the focused object, ob- servers were nonetheless more likely to fixate the focused object than a control object within close temporal proximity of fixation on the face. We conclude that during real-world scene perception, observers are sen- sitive to another’s direction of gaze and use it to help guide their own eye movements.},
	journal = {Attention in Cognitive Systems. Theories and Systems from an Interdisciplinary Viewpoint},
	author = {Castelhano, Monica S and Wieth, Mareike and Henderson, John M},
	year = {2007},
	note = {ISBN: 978-3-540-77342-9},
	keywords = {Action understanding, Gaze, Human, Human-Human Interaction (HHI)},
	pages = {251--262},
}

@article{lee_predictability_2011,
	title = {Predictability or adaptivity?: designing robot handoffs modeled from trained dogs and people},
	url = {http://portal.acm.org/citation.cfm?id=1957720},
	doi = {10.1145/1957656.1957720},
	abstract = {One goal of assistive robotics is to design interactive robots that can help disabled people with tasks such as fetching objects. When people do this task, they coordinate their movements closely with receivers. We investigated how a robot should fetch and give household objects to a person. To develop a model for the robot, we first studied trained dogs and person-to-person handoffs. Our findings suggest two models of handoff that differ in their predictability and adaptivity.},
	journal = {Human-robot interaction},
	author = {Lee, Min Kyung and Forlizzi, Jodi and Kiesler, Sara and Cakmak, Maya and Srinivasa, Siddhartha},
	year = {2011},
	note = {ISBN: 9781450305617},
	keywords = {Retrieval, adaptive, coordination, hand-offs, predictability},
	pages = {179--180},
}

@article{mortl_rhythm_2014,
	title = {Rhythm patterns interaction - {Synchronization} behavior for human-robot joint action},
	volume = {9},
	issn = {19326203},
	doi = {10.1371/journal.pone.0095195},
	abstract = {Interactive behavior among humans is governed by the dynamics of movement synchronization in a variety of repetitive tasks. This requires the interaction partners to perform for example rhythmic limb swinging or even goal-directed arm movements. Inspired by that essential feature of human interaction, we present a novel concept and design methodology to synthesize goal-directed synchronization behavior for robotic agents in repetitive joint action tasks. The agents' tasks are described by closed movement trajectories and interpreted as limit cycles, for which instantaneous phase variables are derived based on oscillator theory. Events segmenting the trajectories into multiple primitives are introduced as anchoring points for enhanced synchronization modes. Utilizing both continuous phases and discrete events in a unifying view, we design a continuous dynamical process synchronizing the derived modes. Inverse to the derivation of phases, we also address the generation of goal-directed movements from the behavioral dynamics. The developed concept is implemented to an anthropomorphic robot. For evaluation of the concept an experiment is designed and conducted in which the robot performs a prototypical pick-and-place task jointly with human partners. The effectiveness of the designed behavior is successfully evidenced by objective measures of phase and event synchronization. Feedback gathered from the participants of our exploratory study suggests a subjectively pleasant sense of interaction created by the interactive behavior. The results highlight potential applications of the synchronization concept both in motor coordination among robotic agents and in enhanced social interaction between humanoid agents and humans.},
	number = {4},
	journal = {PLoS ONE},
	author = {Mörtl, Alexander and Lorenz, Tamara and Hirche, Sandra},
	year = {2014},
	pmid = {24752212},
	keywords = {Human-Robot Interaction(HRI), Joint-action},
}

@article{darling_introduction_2016,
	title = {Introduction to {Journal} of {Human}-{Robot} {Interaction} {Special} {Issue} on {Law} and {Policy}},
	volume = {5},
	issn = {2163-0364},
	url = {http://dl.acm.org/citation.cfm?id=3109964},
	doi = {10.5898/JHRI.5.3.Darling},
	abstract = {Coming generations of robots will share physical space with humans, engaging in contact interactions (physical Human Robot Interaction, or pHRI) as they carry out cooperative tasks. This special issue turns a spotlight on the specific roles that crafted haptic interaction can play in cooperation and communication between a human and a robotic partner, from the viewpoints of human needs, capabilities, and expectations and of engineering implementation.},
	number = {3},
	journal = {Journal of Human-Robot Interaction},
	author = {Darling, Kate and Calo, Ryan},
	year = {2016},
	pages = {1},
}

@article{mutlu_footing_2009,
	title = {Footing in human-robot conversations: how robots might shape participant roles using gaze cues},
	volume = {2},
	issn = {9781605584041},
	url = {http://portal.acm.org/citation.cfm?id=1514109},
	doi = {10.1145/1514095.1514109},
	abstract = {During conversations, speakers establish their and others' participant roles (who participates in the conversation and in what capacity)-or "footing" as termed by Goffman-using gaze cues. In this paper, we study how a robot can establish the participant roles of its conversational partners using these cues. We designed a set of gaze behaviors for Robovie to signal three kinds of participant roles: addressee, bystander, and overhearer. We evaluated our design in a controlled laboratory experiment with 72 subjects in 36 trials. In three conditions, the robot signaled to two subjects, only by means of gaze, the roles of (1) two addressees, (2) an addressee and a bystander, or (3) an addressee and an overhearer. Behavioral measures showed that subjects' participation behavior conformed to the roles that the robot communicated to them. In subjective evaluations, significant differences were observed in feelings of groupness between addressees and others and liking between overhearers and others. Participation in the conversation did not affect task performance-measured by recall of information presented by the robot-but affected subjects' ratings of how much they attended to the task.},
	number = {1},
	journal = {Human Factors},
	author = {Mutlu, Bilge and Shiwa, Toshiyuki and Kanda, Takayuki and Ishiguro, Hiroshi and Hagita, Norihiro},
	year = {2009},
	note = {ISBN: 9781605584041},
	keywords = {11, Gaze, Human-Robot Interaction(HRI), addition a, conversational participation, footing, gaze, non participant, participant roles, participation structure, robovie, scenario differs with},
	pages = {61--68},
}

@article{fairhurst_leading_2014,
	title = {Leading the follower: {An} {fMRI} investigation of dynamic cooperativity and leader-follower strategies in synchronization with an adaptive virtual partner},
	volume = {84},
	issn = {10959572},
	url = {http://dx.doi.org/10.1016/j.neuroimage.2013.09.027},
	doi = {10.1016/j.neuroimage.2013.09.027},
	abstract = {From everyday experience we know that it is generally easier to interact with someone who adapts to our behavior. Beyond this, achieving a common goal will very much depend on who adapts to whom and to what degree. Therefore, many joint action tasks such as musical performance prove to be more successful when defined leader-follower roles are established. In the present study, we present a novel approach to explore the mechanisms of how individuals lead and, using functional magnetic resonance imaging (fMRI), probe the neural correlates of leading. Specifically, we implemented an adaptive virtual partner (VP), an auditory pacing signal, with which individuals were instructed to tap in synchrony while maintaining a steady tempo. By varying the degree of temporal adaptation (period correction) implemented by the VP, we manipulated the objective control individuals had to exert to maintain the overall tempo of the pacing sequence (which was prone to tempo drift with high levels of period correction). Our imaging data revealed that perceiving greater influence and leading are correlated with right lateralized frontal activation of areas involved in cognitive control and self-related processing. Using participants' subjective ratings of influence and task difficulty, we classified a subgroup of our cohort as "leaders", individuals who found the task of synchronizing easier when they felt more in control. Behavioral tapping measures showed that leaders employed less error correction and focused more on self-tapping (prioritizing the instruction to maintain the given tempo) than on the stability of the interaction (prioritizing the instruction to synchronize with the VP), with correlated activity in areas involved in self-initiated action including the pre-supplementary motor area. © 2013 Elsevier Inc.},
	journal = {NeuroImage},
	author = {Fairhurst, Merle T. and Janata, Petr and Keller, Peter E.},
	year = {2014},
	pmid = {24064075},
	note = {Publisher: Elsevier Inc.
ISBN: 1053-8119},
	keywords = {Cooperation, Human-Human Interaction (HHI), Inferior frontal gyrus, Leading, Sensorimotor synchronization, Virtual partner interaction},
	pages = {688--697},
}

@article{DEHAIS2011785,
	title = {Physiological and subjective evaluation of a human-robot object hand-over task},
	volume = {42},
	issn = {18729126},
	url = {http://www.sciencedirect.com/science/article/pii/S0003687011000044},
	doi = {10.1016/j.apergo.2010.12.005},
	abstract = {In the context of task sharing between a robot companion and its human partners, the notions of safe and compliant hardware are not enough. It is necessary to guarantee ergonomic robot motions. Therefore, we have developed Human Aware Manipulation Planner (Sisbot et al., 2010), a motion planner specifically designed for human-robot object transfer by explicitly taking into account the legibility, the safety and the physical comfort of robot motions. The main objective of this research was to define precise subjective metrics to assess our planner when a human interacts with a robot in an object hand-over task. A second objective was to obtain quantitative data to evaluate the effect of this interaction. Given the short duration, the "relative ease" of the object hand-over task and its qualitative component, classical behavioral measures based on accuracy or reaction time were unsuitable to compare our gestures. In this perspective, we selected three measurements based on the galvanic skin conductance response, the deltoid muscle activity and the ocular activity. To test our assumptions and validate our planner, an experimental set-up involving Jido, a mobile manipulator robot, and a seated human was proposed. For the purpose of the experiment, we have defined three motions that combine different levels of legibility, safety and physical comfort values. After each robot gesture the participants were asked to rate them on a three dimensional subjective scale. It has appeared that the subjective data were in favor of our reference motion. Eventually the three motions elicited different physiological and ocular responses that could be used to partially discriminate them. © 2011 Elsevier Ltd and the Ergonomics Society.},
	number = {6},
	journal = {Applied Ergonomics},
	author = {Dehais, Frédéric and Sisbot, Emrah Akin and Alami, Rachid and Causse, Mickaël},
	year = {2011},
	pmid = {21296335},
	note = {Publisher: Elsevier Ltd
ISBN: 0003-6870},
	keywords = {Eye tracking, Gaze, Grasp, Handovers, Human aware planning, Human-Robot Interaction(HRI), Human-Robot interaction, Human–Robot interaction, Motion, Physiology, Robot companion, Subjective evaluation},
	pages = {785--791},
}

@article{GALLOTTI2017253,
	title = {Alignment in social interactions},
	volume = {48},
	issn = {1053-8100},
	url = {http://www.sciencedirect.com/science/article/pii/S1053810016303749},
	doi = {https://doi.org/10.1016/j.concog.2016.12.002},
	journal = {Consciousness and Cognition},
	author = {Gallotti, M and Fairhurst, M T and Frith, C D},
	year = {2017},
	keywords = {Action understanding, Human, Human-Human Interaction (HHI)},
	pages = {253--261},
}

@article{nagai_can_2007,
	title = {Can {Motionese} {Tell} {Infants} and {Robots} “{What} to {Imitate}”?},
	abstract = {An open question in imitating actions by infants and robots is how they know “what to imitate.”We suggest that parental modifications in their actions, called motionese, can help infants and robots to detect the meaningful structure of the actions. Parents tend to modify their infant-directed actions, e.g., put longer pauses be- tween actions and exaggerate actions, which are assumed to help in- fants to understand the meaning and the structure of the actions. To investigate how such modifications contribute to the infants’ under- standing of the actions, we analyzed parental actions from an infant- like viewpoint by applying a model of saliency-based visual atten- tion. Our model of an infant-like viewpoint does not suppose any a priori knowledge about actions or objects used in the actions, or any specific capability to detect a parent’s face or his/her hands. Instead, it is able to detect and gaze at salient locations, which are standing out from the surroundings because of the primitive visual features, in a scene. The model thus demonstrates what low-level aspects of parental actions are highlighted in their action sequences and could attract the attention of young infants and robots. Our quantitative analysis revealed that motionese can help them (1) to receive im- mediate social feedback on the actions, (2) to detect the initial and goal states of the actions, and (3) to look at the static features of the objects used in the actions. We discuss these results addressing the issue of “what to imitate.”},
	number = {April},
	journal = {Proceedings of the 4th International Symposium on Imitation in Animals and Artifacts},
	author = {Nagai, Yukie and Rohlfing, Katharina J},
	year = {2007},
	keywords = {Human Activity, Infants, Motion, Motionese},
	pages = {299--306},
}

@article{itti_model_1998,
	title = {A {Model} of {Saliency}-{Based} {Visual} {Attention} for {Rapic} {Scene} {Analysis}},
	volume = {20},
	issn = {01628828},
	doi = {10.1109/34.730558},
	abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Itti, Laurent and Koch, Christof and Niebur, Ernst},
	year = {1998},
	pmid = {22641701},
	note = {ISBN: 2010070550},
	keywords = {Human, Vision},
	pages = {1254--1259},
}

@article{shic_behavioral_2007,
	title = {A behavioral analysis of computational models of visual attention},
	volume = {73},
	issn = {09205691},
	doi = {10.1007/s11263-006-9784-6},
	abstract = {Robots often incorporate computational models of visual attention to streamline processing. Even though the number of visual attention systems employed on robots has increased dramatically in recent years, the evaluation of these systems has remained primarily qualitative and subjective. We introduce quantitative methods for evaluating computational models of visual attention by direct comparison with gaze trajectories acquired from humans. In particular, we focus on the need for metrics based not on distances within the image plane, but that instead operate at the level of underlying features. We present a framework, based on dimensionality-reduction over the features of human gaze trajectories, that can simultaneously be used for both optimizing a particular computational model of visual attention and for evaluating its performance in terms of similarity to human behavior. We use this framework to evaluate the Itti et al. (1998) model of visual attention, a computational model that serves as the basis for many robotic visual attention systems.},
	number = {2},
	journal = {International Journal of Computer Vision},
	author = {Shic, Frederick and Scassellati, Brian},
	year = {2007},
	note = {ISBN: 09205691},
	keywords = {Behavioral analysis, Classification strategy, Computational attention, Dimensionality reduction, Eye-tracking, Gaze metric, Human, Human validation, Robot attention, Saliency map, Vision, Visual attention model},
	pages = {159--177},
}

@article{flanagan_role_2013,
	title = {The role of observers' gaze behaviour when watching object manipulation tasks: predicting and evaluating the consequences of action},
	volume = {368},
	issn = {0962-8436},
	url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0063},
	doi = {10.1098/rstb.2013.0063},
	abstract = {When watching an actor manipulate objects, observers, like the actor, naturally direct their gaze to each object as the hand approaches and typically maintain gaze on the object until the hand departs. Here, we probed the function of observers' eye movements, focusing on two possibilities: (i) that observers' gaze behaviour arises from processes involved in the prediction of the target object of the actor's reaching movement and (ii) that this gaze behaviour supports the evaluation of mechanical events that arise from interactions between the actor's hand and objects. Observers watched an actor reach for and lift one of two presented objects. The observers' task was either to predict the target object or judge its weight. Proactive gaze behaviour, similar to that seen in self-guided action-observation, was seen in the weight judgement task, which requires evaluating mechanical events associated with lifting, but not in the target prediction task. We submit that an important function of gaze behaviour in self-guided action observation is the evaluation of mechanical events associated with interactions between the hand and object. By comparing predicted and actual mechanical events, observers, like actors, can gain knowledge about the world, including information about objects they may subsequently act upon.},
	number = {1628},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Flanagan, J. R. and Rotman, G. and Reichelt, A. F. and Johansson, R. S.},
	year = {2013},
	pmid = {24018725},
	note = {ISBN: 1471-2970 (Electronic){\textbackslash}r0962-8436 (Linking)},
	keywords = {Eyes, Gaze, Grasp, Human-Human Interaction (HHI), cognition, neuroscience},
	pages = {20130063--20130063},
}

@article{kluttz_effect_2009,
	title = {The effect of head turn on the perception of gaze},
	volume = {49},
	issn = {00426989},
	url = {http://dx.doi.org/10.1016/j.visres.2009.05.013},
	doi = {10.1016/j.visres.2009.05.013},
	abstract = {When subjects viewed straight and turned eyes that were isolated singly or in pairs from a head that was straight or turned, they underestimated their true direction of gaze. They also underestimated the direction of head turn when both eyes were closed. However, the judged direction of gaze was improved when the eyes were layered against the heads. Judged direction of averted gaze was primarily based on the abducting eye. The effect that the deviation between an eye's optical axis and its true direction of gaze (angle kappa) has on its judged direction of gaze is discussed. © 2009 Elsevier Ltd. All rights reserved.},
	number = {15},
	journal = {Vision Research},
	author = {Kluttz, Nathan L. and Mayes, Brandon R. and West, Roger W. and Kerby, Dave S.},
	year = {2009},
	pmid = {19467254},
	note = {Publisher: Elsevier Ltd
ISBN: 0042-6989},
	keywords = {Angle kappa, Gaze, Gaze direction, Gaze discrimination, Gaze shifts, Head-Orientation, Psychology, Wollaston effect},
	pages = {1979--1993},
}

@article{torres_modeling_1997,
	title = {Modeling gaze behavior as a function of discourse structure},
	abstract = {This paper addresses the problem of designing conversational agents that exhibit appropriate gaze behavior during dialogues with human users. Previous research on gaze behavior has concentrated on its relationship to turn-taking phenomena [3,4,5]. Recent work has incorporated some of these findings into the design of autonomous human-like conversational agents and interactive communicative humanoids [1,13]. However, none of this research has examined the relationship between information structure and gaze behavior. In this paper we discuss why turn-taking is not an adequate explanation for gaze behavior in conversation and why information structure should be integrated with turn-taking as an explanation for this behavior. We then examine the relationship of gaze behavior to information structure and turn-taking through an empirical analysis of discourse transcripts for several dyadic conversations. A simple algorithm for assigning gaze behavior is proposed on the basis of the findings of this empirical analysis. We describe work in progress on implementing this algorithm in an existent communicative humanoid agent [14] with the goal of producing more natural gaze behavior related to propositional content in human-computer conversations.},
	journal = {First International Workshop on …},
	author = {Torres, O and Cassell, Justine and Prevost, Scott},
	year = {1997},
	keywords = {Gaze, Human-Human Interaction (HHI), Modelling, Speech},
}

@article{pasquale_enabling_2015,
	title = {Enabling {Depth}-driven {Visual} {Attention} on the {iCub} {Humanoid} {Robot}: {Instructions} for {Use} and {New} {Perspectives}},
	issn = {2296-9144},
	url = {http://arxiv.org/abs/1509.06939},
	doi = {10.3389/frobt.2016.00035},
	abstract = {The importance of depth perception in the interactions that humans have within their nearby space is a well established fact. Consequently, it is also well known that the possibility of exploiting good stereo information would ease and, in many cases, enable, a large variety of attentional and interactive behaviors on humanoid robotic platforms. However, the difficulty of computing real-time and robust binocular disparity maps from moving stereo cameras often prevents from relying on this kind of cue to visually guide robots' attention and actions in real-world scenarios. The contribution of this paper is two-fold: first, we show that the Efficient Large-scale Stereo Matching algorithm (ELAS) by A. Geiger et al. 2010 for computation of the disparity map is well suited to be used on a humanoid robotic platform as the iCub robot; second, we show how, provided with a fast and reliable stereo system, implementing relatively challenging visual behaviors in natural settings can require much less effort. As a case of study we consider the common situation where the robot is asked to focus the attention on one object close in the scene, showing how a simple but effective disparity-based segmentation solves the problem in this case. Indeed this example paves the way to a variety of other similar applications.},
	author = {Pasquale, Giulia and Mar, Tanis and Ciliberto, Carlo and Rosasco, Lorenzo and Natale, Lorenzo},
	year = {2015},
	note = {arXiv: 1509.06939},
}

@article{vannucci_adaptation_2013,
	title = {Adaptation to a humanoid robot in a collaborative joint task},
	doi = {10.1109/ROMAN.2017.8172327},
	author = {Vannucci, Fabio and Sciutti, Alessandra and Jacono, Marco and Sandini, Giulio and Rea, Francesco},
	year = {2013},
	note = {ISBN: 9781538635179},
	keywords = {Gaze, Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Joint-action},
}

@article{nguyen_compact_2018,
	title = {Compact {Real}-time avoidance on a {Humanoid} {Robot} for {Human}-robot {Interaction}},
	issn = {21672148},
	url = {http://arxiv.org/abs/1801.05671%0Ahttp://dx.doi.org/10.1145/3171221.3171245},
	doi = {10.1145/3171221.3171245},
	abstract = {With robots leaving factories and entering less controlled domains, possibly sharing the space with humans, safety is paramount and multimodal awareness of the body surface and the surrounding environment is fundamental. Taking inspiration from peripersonal space representations in humans, we present a framework on a humanoid robot that dynamically maintains such a protective safety zone, composed of the following main components: (i) a human 2D keypoints estimation pipeline employing a deep learning based algorithm, extended here into 3D using disparity; (ii) a distributed peripersonal space representation around the robot's body parts; (iii) a reaching controller that incorporates all obstacles entering the robot's safety zone on the fly into the task. Pilot experiments demonstrate that an effective safety margin between the robot's and the human's body parts is kept. The proposed solution is flexible and versatile since the safety zone around individual robot and human body parts can be selectively modulated---here we demonstrate stronger avoidance of the human head compared to rest of the body. Our system works in real time and is self-contained, with no external sensory equipment and use of onboard cameras only.},
	author = {Nguyen, Dong Hai Phuong and Hoffmann, Matej and Roncone, Alessandro and Pattacini, Ugo and Metta, Giorgio},
	year = {2018},
	note = {arXiv: 1801.05671
ISBN: 9781450349536},
	keywords = {deep learn-, human keypoints estimation, humanoid robots, ing for robotics, margin of safety, ness, peripersonal space, physical human-robot interaction, whole-body aware-},
}

@article{perez-darpino_fast_2016,
	title = {Fast motion prediction for collaborative robotics},
	volume = {2016-Janua},
	issn = {10450823},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Pérez-D'Arpino, Claudia and Shah, Julie A.},
	year = {2016},
	keywords = {GMM, Human Demonstration, Human-Human Interaction (HHI), Human-Robot Interaction(HRI), Motion, Prediction},
	pages = {3988--3989},
}

@article{medathati_bio-inspired_2015,
	title = {Bio-inspired computer vision: {Towards} a synergistic approach of artificial and biological vision},
	volume = {150},
	issn = {1090235X},
	doi = {10.1016/j.cviu.2016.04.009},
	abstract = {Studies in biological vision have always been a great source of inspiration for design of computer vision algorithms. In the past, several successful methods were designed with varying degrees of correspondence with biological vision studies, ranging from purely functional inspiration to methods that utilise models that were primarily developed for explaining biological observations. Even though it seems well recognised that computational models of biological vision can help in design of computer vision algorithms, it is a non-trivial exercise for a computer vision researcher to mine relevant information from biological vision literature as very few studies in biology are organised at a task level. In this paper we aim to bridge this gap by providing a computer vision task centric presentation of models primarily originating in biological vision studies. Not only do we revisit some of the main features of biological vision and discuss the foundations of existing computational studies modelling biological vision, but also we consider three classical computer vision tasks from a biological perspective: image sensing, segmentation and optical flow. Using this task-centric approach, we discuss well-known biological functional principles and compare them with approaches taken by computer vision. Based on this comparative analysis of computer and biological vision, we present some recent models in biological vision and highlight a few models that we think are promising for future investigations in computer vision. To this extent, this paper provides new insights and a starting point for investigators interested in the design of biology-based computer vision algorithms and pave a way for much needed interaction between the two communities leading to the development of synergistic models of artificial and biological vision.},
	journal = {Computer Vision and Image Understanding},
	author = {Medathati, N. V.Kartheek and Neumann, Heiko and Masson, Guillaume S. and Kornprobst, Pierre},
	year = {2015},
	note = {Publisher: Elsevier Inc.
ISBN: 1077-3142},
	keywords = {Canonical computations, Dynamic sensors, Event based processing, Feedback, Form-motion interactions, Lateral interactions, Multiplexed representation, Population coding, Soft selectivity, Vision},
	pages = {1--30},
}

@article{khoramshahi_role_2016,
	title = {Role of gaze cues in interpersonal motor coordination: {Towards} higher affiliation in human-robot interaction},
	volume = {11},
	issn = {19326203},
	doi = {10.1371/journal.pone.0156874},
	abstract = {BACKGROUND: The ability to follow one another's gaze plays an important role in our social cognition; especially when we synchronously perform tasks together. We investigate how gaze cues can improve performance in a simple coordination task (i.e., the mirror game), whereby two players mirror each other's hand motions. In this game, each player is either a leader or follower. To study the effect of gaze in a systematic manner, the leader's role is played by a robotic avatar. We contrast two conditions, in which the avatar provides or not explicit gaze cues that indicate the next location of its hand. Specifically, we investigated (a) whether participants are able to exploit these gaze cues to improve their coordination, (b) how gaze cues affect action prediction and temporal coordination, and (c) whether introducing active gaze behavior for avatars makes them more realistic and human-like (from the user point of view). METHODOLOGY/PRINCIPAL FINDINGS: 43 subjects participated in 8 trials of the mirror game. Each subject performed the game in the two conditions (with and without gaze cues). In this within-subject study, the order of the conditions was randomized across participants, and subjective assessment of the avatar's realism was assessed by administering a post-hoc questionnaire. When gaze cues were provided, a quantitative assessment of synchrony between participants and the avatar revealed a significant improvement in subject reaction-time (RT). This confirms our hypothesis that gaze cues improve the follower's ability to predict the avatar's action. An analysis of the pattern of frequency across the two players' hand movements reveals that the gaze cues improve the overall temporal coordination across the two players. Finally, analysis of the subjective evaluations from the questionnaires reveals that, in the presence of gaze cues, participants found it not only more human-like/realistic, but also easier to interact with the avatar. CONCLUSION/SIGNIFICANCE: This work confirms that people can exploit gaze cues to predict another person's movements and to better coordinate their motions with their partners, even when the partner is a computer-animated avatar. Moreover, this study contributes further evidence that implementing biological features, here task-relevant gaze cues, enable the humanoid robotic avatar to appear more human-like, and thus increase the user's sense of affiliation.},
	number = {6},
	journal = {PLoS ONE},
	author = {Khoramshahi, Mahdi and Shukla, Ashwini and Raffard, Stï¿½phane and Bardy, Benoï¿½t G. and Billard, Aude},
	year = {2016},
	pmid = {27281341},
	keywords = {Action understanding, Gaze, Human-Robot Interaction(HRI), Motion, Prediction},
	pages = {1--21},
}

@article{luo_framework_2015,
	title = {A framework for unsupervised online human reaching motion recognition and early prediction},
	volume = {2015-Decem},
	issn = {21530866},
	doi = {10.1109/IROS.2015.7353706},
	abstract = {This paper focuses on recognition and prediction of human reaching motion in industrial manipulation tasks. Several supervised learning methods have been proposed for this purpose, but we seek a method that can build models on-the-fly and adapt to new people and new motion styles as they emerge. Thus, unlike previous work, we propose an unsupervised online learning approach to the problem, which requires no offline training or manual categorization of trajectories. Our approach consists of a two-layer library of Gaussian Mixture Models that can be used both for recognition and prediction. We do not assume that the number of motion classes is known a priori, and thus the library grows if it cannot explain a new observed trajectory. Given an observed portion of a trajectory, the framework can predict the remainder of the trajectory by first determining what GMM it belongs to, and then using Gaussian Mixture Regression to predict the remainder of the trajectory. We tested our method on motion- capture data recorded during assembly tasks. Our results suggest that the proposed framework outperforms supervised methods in terms of both recognition and prediction. We also show the benefit of using our two-layer framework over simpler approaches.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Luo, Ruikun and Berenson, Dmitry},
	year = {2015},
	note = {ISBN: 9781479999941},
	keywords = {Action Recognition, GMM, Hidden Markov models, Human Demonstration, Libraries, Motion, Prediction, Prediction algorithms, Robots, Supervised learning, Training, Trajectory},
	pages = {2426--2433},
}

@article{sacheli_prejudiced_2014,
	title = {Prejudiced interactions: {Implicit} racial bias reduces predictive simulation during joint action with an out-group avatar},
	volume = {5},
	issn = {20452322},
	doi = {10.1038/srep08507},
	abstract = {During social interactions people automatically apply stereotypes in order to rapidly categorize others. Racial differences are among the most powerful cues that drive these categorizations and modulate our emotional and cognitive reactivity to others. We investigated whether implicit racial bias may also shape hand kinematics during the execution of realistic joint actions with virtual in- and out-group partners. Caucasian participants were required to perform synchronous imitative or complementary reach-to-grasp movements with avatars that had different skin color (white and black) but showed identical action kinematics. Results demonstrate that stronger visuo-motor interference (indexed here as hand kinematics differences between complementary and imitative actions) emerged: i) when participants were required to predict the partner's action goal in order to on-line adapt their own movements accordingly; ii) during interactions with the in-group partner, indicating the partner's racial membership modulates interactive behaviors. Importantly, the in-group/out-group effect positively correlated with the implicit racial bias of each participant. Thus visuo-motor interference during joint action, likely reflecting predictive embodied simulation of the partner's movements, is affected by cultural inter-individual differences.},
	journal = {Scientific Reports},
	author = {Sacheli, Lucia Maria and Christensen, Andrea and Giese, Martin A. and Taubert, Nick and Pavone, Enea Francesco and Aglioti, Salvatore Maria and Candidi, Matteo},
	year = {2014},
	pmid = {25687636},
	keywords = {Perception, Racial-Bias},
	pages = {1--8},
}

@article{dong_learning_2012,
	title = {Learning and {Recognition} of {Hybrid} {Manipulation} {Motions} in {Variable} {Environments} {Using} {Probabilistic} {Flow} {Tubes}},
	volume = {4},
	issn = {18754791},
	doi = {10.1007/s12369-012-0155-x},
	abstract = {For robots to work effectively with humans, they must learn and recognize activities that humans perform. We enable a robot to learn a library of activities from user demonstrations and use it to recognize an action performed by an operator in real time. Our contributions are threefold: (1) a novel probabilistic flow tube representation that can intuitively capture a wide range of motions and can be used to support compliant execution; (2) a method to identify the relevant features of a motion, and ensure that the learned representation preserves these features in new and unfore- seen situations; (3) a fast incremental algorithm for rec- ognizing user-performed motions using this representation. Our approach provides several capabilities beyond those of existing algorithms. First, we leverage temporal information to model motions that may exhibit non-Markovian charac- teristics. Second, our approach can identify parameters of a motion not explicitly specified by the user. Third, we model hybrid continuous and discrete motions in a unified repre- sentation that avoids abstracting out the continuous details of the data. Experimental results show a 49 \% improve- ment over prior art in recognition rate for varying environ- ments, and a 24 \% improvement for a static environment, while maintaining average computing times for incremental recognition of less than half of human reaction time.We also demonstratemotion learning and recognition capabilities on real-world robot platforms.},
	number = {4},
	journal = {International Journal of Social Robotics},
	author = {Dong, Shuonan and Williams, Brian},
	year = {2012},
	note = {ISBN: 1875-4791},
	keywords = {Flow tubes, Human Demonstration, Learning from demonstration, Motion, Motion learning, Prediction, Real-time recognition},
	pages = {357--368},
}

@article{sartori_does_2009,
	title = {Does the intention to communicate affect action kinematics?},
	volume = {18},
	issn = {10538100},
	url = {http://dx.doi.org/10.1016/j.concog.2009.06.004},
	doi = {10.1016/j.concog.2009.06.004},
	abstract = {The aim of the present study was to investigate the effects of communicative intention on action. In Experiment 1 participants were requested to reach towards an object, grasp it, and either simply lift it (individual condition) or lift it with the intent to communicate a meaning to a partner (communicative condition). Movement kinematics were recorded using a three-dimensional motion analysis system. The results indicate that kinematics was sensitive to communicative intention. Although the to-be-grasped object remained the same, movements performed for the 'communicative' condition were characterized by a kinematic pattern which differed from those obtained for the 'individual' condition. These findings were confirmed in a subsequent experiment in which the communicative condition was compared to a control condition, in which the communicative exchange was prevented. Results are discussed in terms of cognitive pragmatics and current knowledge on how social behavior shapes action kinematics. ?? 2009 Elsevier Inc. All rights reserved.},
	number = {3},
	journal = {Consciousness and Cognition},
	author = {Sartori, Luisa and Becchio, Cristina and Bara, Bruno G. and Castiello, Umberto},
	year = {2009},
	pmid = {19632134},
	note = {Publisher: Elsevier Inc.
ISBN: 1053-8100},
	keywords = {Communication, Eyes, Gaze, Grasp, Human Demonstration, Human-Human Interaction (HHI), Kinematics, Motion, Reach-to-grasp, Social cognition, Social intention},
	pages = {766--772},
}

@article{pezzulo_what_2011,
	title = {What should {I} do next? {Using} shared representations to solve interaction problems},
	volume = {211},
	issn = {00144819},
	doi = {10.1007/s00221-011-2712-1},
	abstract = {Studies on how "the social mind" works reveal that cognitive agents engaged in joint actions actively estimate and influence another's cognitive variables and form shared representations with them. (How) do shared representations enhance coordination? In this paper, we provide a probabilistic model of joint action that emphasizes how shared representations help solving interaction problems. We focus on two aspects of the model. First, we discuss how shared representations permit to coordinate at the level of cognitive variables (beliefs, intentions, and actions) and determine a coherent unfolding of action execution and predictive processes in the brains of two agents. Second, we discuss the importance of signaling actions as part of a strategy for sharing representations and the active guidance of another's actions toward the achievement of a joint goal. Furthermore, we present data from a human-computer experiment (the Tower Game) in which two agents (human and computer) have to build together a tower made of colored blocks, but only the human knows the constellation of the tower to be built (e.g., red-blue-red-blue-…). We report evidence that humans use signaling strategies that take another's uncertainty into consideration, and that in turn our model is able to use humans' actions as cues to "align" its representations and to select complementary actions.},
	number = {3-4},
	journal = {Experimental Brain Research},
	author = {Pezzulo, Giovanni and Dindo, Haris},
	year = {2011},
	pmid = {21559745},
	note = {ISBN: 00144819},
	keywords = {Action understanding, Anticipation, Bayesian model, Grasp, Human, Human-Computer Interaction, Joint action, Motion, Motor simulation, Perception, Shared representations, Signaling},
	pages = {613--630},
}

@article{sacheli_and_2012,
	title = {And {Yet} {They} {Act} {Together}: {Interpersonal} {Perception} {Modulates} {Visuo}-{Motor} {Interference} and {Mutual} {Adjustments} during a {Joint}-{Grasping} {Task}},
	volume = {7},
	issn = {19326203},
	doi = {10.1371/journal.pone.0050223},
	abstract = {Prediction of "when" a partner will act and "what" he is going to do is crucial in joint-action contexts. However, studies on face-to-face interactions in which two people have to mutually adjust their movements in time and space are lacking. Moreover, while studies on passive observation have shown that somato-motor simulative processes are disrupted when the observed actor is perceived as an out-group or unfair individual, the impact of interpersonal perception on joint-actions has never been directly addressed. Here we explored this issue by comparing the ability of pairs of participants who did or did not undergo an interpersonal perception manipulation procedure to synchronise their reach-to-grasp movements during: i) a guided interaction, requiring pure temporal reciprocal coordination, and ii) a free interaction, requiring both time and space adjustments. Behavioural results demonstrate that while in neutral situations free and guided interactions are equally challenging for participants, a negative interpersonal relationship improves performance in guided interactions at the expense of the free interactive ones. This was paralleled at the kinematic level by the absence of movement corrections and by low movement variability in these participants, indicating that partners cooperating within a negative interpersonal bond executed the cooperative task on their own, without reciprocally adapting to the partner's motor behaviour. Crucially, participants' performance in the free interaction improved in the manipulated group during the second experimental session while partners became interdependent as suggested by higher movement variability and by the appearance of interference between the self-executed actions and those observed in the partner. Our study expands current knowledge about on-line motor interactions by showing that visuo-motor interference effects, mutual motor adjustments and motor-learning mechanisms are influenced by social perception.},
	number = {11},
	journal = {PLoS ONE},
	author = {Sacheli, Lucia Maria and Candidi, Matteo and Pavone, Enea Francesco and Tidoni, Emmanuele and Aglioti, Salvatore Maria},
	year = {2012},
	pmid = {23209680},
	keywords = {Grasp, Human-Human Interaction (HHI), Joint-action, Motion, Psychology},
}

@article{scorolli_i_2014,
	title = {I give you a cup, {I} get a cup: {A} kinematic study on social intention},
	volume = {57},
	issn = {18733514},
	url = {http://dx.doi.org/10.1016/j.neuropsychologia.2014.03.006},
	doi = {10.1016/j.neuropsychologia.2014.03.006},
	abstract = {While affordances have been intensively studied, the mechanisms according to how their activation is modulated by context are poorly understood. We investigated how the Agent[U+05F3]s reach-to-grasp movement towards a target-object (e.g. a can) is influenced by the other[U+05F3]s interaction with a second object (manipulative/functional) and by his/her eye-gaze communication. To manipulate physical context we showed participants two objects that could be linked by a spatial relation (e.g. can-knife, typically found in the same context), or by different functional relations. The functional relations could imply an action to perform with another person (functional-cooperative: e.g. can-glass), or on our own (functional-individual: e.g. can-straw). When objects were not related (e.g. can-toothbrush) participants had to refrain from responding. In order to respond, in the giving condition participants had to move the target object towards the other person, in the getting condition towards their own body.When participants (Agents) performed a reach-to-grasp movement to give the target object, in presence of eye-gaze communication they reached the wrist[U+05F3]s acceleration peak faster if the Other previously interacted with the second object in accordance with its conventional use. Consistently participants reached faster the MFA when the objects were related by a functional-individual than a functional-cooperative relation. The Agent[U+05F3]s getting response strongly affected the grasping component of the movement: in case of eye-gaze sharing, MFA was greater when the other previously performed a manipulative than a functional grip. Results reveal that humans have developed a sophisticated capability in detecting information from hand posture and eye-gaze, which are informative as to the Agent[U+05F3]s intention. © 2014 Elsevier Ltd.},
	number = {1},
	journal = {Neuropsychologia},
	author = {Scorolli, Claudia and Miatton, Massimiliano and Wheaton, Lewis A. and Borghi, Anna M.},
	year = {2014},
	pmid = {24680723},
	note = {Publisher: Elsevier},
	keywords = {Affordances, Eye-gaze communication, Eyes, Functional grip, Gaze, Grasp, Human-Human Interaction (HHI), Manipulative grip, Motion, Perception, Physical context, Social context, Social requests},
	pages = {196--204},
}

@article{butepage_deep_2017,
	title = {Deep representation learning for human motion prediction and classification},
	issn = {1063-6919},
	url = {http://arxiv.org/abs/1702.07486},
	doi = {10.1109/CVPR.2017.173},
	abstract = {Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.},
	journal = {arXiv},
	author = {Bütepage, Judith and Black, Michael and Kragic, Danica and Kjellström, Hedvig},
	year = {2017},
	note = {arXiv: 1702.07486
ISBN: 9781538604571},
	keywords = {Action Recognition, Human Demonstration, Human Pose, Motion, Prediction},
}

@misc{sacheli_kinematics_2013,
	title = {Kinematics fingerprints of leader and follower role-taking during cooperative joint actions},
	volume = {226},
	isbn = {1432-1106 (Electronic){\textbackslash}r0014-4819 (Linking)},
	abstract = {Performing online complementary motor adjustments is quintessential to joint actions since it allows interacting people to coordinate efficiently and achieve a common goal. We sought to determine whether, during dyadic interactions, signaling strategies and simulative processes are differentially implemented on the basis of the interactional role played by each partner. To this aim, we recorded the kinematics of the right hand of pairs of individuals who were asked to grasp as synchronously as possible a bottle-shaped object according to an imitative or complementary action schedule. Task requirements implied an asymmetric role assignment so that participants performed the task acting either as (1) Leader (i.e., receiving auditory information regarding the goal of the task with indications about where to grasp the object) or (2) Follower (i.e., receiving instructions to coordinate their movements with their partner's by performing imitative or complementary actions). Results showed that, when acting as Leader, participants used signaling strategies to enhance the predictability of their movements. In particular, they selectively emphasized kinematic parameters and reduced movement variability to provide the partner with implicit cues regarding the action to be jointly performed. Thus, Leaders make their movements more "communicative" even when not explicitly instructed to do so. Moreover, only when acting in the role of Follower did participants tend to imitate the Leader, even in complementary actions where imitation is detrimental to joint performance. Our results show that mimicking and signaling are implemented in joint actions according to the interactional role of the agent, which in turn is reflected in the kinematics of each partner.},
	journal = {Experimental Brain Research},
	author = {Sacheli, Lucia Maria and Tidoni, Emmanuele and Pavone, Enea Francesco and Aglioti, Salvatore Maria and Candidi, Matteo},
	year = {2013},
	pmid = {23503771},
	doi = {10.1007/s00221-013-3459-7},
	note = {Issue: 4
ISSN: 00144819},
	keywords = {Grasp, Grasping kinematics, Human Demonstration, Human-Human Interaction (HHI), Joint action, Motion, Motor signaling, Predictive simulation, Visuo-motor interference},
	pages = {473--486},
}

@article{langton_influence_2004,
	title = {The influence of head contour and nose angle on the perception of eye-gaze direction},
	volume = {66},
	issn = {00315117},
	doi = {10.3758/BF03194970},
	abstract = {We report seven experiments that investigate the influence that head orientation exerts on the perception of eye-gaze direction. In each of these experiments, participants were asked to decide whether the eyes in a brief and masked presentation were looking directly at them or were averted. In each case, the eyes could be presented alone, or in the context of congruent or incongruent stimuli. In Experiment 1A, the congruent and incongruent stimuli were provided by the orientation of face features and head outline. Discrimination of gaze direction was found to be better when face and gaze were congruent than in both of the other conditions, an effect that was not eliminated by inversion of the stimuli (Experiment 1B). In Experiment 2A, the internal face features were removed, but the outline of the head profile was found to produce an identical pattern of effects on gaze discrimination, effects that were again insensitive to inversion (Experiment 2B) and which persisted when lateral displacement of the eyes was controlled (Experiment 2C). Finally, in Experiment 3A, nose angle was also found to influence participants’ ability to discriminate direct gaze from averted gaze, but here the effectwas eliminated by inversion of the stimuli (Experiment 3B). We concluded that an image-based mechanism is responsible for the influence of head profile on gaze perception, whereas the analysis of nose angle involves the configural processing of face features.},
	number = {5},
	journal = {Perception and Psychophysics},
	author = {Langton, Stephen R.H. and Honeyman, Helen and Tessler, Emma},
	year = {2004},
	pmid = {15495901},
	note = {ISBN: 0031-5117},
	keywords = {Eyes, Gaze, Head-Orientation, Psychology},
	pages = {752--771},
}

@article{miall_cerebellum_2001,
	title = {The cerebellum coordinates eye and hand tracking movements},
	volume = {4},
	url = {http://dx.doi.org/10.1038/88465},
	journal = {Nature Neuroscience},
	author = {Miall, R C and Reckess, G Z and Imamizu, H},
	month = jun,
	year = {2001},
	note = {Publisher: Nature Publishing Group},
	pages = {638},
}

@misc{unknown,
	title = {Anticipation in {Human}-{Robot} {Cooperation}: {A} recurrent neural network approach for multiple action sequences prediction},
	author = {Schydlo, Paul and Raković, Mirko and Santos-Victor, José},
	year = {2017},
}

@article{mainprice_human-robot_2013,
	title = {Human-robot collaborative manipulation planning using early prediction of human motion},
	issn = {2153-0858},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6696368},
	doi = {10.1109/IROS.2013.6696368},
	abstract = {In this paper we present a framework that allows a human and a robot to perform simultaneous manipulation tasks safely in close proximity. The proposed framework is based on early prediction of the human’s motion. The prediction system, which builds on previous work in the area of gesture recognition, generates a prediction of human workspace occu- pancy by computing the swept volume of learned human motion trajectories. The motion planner then plans robot trajectories that minimize a penetration cost in the human workspace occupancy while interleaving planning and execution. Multiple plans are computed in parallel, one for each robot task available at the current time, and the trajectory with the least cost is selected for execution. We test our framework in simulation using recorded human motions and a simulated PR2 robot. Our results show that our framework enables the robot to avoid the human while still accomplishing the robot’s task, even in cases where the initial prediction of the human’s motion is incorrect. We also show that taking into account the predicted human workspace occupancy in the robot’s motion planner leads to safer and more efficient interactions between the user and the robot than only considering the human’s current configuratio},
	journal = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Mainprice, Jim and Berenson, Dmitry},
	year = {2013},
	note = {ISBN: 978-1-4673-6358-7},
	keywords = {Cooperative Manipulators, GMM, Grasp, Human Centered Planning and Control, Human-Robot Interaction(HRI), Manipulation Planning and Control, Motion, Planning},
	pages = {299--306},
}

@article{chan_human-inspired_2013,
	title = {A human-inspired object handover controller},
	volume = {32},
	issn = {02783649},
	doi = {10.1177/0278364913488806},
	abstract = {In this paper, we present a novel controller for safe, efficient, and intuitive robot-to-human object handovers and a set of experiments to evaluate user responses to the robot’s handover behavior. The controller enables a robot to mimic human behavior by actively regulating the applied grip force according to the measured load force during a handover. We provide an implementation of the controller on a Willow Garage PR2 robot, demonstrating the feasibility of realizing our design on robots with basic sensor/actuator capabilities. A user study comparing four variations of our controller shows that our design yields both human-like and human-preferred object handovers.},
	number = {8},
	journal = {International Journal of Robotics Research},
	author = {Chan, Wesley P. and Parker, Chris A.C. and Van Der Loos, Hf Machiel and Croft, Elizabeth A.},
	year = {2013},
	note = {ISBN: 0278-3649},
	keywords = {object handover controller, robot-to-human handover},
	pages = {971--983},
}

@article{Zheng2015,
	title = {Impacts of {Robot} {Head} {Gaze} on {Robot}-to-{Human} {Handovers}},
	volume = {7},
	issn = {18754805},
	doi = {10.1007/s12369-015-0305-z},
	abstract = {? 2015, Springer Science+Business Media Dordrecht.In this paper, we investigate the use of a robot?s gaze to improve the timing and subjective experience of face-to-face robot-to-human handovers. Based on observations of human gaze behaviors during face-to-face human?human handovers, we implement various gaze behaviors on a PR2 humanoid robot. We conducted two consecutive robot-to-human handover studies. Results show that when the robot continually gazes at a projected handover position while handing over an object, the human receivers reach for the object significantly earlier than when the robot looks down, away from the handover location; further, when the robot continually gazes at the receiver?s face instead of the handover position, the receivers reach for the object even earlier. When the robot?instead of continually gazing at a location?transitions its gaze from the handover position to the receivers? face, or vice versa, the receivers? reach time did not improve; however, the receivers perceive these gaze transitions to better communicate handover timing than continual gazes. Finally, the receivers perceive the robot to be more likeable and anthropomorphic when it looks at their face than when it does not. Findings from our studies indicate that robot?s use of gaze can help improve both fluency and subjective experience of the robot-to-human handover interactions.},
	number = {5},
	journal = {International Journal of Social Robotics},
	author = {Zheng, Minhua and Moon, AJung J. and Croft, Elizabeth A. and Meng, Max Q.H.},
	year = {2015},
	note = {Publisher: Springer Netherlands},
	keywords = {Face gaze, Robot head gaze, Robot-to-human handover, Shared attention gaze},
	pages = {783--798},
}

@article{dermy_prediction_2017,
	title = {Prediction of {Intention} during {Interaction} with {iCub} with {Probabilistic} {Movement} {Primitives}},
	volume = {4},
	issn = {2296-9144},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2017.00045/full},
	doi = {10.3389/frobt.2017.00045},
	abstract = {Word count: 211 This paper describes our open-source software for predicting the intention of a user interacting physically with the humanoid robot iCub. Our approach is based on Probabilistic Movement Primitives (ProMPs), a versatile method for representing, generalizing, and reproducing complex motor skills. Our approach is used to endow the iCub with basic capabilities of intention recognition in human-robot collaboration scenarios. The robot learns a set of motion primitives from several demonstrations, provided by the human via physical interaction. During training, we model from human demonstrations the collaborative scenario. During the reproduction of the collaborative task, we use the acquired knowledge to recognize the intention of the human partner. Using a few early observations of the state of the robot, we can not only infer the intention of the partner, but also to complete the movement even if the user breaks the contact and the physical interaction with the robot. We evaluate our approach in simulation and on the real iCub. In simulation, the iCub is driven by the user using the Geomagic Touch haptic device. In the real robot experiment, we directly interact with the iCub by grabbing and manually guiding the robot's arm. The software implementing our approach is open-source and available on the GitHub platform. Additionally, we provide tutorials and videos. Funding statement This paper was partially funded by the European projects CoDyCo (no. 600716 ICT211.2.1) and AnDy (no. 731540 H2020-ICT-2016-1), and the French CPER project SCIARAT. ABSTRACT 2 This paper describes our open-source software for predicting the intention of a user interacting 3 physically with the humanoid robot iCub. Our approach is based on Probabilistic Movement 4 Primitives (ProMPs), a versatile method for representing, generalizing, and reproducing complex 5 motor skills. Our approach is used to endow the iCub with basic capabilities of intention recognition 6 in human-robot collaboration scenarios. The robot learns a set of motion primitives from several 7 demonstrations, provided by the human via physical interaction. During training, we model from 8 human demonstrations the collaborative scenario. During the reproduction of the collaborative 9 task, we use the acquired knowledge to recognize the intention of the human partner. Using a few 10 early observations of the state of the robot, we can not only infer the intention of the partner, but 11 also to complete the movement even if the user breaks the contact and the physical interaction 12 with the robot. We evaluate our approach in simulation and on the real iCub. In simulation, the 13 iCub is driven by the user using the Geomagic Touch haptic device. In the real robot experiment, 14 we directly interact with the iCub by grabbing and manually guiding the robot's arm. The software 15 implementing our approach is open-source and available on the GitHub platform. Additionally, we 16 provide tutorials and videos.},
	number = {October},
	journal = {Frontiers in Robotics and AI},
	author = {Dermy, Oriane and Paraschos, Alexandros and Ewerton, Marco and Peters, Jan and Charpillet, François and Ivaldi, Serena},
	year = {2017},
	keywords = {intention, interaction, prediction, probabilistic models, robot, robot, prediction, intention, interaction, probabi},
	pages = {1--27},
}

@article{cohen_influence_2017,
	title = {Influence of facial feedback during a cooperative human-robot task in schizophrenia},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-14773-3},
	doi = {10.1038/s41598-017-14773-3},
	number = {1},
	journal = {Scientific Reports},
	author = {Cohen, Laura and Khoramshahi, Mahdi and Salesse, Robin N. and Bortolon, Catherine and Słowiński, Piotr and Zhai, Chao and Tsaneva-Atanasova, Krasimira and Di Bernardo, Mario and Capdevielle, Delphine and Marin, Ludovic and Schmidt, Richard C. and Bardy, Benoit G. and Billard, Aude and Raffard, Stéphane},
	year = {2017},
	note = {ISBN: 4159801714},
	pages = {15023},
}

@article{cross_dynamics_2008,
	title = {'{Dynamics} of interpersonal coordination, '},
	number = {January},
	author = {Cross, Holy},
	year = {2008},
}

@article{claudia_fast_2017,
	title = {Fast target prediction of human reaching motion for cooperative human-robot manipulation tasks using time series classification {The} {MIT} {Faculty} has made this article openly available . {Please} share {Citation} {Prediction} of {Human} {Reaching} {Motion} for {Cooperat}},
	author = {Claudia, P and Shah, Julie A},
	year = {2017},
	note = {ISBN: 9781479969234},
	keywords = {Anticipation, Human-Robot Interaction(HRI), Planning, Prediction},
	pages = {6175--6182},
}

@article{silva_propagation_2017,
	title = {Propagation of {Uncertainty} in the {Water} {Balance} {Calculation} - a new approach based on high density regions ˜},
	author = {Silva, Maria Almeida and Alegre, Helena},
	year = {2017},
	keywords = {high density regions, uncertainty propagation, water balance, water supply systems},
	pages = {1--25},
}

@article{langton_eyes_2000,
	title = {Do the eyes have it? {Cues} to the direction of {sTowards} expressive gaze manner in embodied virtual agentsocial attention.},
	volume = {4},
	issn = {1879-307X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10652522},
	abstract = {The face communicates an impressive amount of visual information. We use it to identify its owner, how they are feeling and to help us understand what they are saying. Models of face processing have considered how we extract such meaning from the face but have ignored another important signal - eye gaze. In this article we begin by reviewing evidence from recent neurophysiological studies that suggests that the eyes constitute a special stimulus in at least two senses. First, the structure of the eyes is such that it provides us with a particularly powerful signal to the direction of another person's gaze, and second, we may have evolved neural mechanisms devoted to gaze processing. As a result, gaze direction is analysed rapidly and automatically, and is able to trigger reflexive shifts of an observer's visual attention. However, understanding where another individual is directing their attention involves more than simply analysing their gaze direction. We go on to describe research with adult participants, children and non-human primates that suggests that other cues such as head orientation and pointing gestures make significant contributions to the computation of another's direction of attention.},
	number = {2},
	journal = {Trends in cognitive sciences},
	author = {Langton, Sr and Watt, Rj and Bruce, I},
	year = {2000},
	pmid = {10652522},
	pages = {50--59},
}

@article{versa_luka_nodate,
	title = {Luka {Lukic}},
	author = {Versa, Vice},
}

@article{shukla_coupled_2012,
	title = {Coupled dynamical system based hand-arm grasp planning under real-time perturbations},
	volume = {7},
	issn = {2330765X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959289300&partnerID=40&md5=af385f3a276443230928ea1c3a37687a},
	abstract = {Robustness to perturbation has been advocated as a key element to robot control and efforts in that direction are numerous. While in essence these approaches aim at "endowing robots with a flexibility similar to that displayed by humans", few have actually looked at how humans react in the face of fast perturbations. We recorded the kinematic data from human subjects during grasping motions under very fast perturbations. Results show a strong coupling between the reach and grasp components of the task that enables rapid adaptation of the fingers in coordination with the hand posture when the target object is perturbed. We develop a robot controller based on Coupled Dynamical Systems that exploits coupling between two dynamical systems driving the hand and finger motions. This offers a compact encoding for a variety of reach and grasp motions that adapts on-the-fly to perturbations without the need for any re-planning. To validate the model we control the motion of the iCub robot when reaching for different objects.},
	journal = {International Conference on Robotics Science and Systems, RSS 2011},
	author = {Shukla, A and Billard, A},
	year = {2012},
	note = {ISBN: 23307668 (ISSN); 9780262517799 (ISBN)},
	pages = {313--320},
}

@article{welsh_does_2005,
	title = {Does {Joe} influence {Fred}'s action? {Inhibition} of return across different nervous systems},
	volume = {385},
	issn = {03043940},
	doi = {10.1016/j.neulet.2005.05.013},
	abstract = {Inhibition of return (IOR) refers to the slowing of responses to a target that appears in the same location as a previous event. Many researchers have speculated that IOR arises from inhibitory neural processes that have developed through evolution to facilitate efficient search patterns by biasing the action and/or attention of an individual towards novel locations. Throughout evolution, however, humans conducted searches as individuals as well as members of a group. In this context, we sought to determine if IOR could also be observed in the behavior of one individual after the observation of another's behavior. Consistent with our reasoning, there was no difference in the magnitude of the IOR effect found when participants followed their own response or the response of their partner. These results are discussed in the context of action-based attention and possible underlying neural mechanisms. © 2005 Elsevier Ireland Ltd. All rights reserved.},
	number = {2},
	journal = {Neuroscience Letters},
	author = {Welsh, Timothy N. and Elliott, Digby and Anson, J. Greg and Dhillon, Victoria and Weeks, Daniel J. and Lyons, James L. and Chua, Romeo},
	year = {2005},
	pmid = {15927370},
	note = {ISBN: 0304-3940},
	keywords = {Action-based attention, Inhibition of return, Movement, Social interaction},
	pages = {99--104},
}

@article{richardson_effects_2005,
	title = {Effects of {Visual} and {Verbal} {Interaction} on {Unintentional} {Interpersonal} {Coordination}.},
	volume = {31},
	issn = {1939-1277},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.31.1.62},
	doi = {10.1037/0096-1523.31.1.62},
	abstract = {Previous research has demonstrated that people's movements can become unintentionally coordinated during interpersonal interaction. The current study sought to uncover the degree to which visual and verbal (conversation) interaction constrains and organizes the rhythmic limb movements of coactors. Two experiments were conducted in which pairs of participants completed an interpersonal puzzle task while swinging handheld pendulums with instructions that minimized intentional coordination but facilitated either visual or verbal interaction. Cross-spectral analysis revealed a higher degree of coordination for conditions in which the pairs were visually coupled. In contrast, verbal interaction alone was not found to provide a sufficient medium for unintentional coordination to occur, nor did it enhance the unintentional coordination that emerged during visual interaction. The results raise questions concerning differences between visual and verbal informational linkages during interaction and how these differences may affect interpersonal movement production and its coordination.},
	number = {1},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Richardson, Michael J. and Marsh, Kerry L. and Schmidt, R. C.},
	year = {2005},
	pmid = {15709863},
	note = {ISBN: 0191-5886{\textbackslash}n1573-3653},
	pages = {62--79},
}

@article{ivry_neural_2004,
	title = {The neural representation of time},
	volume = {14},
	issn = {09594388},
	doi = {10.1016/j.conb.2004.03.013},
	abstract = {This review summarizes recent investigations of temporal processing. We focus on motor and perceptual tasks in which crucial events span hundreds of milliseconds. One key question concerns whether the representation of temporal information is dependent on a specialized system, distributed across a network of neural regions, or computed in a local task-dependent manner. Consistent with the specialized system framework, the cerebellum is associated with various tasks that require precise timing. Computational models of timing mechanisms within the cerebellar cortex are beginning to motivate physiological studies. Emphasis has also been placed on the basal ganglia as a specialized timing system, particularly for longer intervals. We outline an alternative hypothesis in which this structure is associated with decision processes.},
	number = {2},
	journal = {Current Opinion in Neurobiology},
	author = {Ivry, Richard B. and Spencer, Rebecca M C},
	year = {2004},
	pmid = {15082329},
	note = {ISBN: 0959-4388 (Print)},
	keywords = {CR, Conditioned response, Functional magnetic resonance imaging, PD, Parkinson's disease, SMA, Supplementary motor area, fMRI},
	pages = {225--232},
}

@article{clabaugh_estimating_2015,
	title = {Estimating {Visual} {Focus} of {Attention} in {Dyadic} {Human}-{Robot} {Interaction} for {Planar} {Tasks}},
	journal = {Wonder At Icsr},
	author = {Clabaugh, Caitlyn and Ram, Tejas and Matari, Maja},
	year = {2015},
	keywords = {and maja matari, attention in dyadic, caitlyn clabaugh, human-robot interaction for planar, imating visual focus of, tasks, tejas ram},
}

@article{tidoni_audio-visual_2014,
	title = {Audio-visual feedback improves the {BCI} performance in the navigational control of a humanoid robot},
	volume = {8},
	issn = {16625218},
	doi = {10.3389/fnbot.2014.00020},
	abstract = {Advancement in brain computer interfaces (BCI) technology allows people to actively interact in the world through surrogates. Controlling real humanoid robots using BCI as intuitively as we control our body represents a challenge for current research in robotics and neuroscience. In order to successfully interact with the environment the brain integrates multiple sensory cues to form a coherent representation of the world. Cognitive neuroscience studies demonstrate that multisensory integration may imply a gain with respect to a single modality and ultimately improve the overall sensorimotor performance. For example, reactivity to simultaneous visual and auditory stimuli may be higher than to the sum of the same stimuli delivered in isolation or in temporal sequence. Yet, knowledge about whether audio-visual integration may improve the control of a surrogate is meager. To explore this issue, we provided human footstep sounds as audio feedback to BCI users while controlling a humanoid robot. Participants were asked to steer their robot surrogate and perform a pick-and-place task through BCI-SSVEPs. We found that audio-visual synchrony between footsteps sound and actual humanoid's walk reduces the time required for steering the robot. Thus, auditory feedback congruent with the humanoid actions may improve motor decisions of the BCI's user and help in the feeling of control over it. Our results shed light on the possibility to increase robot's control through the combination of multisensory feedback to a BCI user.},
	number = {JUN},
	journal = {Frontiers in Neurorobotics},
	author = {Tidoni, Emmanuele and Gergondet, Pierre and Kheddar, Abderrahmane and Aglioti, Salvatore M.},
	year = {2014},
	pmid = {24987350},
	keywords = {Brain computer interface, Humanoid, Motor control, SSVEPs, Sense of agency, Teleoperation},
	pages = {1--8},
}

@article{abreu_action_2012,
	title = {Action anticipation beyond the action observation network: {A} functional magnetic resonance imaging study in expert basketball players},
	volume = {35},
	issn = {0953816X},
	doi = {10.1111/j.1460-9568.2012.08104.x},
	abstract = {The ability to predict the actions of others is quintessential for effective social interactions, particularly in competitive contexts (e.g. in sport) when knowledge about upcoming movements allows anticipating rather than reacting to opponents. Studies suggest that we predict what others are doing by using our own motor system as an internal forward model and that the fronto-parietal action observation network (AON) is fundamental for this ability. However, multiple-duty cells dealing with action perception and execution have been found in a variety of cortical regions. Here we used functional magnetic resonance imaging to explore, in expert basketball athletes and novices, whether the ability to make early predictions about the fate of sport-specific actions (i.e. free throws) is underpinned by neural regions beyond the classical AON. We found that, although involved in action prediction, the fronto-parietal AON was similarly activated in novices and experts. Importantly, athletes exhibited relatively greater activity in the extrastriate body area during the prediction task, probably due to their expert reading of the observed action kinematics. Moreover, experts exhibited higher activation in the bilateral inferior frontal gyrus and in the right anterior insular cortex when producing errors, suggesting that they might become aware of their own errors. Correct action prediction induced higher posterior insular cortex activity in experts and higher orbito-frontal activity in novices, suggesting that body awareness is important for performance monitoring in experts, whereas novices rely more on higher-order decision-making strategies. This functional reorganization highlights the tight relationship between action anticipation, error awareness and motor expertise leading to body-related processing and differences in decision-making processes.},
	number = {10},
	journal = {European Journal of Neuroscience},
	author = {Abreu, A. M. and Macaluso, E. and Azevedo, R. T. and Cesari, P. and Urgesi, C. and Aglioti, S. M.},
	year = {2012},
	pmid = {22541026},
	note = {ISBN: 1460-9568},
	keywords = {Action anticipation, Motor expertise, Motor resonance system, Sports},
	pages = {1646--1654},
}

@article{gergondet_role_2016,
	title = {The role of audio-visual feedback in a thought- based control of a humanoid robot : a {BCI} study in healthy and ... {The} role of audio - visual feedback in a thought - based control of a humanoid robot : a {BCI} study in healthy and spinal cord injured people},
	issn = {1534-4320},
	doi = {10.1109/TNSRE.2016.2597863},
	number = {August},
	author = {Gergondet, Pierre and Tidoni, Emmanuele and Gergondet, Pierre and Fusco, Gabriele and Kheddar, Abderrahmane and Member, Senior},
	year = {2016},
	pmid = {27514061},
}

@article{avenanti_compensatory_2013,
	title = {Compensatory plasticity in the action observation network: {Virtual} lesions of {STS} enhance anticipatory simulation of seen actions},
	volume = {23},
	issn = {10473211},
	doi = {10.1093/cercor/bhs040},
	abstract = {Observation of snapshots depicting ongoing motor acts increases corticospinal motor excitability. Such motor facilitation indexes the anticipatory simulation of observed (implied) actions and likely reflects computations occurring in the parietofrontal nodes of a cortical network subserving action perception (action observation network, AON). However, direct evidence for the active role of AON in simulating the future of seen actions is lacking. Using a perturb-and-measure transcranial magnetic stimulation (TMS) approach, we show that off-line TMS disruption of regions within (inferior frontal cortex, IFC) and upstream (superior temporal sulcus, STS) the parietofrontal AON transiently abolishes and enhances the motor facilitation to observed implied actions, respectively. Our findings highlight the critical role of IFC in anticipatory motor simulation. More importantly, they show that disruption of STS calls into play compensatory motor simulation activity, fundamental for counteracting the noisy visual processing induced by TMS. Thus, short-term plastic changes in the AON allow motor simulation to deal with any gap or ambiguity of ever-changing perceptual worlds. These findings support the active, compensatory, and predictive role of frontoparietal nodes of the AON in the perception and anticipatory simulation of implied actions.},
	number = {3},
	journal = {Cerebral Cortex},
	author = {Avenanti, Alessio and Annella, Laura and Candidi, Matteo and Urgesi, Cosimo and Aglioti, Salvatore M.},
	year = {2013},
	pmid = {22426335},
	note = {ISBN: 1047-3211},
	keywords = {action prediction and simulation, functional connectivity, plasticity, superior temporal sulcus, transcranial magnetic stimulation},
	pages = {570--580},
}

@article{khansari-zadeh_imitation_2010,
	title = {Imitation learning of globally stable non-linear point-to-point robot motions using nonlinear programming},
	issn = {2153-0858},
	doi = {10.1109/IROS.2010.5651259},
	abstract = {This paper presents a methodology for learning arbitrary discrete motions from a set of demonstrations. We model a motion as a nonlinear autonomous (i.e. time-invariant) dynamical system, and define the sufficient conditions to make such a system globally asymptotically stable at the target. The convergence of all trajectories is ensured starting from any point in the operational space. We propose a learning method, called Stable Estimator of Dynamical Systems (SEDS), that estimates parameters of a Gaussian Mixture Model via an optimization problem under non-linear constraints. Being time-invariant and globally stable, the system is able to handle both temporal and spatial perturbations, while performing the motion as close to the demonstrations as possible. The method is evaluated through a set of robotic experiments.},
	number = {2},
	journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
	author = {Khansari-Zadeh, S. Mohammad and Billard, Aude},
	year = {2010},
	note = {ISBN: 9781424466757},
	pages = {2676--2683},
}

@article{khansari-zadeh_learning_2011,
	title = {Learning stable nonlinear dynamical systems with {Gaussian} mixture models},
	volume = {27},
	issn = {15523098},
	doi = {10.1109/TRO.2011.2159412},
	abstract = {This paper presents a method to learn discrete robot motions froma set of demonstrations.We model a motion as a non- linear autonomous (i.e., time-invariant) dynamical system (DS) and define sufficient conditions to ensure global asymptotic stability at the target. We propose a learning method, which is called Stable Estimator of Dynamical Systems (SEDS), to learn the parameters of the DS to ensure that all motions closely follow the demonstra- tions while ultimately reaching and stopping at the target. Time- invariance and global asymptotic stability at the target ensures that the system can respond immediately and appropriately to pertur- bations that are encountered during the motion. The method is evaluated through a set of robot experiments and on a library of human handwriting motions.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Khansari-Zadeh, S. Mohammad and Billard, Aude},
	year = {2011},
	note = {ISBN: 1552-3098},
	keywords = {DS, Dynamical systems (DS), GMM, Gaussian mixture model, Modelling, imitation learning, point-to-point motions, stability analysis},
	pages = {943--957},
}

@article{hersch_dynamical_2008,
	title = {Dynamical system modulation for robot learning via kinesthetic demonstrations},
	volume = {24},
	issn = {15523098},
	doi = {10.1109/TRO.2008.2006703},
	abstract = {We present a system for robust robot skill acquisition from kinesthetic demonstrations. This system allows a robot to learn a simple goal-directed gesture and correctly reproduce it despite changes in the initial conditions and perturbations in the environment. It combines a dynamical system control approach with tools of statistical learning theory and provides a solution to the inverse kinematics problem when dealing with a redundant manipulator. The system is validated on two experiments involving a humanoid robot: putting an object into a box and reaching for and grasping an object.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Hersch, Micha and Guenter, Florent and Calinon, Sylvain and Billard, Aude},
	year = {2008},
	note = {ISBN: 1552-3098},
	keywords = {Dynamical system control, Gaussian mixture regression, Hybrid joint and end-effector control, Intelligent robots, Manipulators, Robot programming by demonstration (PbD), Simple robotic manipulation},
	pages = {1463--1467},
}

@article{palinko_if_2016,
	title = {If looks could kill: {Humanoid} robots play a gaze-based social game with humans},
	issn = {21640580},
	doi = {10.1109/HUMANOIDS.2016.7803380},
	abstract = {Gaze plays an important role in everyday communication between humans. Eyes are not only used to perceive information during interaction, but also to control it. Humanoid robots on the other hand are not yet very proficient in understanding and using gaze. In our study we enabled two humanoid robots to perceive and exert gaze actions. We then performed a pilot experiment with the two android robots playing the “Wink Murder” game with human players. We demonstrate that the designed framework allows the robots to complete the game successfully, validating the efficacy of our gaze tracking system. Moreover, human participants exhibited a rich variety of natural behaviors in the game, suggesting that it could represent a valid scenario for a more in-depth investigation of human-humanoid interaction.},
	number = {November},
	journal = {IEEE-RAS International Conference on Humanoid Robots},
	author = {Palinko, Oskar and Sciutti, Alessandra and Wakita, Yujin and Matsumoto, Yoshio and Sandini, Giulio},
	year = {2016},
	note = {ISBN: 9781509047185},
	pages = {905--910},
}

@article{broz_mutual_2012,
	title = {Mutual gaze, personality, and familiarity: {Dual} eye-tracking during conversation},
	issn = {1944-9445},
	doi = {10.1109/ROMAN.2012.6343859},
	abstract = {Mutual gaze is an important aspect of face-to-face communication that arises from the interaction of the gaze behavior of two individuals. In this dual eye-tracking study, gaze data was collected from human conversational pairs with the goal of gaining insight into what characteristics of the conversation partners influence this behavior. We investigate the link between personality, familiarity and mutual gaze. The results found indicate that mutual gaze behavior depends on the characteristics of both partners rather than on either individual considered in isolation. We discuss the implications of these findings for the design of socially appropriate gaze controllers for robots that interact with people.},
	number = {September 2012},
	journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
	author = {Broz, Frank and Lehmann, Hagen and Nehaniv, Chrystopher L. and Dautenhahn, Kerstin},
	year = {2012},
	note = {ISBN: 9781467346054},
	pages = {858--864},
}

@article{marin-jimenez_detecting_2014,
	title = {Detecting people looking at each other in videos},
	volume = {106},
	issn = {09205691},
	doi = {10.1007/s11263-013-0655-7},
	abstract = {The objective of this work is to determine if people are interacting in TV video by detecting whether they are looking at each other or not. We determine both the temporal period of the interaction and also spatially localize the relevant people. We make the following three contributions: (i) head pose estimation in unconstrained scenarios (TV video) using Gaussian Process regression; (ii) propose and evaluate several methods for assessing whether and when pairs of people are looking at each other in a video shot; and (iii) introduce new ground truth annotation for this task, extending the TV Human Interactions Dataset [22]. The peformance of the methods is evaluated on this dataset, which consists of 300 video clips extracted from TV shows. Despite the variety and difficulty of this video material, our best method obtains an average precision of 86:2\%.},
	number = {3},
	journal = {International Journal of Computer Vision},
	author = {Marin-Jimenez, M. J. and Zisserman, A. and Eichner, M. and Ferrari, V.},
	year = {2014},
	note = {ISBN: 1-901725-43-X},
	keywords = {Action recognition, Head pose estimation, Person interactions, Video search},
	pages = {282--296},
}

@article{ivaldi_robot_2014,
	title = {Robot initiative in a team learning task increases the rhythm of interaction but not the perceived engagement},
	volume = {8},
	issn = {16625218},
	doi = {10.3389/fnbot.2014.00005},
	abstract = {We hypothesize that the initiative of a robot during a collaborative task with a human can influence the pace of interaction, the human response to attention cues, and the perceived engagement. We propose an object learning experiment where the human interacts in a natural way with the humanoid iCub. Through a two-phases scenario, the human teaches the robot about the properties of some objects. We compare the effect of the initiator of the task in the teaching phase (human or robot) on the rhythm of the interaction in the verification phase. We measure the reaction time of the human gaze when responding to attention utterances of the robot. Our experiments show that when the robot is the initiator of the learning task, the pace of interaction is higher and the reaction to attention cues faster. Subjective evaluations suggest that the initiating role of the robot, however, does not affect the perceived engagement. Moreover, subjective and third-person evaluations of the interaction task suggest that the attentive mechanism we implemented in the humanoid robot iCub is able to arouse engagement and make the robot’s behavior readable. Keywords:},
	number = {FEB},
	journal = {Frontiers in Neurorobotics},
	author = {Ivaldi, Serena and Anzalone, Salvatore M. and Rousseau, Woody and Sigaud, Olivier and Chetouani, Mohamed},
	year = {2014},
	pmid = {24596554},
	note = {ISBN: 1662-5218 (Electronic) 1662-5218 (Linking)},
	keywords = {Engagement, Human-robot interaction, Joint attention, Social learning},
	pages = {1--16},
}

@article{kim_robotic_2008,
	title = {A robotic model of the development of gaze following},
	doi = {10.1109/DEVLRN.2008.4640836},
	abstract = {For humanoid robots, the skill of gaze following is a foundational component in social interaction and imitation learning.We present a robotic system capable of learning the gaze following behavior in a real-world environment. First, the system learns to detect salient objects and to distinguish a caregiverpsilas head poses in a semi-autonomous manner. Then we present multiple scenes containing different combinations of objects and head poses to the robot head. The system learns to associate the detected head pose with correct spatial location of where potentially ldquorewardingrdquo objects would be using a biologically plausible reinforcement learning mechanism.},
	journal = {2008 IEEE 7th International Conference on Development and Learning, ICDL},
	author = {Kim, Hyundo and Jasso, Hector and De??k, Gedeon and Triesch, Jochen},
	year = {2008},
	note = {ISBN: 9781424426621},
	keywords = {Actor-critic reinforcement learning, Gaze following, Habituation, Human-robot interaction, Joint attention},
	pages = {238--243},
}

@article{boucher_i_2012,
	title = {I reach faster when i see you look: {Gaze} effects in human-human and human-robot face-to-face cooperation},
	volume = {6},
	issn = {16625218},
	doi = {10.3389/fnbot.2012.00003},
	abstract = {Human-human interaction in natural environments relies on a variety of perceptual cues. Humanoid robots are becoming increasingly refined in their sensorimotor capabilities, and thus should now be able to manipulate and exploit these social cues in cooperation with their human partners. Previous studies have demonstrated that people follow human and robot gaze, and that it can help them to cope with spatially ambiguous language. Our goal is to extend these findings into the domain of action, to determine how human and robot gaze can influence the speed and accuracy of human action. We report on results from a human-human cooperation experiment demonstrating that an agent's vision of her/his partner's gaze can significantly improve that agent's performance in a cooperative task. We then implement a heuristic capability to generate such gaze cues by a humanoid robot that engages in the same cooperative interaction. The subsequent human-robot experiments demonstrate that a human agent can indeed exploit the predictive gaze of their robot partner in a cooperative task. This allows us to render the humanoid robot more human-like in its ability to communicate with humans. The long term objectives of the work are thus to identify social cooperation cues, and to validate their pertinence through implementation in a cooperative robot. The current research provides the robot with the capability to produce appropriate speech and gaze cues in the context of human-robot cooperation tasks. Gaze is manipulated in three conditions: Full gaze (coordinated eye and head), eyes hidden with sunglasses, and head fixed. We demonstrate the pertinence of these cues in terms of statistical measures of action times for humans in the context of a cooperative task, as gaze significantly facilitates cooperation as measured by human response times.},
	number = {MAY},
	journal = {Frontiers in Neurorobotics},
	author = {Boucher, Jean David and Pattacini, Ugo and Lelong, Amelie and Bailly, Gerard and Elisei, Frederic and Fagel, Sascha and Dominey, Peter Ford and Ventre-Dominey, Jocelyne},
	year = {2012},
	pmid = {22563315},
	note = {ISBN: 1662-5218},
	keywords = {Cooperation, Gaze, Human-human interaction, Human-robot interaction},
	pages = {1--11},
}

@article{doniec_active_2006,
	title = {Active {Learning} of {Joint} {Attention}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4115577},
	doi = {10.1109/ICHR.2006.321360},
	journal = {IEEE-RAS International Conference on Humanoid Robots},
	author = {Doniec, Marek and Sun, Ganghua and Scassellati, Brian},
	year = {2006},
	note = {ISBN: 1-4244-0199-2},
	pages = {34--39},
}

@article{ye_detecting_2012,
	title = {Detecting eye contact using wearable eye-tracking glasses},
	url = {http://dl.acm.org/citation.cfm?doid=2370216.2370368},
	doi = {10.1145/2370216.2370368},
	abstract = {We describe a system for detecting moments of eye contact between an adult and a child, based on a single pair of gaze-tracking glasses which are worn by the adult. Our method utilizes commercial gaze tracking technology to determine the adult's point of gaze, and combines this with computer vision analysis of video of the child's face to determine their gaze direction. Eye contact is then detected as the event of simultaneous, mutual looking at faces by the dyad. We report encouraging findings from an initial implementation and evaluation of this approach.},
	journal = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing - UbiComp '12},
	author = {Ye, Zhefan and Li, Yin and Fathi, Alireza and Han, Yi and Rozga, Agata and Abowd, Gregory D. and Rehg, James M.},
	year = {2012},
	note = {ISBN: 9781450312240},
	pages = {699},
}

@article{borji_complementary_2014,
	title = {Complementary effects of gaze direction and early saliency in guiding fixations during free viewing.},
	volume = {14},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?articleid=2213040},
	doi = {10.1167/14.13.3},
	abstract = {Gaze direction provides an important and ubiquitous communication channel in daily behavior and social interaction of humans and some animals. While several studies have addressed gaze direction in synthesized simple scenes, few have examined how it can bias observer attention and how it might interact with early saliency during free viewing of natural and realistic scenes. Experiment 1 used a controlled, staged setting in which an actor was asked to look at two different objects in turn, yielding two images that differed only by the actor's gaze direction, to causally assess the effects of actor gaze direction. Over all scenes, the median probability of following an actor's gaze direction was higher than the median probability of looking toward the single most salient location, and higher than chance. Experiment 2 confirmed these findings over a larger set of unconstrained scenes collected from the Web and containing people looking at objects and/or other people. To further compare the strength of saliency versus gaze direction cues, we computed gaze maps by drawing a cone in the direction of gaze of the actors present in the images. Gaze maps predicted observers' fixation locations significantly above chance, although below saliency. Finally, to gauge the relative importance of actor face and eye directions in guiding observer's fixations, in Experiment 3, observers were asked to guess the gaze direction from only an actor's face region (with the rest of the scene masked), in two conditions: actor eyes visible or masked. Median probability of guessing the true gaze direction within ±9° was significantly higher when eyes were visible, suggesting that the eyes contribute significantly to gaze estimation, in addition to face region. Our results highlight that gaze direction is a strong attentional cue in guiding eye movements, complementing low-level saliency cues, and derived from both face and eyes of actors in the scene. Thus gaze direction should be considered in constructing more predictive visual attention models in the future.},
	number = {13},
	journal = {Journal of vision},
	author = {Borji, Ali and Parks, Daniel and Itti, Laurent},
	year = {2014},
	pmid = {25371549},
	keywords = {Attention, Attention: physiology, Cues, Eye Movements, Eye Movements: physiology, Face, Female, Fixation, Ocular, Fixation, Ocular: physiology, Humans, Male, Pattern Recognition, Visual, Pattern Recognition, Visual: physiology, Young Adult},
	pages = {3},
}

@article{ahmed_predicting_2016,
	title = {Predicting {Visual} {Search} {Targets} via {Eye}-{Tracking} {Data}},
	author = {Ahmed, Fawad and Borji, Ali},
	year = {2016},
	pages = {3--6},
}

@article{chandra_eye_2015,
	title = {Eye tracking based human computer interaction: {Applications} and their uses},
	url = {http://ieeexplore.ieee.org/document/7456615/},
	doi = {10.1109/MAMI.2015.7456615},
	abstract = {With the evolution of Eye Tracking from a concept to reality, it is being explored scientifically these days in Human Computer Interaction in order to record the eye movements to determine the gaze direction, position of a user on the screen at a given time and the sequence of their movement. The threefold objective of this paper include introducing the reader to the key aspects and issues of eye-movement technology, practical guidance for developing an Eye tracking application, and various opportunities and underlying challenges to develop (Man and Machine Interfacing) MAMI systems using Eye tracking. We have uniquely integrated The Eye Tribe with Unity5.1.1 and through an experiment, we have also inferred that a subject with and without bifocal glasses show relatively similar fixation results if they have correct vision but the results differ with small error if the eye is corrected using lenses. Another experiment using Eye Tribe shows that gaze input requires less time as compared to the mouse input.},
	journal = {2015 International Conference on Man and Machine Interfacing (MAMI)},
	author = {Chandra, Sushil and Sharma, Greeshma and Malhotra, Saloni and Jha, Devendra and Mittal, Alok Prakash},
	year = {2015},
	note = {ISBN: 978-1-5090-0225-2},
	keywords = {a usb, and machine interface, does not require a, eye gaze input, eye tracking, hci-human computer interface, makes it even more, mami-man, portable, separate power source, that this eye tracker, the eye tribe requires, unity},
	pages = {1--5},
}

@article{borji_what_2015,
	title = {What do eyes reveal about the mind?. {Algorithmic} inference of search targets from fixations},
	volume = {149},
	issn = {18728286},
	url = {http://dx.doi.org/10.1016/j.neucom.2014.07.055},
	doi = {10.1016/j.neucom.2014.07.055},
	abstract = {We address the question of inferring the search target from fixation behavior in visual search. Such inference is possible since during search, our attention and gaze are guided toward visual features similar to those in the search target. We strive to answer two fundamental questions: what are the most powerful algorithmic principles for this task, and how does their performance depend on the amount of available eye movement data and the complexity of the target objects? In the first two experiments, we choose a random-dot search paradigm to eliminate contextual influences on search. We present an algorithm that correctly infers the target pattern up to 50 times as often as a previously employed method and promises sufficient power and robustness for interface control. Moreover, the current data suggest a principal limitation of target inference that is crucial for interface design: if the target pattern exceeds a certain spatial complexity level, only a subpattern tends to guide the observers' eye movements, which drastically impairs target inference. In the third experiment, we show that it is possible to predict search targets in natural scenes using pattern classifiers and classic computer vision features significantly above chance. The availability of compelling inferential algorithms could initiate a new generation of smart, gaze-controlled interfaces and wearable visual technologies that deduce from their users' eye movements the visual information for which they are looking. In a broader perspective, our study shows directions for efficient intent decoding from eye movements.},
	number = {PB},
	journal = {Neurocomputing},
	author = {Borji, Ali and Lennartz, Andreas and Pomplun, Marc},
	year = {2015},
	note = {Publisher: Elsevier
ISBN: 0925-2312},
	keywords = {Eye movements, Intent decoding, Mind reading, Visual attention, Visual search, Yarbus},
	pages = {788--799},
}

@article{parks_augmented_2015,
	title = {Augmented saliency model using automatic {3D} head pose detection and learned gaze following in natural scenes},
	volume = {116},
	issn = {18785646},
	url = {http://dx.doi.org/10.1016/j.visres.2014.10.027},
	doi = {10.1016/j.visres.2014.10.027},
	abstract = {Previous studies have shown that gaze direction of actors in a scene influences eye movements of passive observers during free-viewing (Castelhano, Wieth, \& Henderson, 2007; Borji, Parks, \& Itti, 2014). However, no computational model has been proposed to combine bottom-up saliency with actor's head pose and gaze direction for predicting where observers look. Here, we first learn probability maps that predict fixations leaving head regions (gaze following fixations), as well as fixations on head regions (head fixations), both dependent on the actor's head size and pose angle. We then learn a combination of gaze following, head region, and bottom-up saliency maps with a Markov chain composed of head region and non-head region states. This simple structure allows us to inspect the model and make comments about the nature of eye movements originating from heads as opposed to other regions. Here, we assume perfect knowledge of actor head pose direction (from an oracle). The combined model, which we call the Dynamic Weighting of Cues model (DWOC), explains observers' fixations significantly better than each of the constituent components. Finally, in a fully automatic combined model, we replace the oracle head pose direction data with detections from a computer vision model of head pose. Using these (imperfect) automated detections, we again find that the combined model significantly outperforms its individual components. Our work extends the engineering and scientific applications of saliency models and helps better understand mechanisms of visual attention.},
	journal = {Vision Research},
	author = {Parks, Daniel and Borji, Ali and Itti, Laurent},
	year = {2015},
	pmid = {25448115},
	note = {Publisher: Elsevier Ltd
ISBN: 0042-6989},
	keywords = {Eye movements, Fixation prediction, Gaze following, Head pose detection, Saliency modeling, Visual attention},
	pages = {113--126},
}

@article{chandra_predicting_2015,
	title = {Predicting {Visual} {Search} {Targets} via {Eye}-{Tracking} {Data}},
	volume = {116},
	issn = {1534-7362},
	url = {http://dx.doi.org/10.1016/j.visres.2014.10.027},
	doi = {10.1016/j.visres.2014.10.027},
	abstract = {Previous studies have shown that gaze direction of actors in a scene influences eye movements of passive observers during free-viewing (Castelhano, Wieth, \& Henderson, 2007; Borji, Parks, \& Itti, 2014). However, no computational model has been proposed to combine bottom-up saliency with actor's head pose and gaze direction for predicting where observers look. Here, we first learn probability maps that predict fixations leaving head regions (gaze following fixations), as well as fixations on head regions (head fixations), both dependent on the actor's head size and pose angle. We then learn a combination of gaze following, head region, and bottom-up saliency maps with a Markov chain composed of head region and non-head region states. This simple structure allows us to inspect the model and make comments about the nature of eye movements originating from heads as opposed to other regions. Here, we assume perfect knowledge of actor head pose direction (from an oracle). The combined model, which we call the Dynamic Weighting of Cues model (DWOC), explains observers' fixations significantly better than each of the constituent components. Finally, in a fully automatic combined model, we replace the oracle head pose direction data with detections from a computer vision model of head pose. Using these (imperfect) automated detections, we again find that the combined model significantly outperforms its individual components. Our work extends the engineering and scientific applications of saliency models and helps better understand mechanisms of visual attention.},
	number = {13},
	journal = {Vision Research},
	author = {Chandra, Sushil and Sharma, Greeshma and Malhotra, Saloni and Jha, Devendra and Mittal, Alok Prakash and Borji, Ali and Parks, Daniel and Itti, Laurent and Palinko, Oskar and Rea, Francesco and Sandini, Giulio and Sciutti, Alessandra and Ahmed, Fawad and Borji, Ali and Parks, Daniel and Borji, Ali and Itti, Laurent and Ahmed, Fawad and Borji, Ali and Lennartz, Andreas and Pomplun, Marc and Palinko, Oskar and Rea, Francesco and Sandini, Giulio and Sciutti, Alessandra and Borji, Ali and Parks, Daniel and Itti, Laurent and Chandra, Sushil and Sharma, Greeshma and Malhotra, Saloni and Jha, Devendra and Mittal, Alok Prakash},
	year = {2015},
	pmid = {25448115},
	note = {Publisher: Elsevier Ltd
ISBN: 978-1-5090-0225-2},
	keywords = {Attention, Attention: physiology, Cues, Eye Movements, Eye Movements: physiology, Eye movements, Face, Female, Fixation, Fixation prediction, Gaze following, Head pose detection, Humans, Intent decoding, Male, Mind reading, Ocular, Ocular: physiology, Pattern Recognition, Saliency modeling, Visual, Visual attention, Visual search, Visual: physiology, Yarbus, Young Adult, a usb, and machine interface, does not require a, eye gaze input, eye tracking, hci-human computer interface, makes it even more, mami-man, portable, separate power source, that this eye tracker, the eye tribe requires, unity},
	pages = {3--6},
}

@article{nikolaidis_viewpoint-based_2016,
	title = {Viewpoint-based legibility optimization},
	volume = {2016-April},
	issn = {21672148},
	doi = {10.1109/HRI.2016.7451762},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Nikolaidis, Stefanos and Dragan, Anca and Srinivasa, Siddhartha},
	year = {2016},
	note = {ISBN: 9781467383707},
	pages = {271--278},
}

@article{silva_combining_2016,
	title = {Combining intention and emotional state inference in a dynamic neural field architecture for human-robot joint action},
	volume = {24},
	issn = {1059-7123},
	url = {http://adb.sagepub.com/cgi/doi/10.1177/1059712316665451},
	doi = {10.1177/1059712316665451},
	abstract = {We report on our approach towards creating socially intelligent robots, which is heavily inspired by recent experimental findings about the neurocognitive mechanisms underlying action and emotion understanding in humans. Our approach uses neuro-dynamics as a theoretical language to model cognition, emotional states, decision making and action. The control architecture is formalized by a coupled system of dynamic neural fields representing a distributed network of local but connected neural populations. Different pools of neurons encode relevant information in the form of self-sustained activation patterns, which are triggered by input from connected populations and evolve continuously in time. The architecture implements a dynamic and flexible context-dependent mapping from observed hand and facial actions of the human onto adequate complementary behaviors of the robot that take into account the inferred goal and inferred emotional state of the co-actor. The dynamic control architecture was validated in multiple scenarios in which an anthropomorphic robot and a human operator assemble a toy object from its components. The scenarios focus on the robot’s capacity to understand the human’s actions, and emotional states, detect errors and adapt its behavior accordingly by adjusting its decisions and movements during the execution of the task.},
	number = {5},
	journal = {Adaptive Behavior},
	author = {Silva, R. and Louro, L. and Malheiro, T. and Erlhagen, W. and Bicho, E.},
	year = {2016},
	keywords = {decision making, dynamic neural, emotional state inference, error detection, goal inference, human-robot joint action},
	pages = {1059712316665451},
}

@article{judith_synthesis_nodate,
	title = {synthesis for human-robot collaboration},
	abstract = {— Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. A common approach to human intention inference is to model specific trajectories towards known goals with supervised classifiers. However, these approaches do not take possible future movements into account nor do they make use of kinematic cues, such as legible and predictable motion. The bottleneck of these methods is the lack of an accurate model of general human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motions. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold.},
	author = {Judith, B and Kjellstr, Hedvig and Kragic, Danica},
	note = {arXiv: 1702.08212v1},
	pages = {1--8},
}

@inproceedings{grizou_robot_2013,
	title = {Robot learning simultaneously a task and how to interpret human instructions},
	isbn = {978-1-4799-1036-6},
	doi = {10.1109/DevLrn.2013.6652523},
	abstract = {This paper presents an algorithm to bootstrap shared understanding in a human-robot interaction scenario where the user teaches a robot a new task using teaching instructions yet unknown to it. In such cases, the robot needs to estimate simultaneously what the task is and the associated meaning of instructions received from the user. For this work, we consider a scenario where a human teacher uses initially unknown spoken words, whose associated unknown meaning is either a feedback (good/bad) or a guidance (go left, right, ...). We present computational results, within an inverse reinforcement learning framework, showing that a) it is possible to learn the meaning of unknown and noisy teaching instructions, as well as a new task at the same time, b) it is possible to reuse the acquired knowledge about instructions for learning new tasks, and c) even if the robot initially knows some of the instructions' meanings, the use of extra unknown teaching instructions improves learning efficiency.},
	booktitle = {2013 {IEEE} 3rd {Joint} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics}, {ICDL} 2013 - {Electronic} {Conference} {Proceedings}},
	author = {Grizou, Jonathan and Lopes, Manuel and Oudeyer, Pierre Yves},
	year = {2013},
	keywords = {Inverse reinforcement Learning},
}

@inproceedings{strabala_learning_2012,
	title = {Learning the communication of intent prior to physical collaboration},
	isbn = {978-1-4673-4605-4},
	doi = {10.1109/ROMAN.2012.6343875},
	abstract = {When performing physical collaboration tasks, like packing a picnic basket together, humans communicate strongly and often subtly via multiple channels like gaze, speech, gestures, movement and posture. Understanding and participating in this communication enables us to predict a physical action rather than react to it, producing seamless collaboration. In this paper, we automatically learn key discriminative features that predict the intent to handover an object using machine learning techniques. We train and test our algorithm on multi-channel vision and pose data collected from an extensive user study in an instrumented kitchen. Our algorithm outputs a tree of possibilities, automatically encoding various types of pre-handover communication. A surprising outcome is that mutual gaze and inter-personal distance, often cited as being key for interaction, were not key discriminative features. Finally, we discuss the immediate and future impact of this work for human-robot interaction.},
	booktitle = {Proceedings - {IEEE} {International} {Workshop} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Strabala, Kyle and Lee, Min Kyung and Dragan, Anca and Forlizzi, Jodi and Srinivasa, Siddhartha S.},
	year = {2012},
	note = {ISSN: 1944-9445},
	keywords = {Experiment, Grasp, Human Experiment, Human-Human Interaction (HHI), Vision},
	pages = {968--973},
}

@article{bernardino_binocular_nodate,
	title = {Binocular {Visual} {Tracking}: {Integration} of {Perception} and {Control}},
	abstract = {This paper presents an active binocular tracking system including both the perceptual and control points of view. In the perceptual part, a signiicant aspect is that we make use of a space variant sensor geometry implementing a focus of attention in the center of the visual which favours the tracking process. Simple and fast low-level vision algorithms are employed, allowing real-time (16.7 Hz) and reliable performance. The control part is developed according to the visual servoing framework, including the kinematics and dynamics of our system. We show that under certain assumptions the kinematic relations in our system become very simple and decoupled as well as system dynamics can be expressed in the image feature space. These facts allow the design of simple dynamic controllers for each degree of freedom directly from the image feature space properties. The overall system is implemented in the Medusa stereo head without any speciic processing hardware. Results of tracking experiments are presented and illustrate the performance of the system with diierent control strategies and with objects of diierent shapes and motions.},
	author = {Bernardino, Alexandre and Santos-Victor, Jos E},
}

@article{griffin_models_2003,
	title = {Models of dyadic social interaction.},
	volume = {358},
	issn = {0962-8436},
	doi = {10.1098/rstb.2002.1263},
	abstract = {We discuss the logic of research designs for dyadic interaction and present statistical models with parameters that are tied to psychologically relevant constructs. Building on Karl Pearson's classic nineteenth-century statistical analysis of within-organism similarity, we describe several approaches to indexing dyadic interdependence and provide graphical methods for visualizing dyadic data. We also describe several statistical and conceptual solutions to the 'levels of analytic' problem in analysing dyadic data. These analytic strategies allow the researcher to examine and measure psychological questions of interdependence and social influence. We provide illustrative data from casually interacting and romantic dyads.},
	number = {1431},
	journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
	author = {Griffin, Dale and Gonzalez, Richard},
	year = {2003},
	pmid = {12689382},
	note = {ISBN: 09628436},
	keywords = {dyads, interdependence, research design, statistical analysis},
	pages = {573--581},
}

@article{anand_contextually_2013,
	title = {Contextually {Guided} {Semantic} {Labeling} and {Search} for {3D} {Point} {Clouds}},
	volume = {32},
	issn = {0278-3649},
	url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364912461538},
	doi = {10.1177/0278364912461538},
	abstract = {RGB-D cameras, which give an RGB image together with depths, are becoming increasingly popular for robotic perception. In this paper, we address the task of detecting commonly found objects in the three-dimensional (3D) point cloud of indoor scenes obtained from such cameras. Our method uses a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurrence relationships and geometric relationships. With a large number of object classes and relations, the model's parsimony becomes important and we address that by using multiple types of edge potentials. We train the model using a maximum-margin learning approach. In our experiments concerning a total of 52 3D scenes of homes and offices (composed from about 550 views), we get a performance of 84.06\% and 73.38\% in labeling office and home scenes respectively for 17 object classes each. We also present a method for a robot to search for an object using the learned model and the contextual information available from the current labelings of the scene. We applied this algorithm successfully on a mobile robot for the task of finding 12 object classes in 10 different offices and achieved a precision of 97.56\% with 78.43\% recall.1},
	number = {1},
	journal = {International Journal of Robotics Research},
	author = {Anand, Abhishek and Koppula, Hema Swetha and Joachims, Thorsten and Saxena, Ashutosh},
	year = {2013},
	pmid = {84945428},
	note = {arXiv: 1111.5358
ISBN: 0278-3649},
	keywords = {Labeling, RGB-D},
	pages = {19--34},
}

@article{corbetta_lateral_1999,
	title = {Lateral biases and fluctuations in infants' spontaneous arm movements and reaching},
	volume = {34},
	issn = {00121630},
	doi = {10.1002/(SICI)1098-2302(199905)34:2<237::AID-DEV1>3.0.CO;2-#},
	abstract = {The development of hand preference in infant reaching is marked by several lateral fluctuations. This study investigated whether similar lateral fluctuations were present in infants' spontaneous, nonreaching, and freely performed movements. We collected reaching and nonreaching movements kinematics in 4 infants that we followed longitudinally during their 1st year. In their 4th year, we assessed the direction of their hand preference. We found that lateral biases in spontaneous, nonreaching movements in the 1st year showed several shifts that were similar to those observed in reaching. Despite these shifts, all 4 infants traversed a short period of right-handedness. This right-handedness matched the direction of their hand preference at 3 years of age. We propose that shifts in the development of hand preference in the 1st year are linked to successive reorganizations of the motor system. These reorganizations take place as infants learn to sit, crawl, and walk.},
	number = {4},
	journal = {Developmental Psychobiology},
	author = {Corbetta, Daniela and Thelen, Esther},
	year = {1999},
	pmid = {10331149},
	note = {ISBN: 0012-1630 (Print)},
	keywords = {Development, Handedness, Infants, Laterality, Reaching, Upper arm movements},
	pages = {237--255},
}

@article{busso_iemocap:_2008,
	title = {{IEMOCAP}: {Interactive} emotional dyadic motion capture database},
	volume = {42},
	issn = {1574020X},
	doi = {10.1007/s10579-008-9076-6},
	abstract = {Since emotions are expressed through a combination of verbal and nonverbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the interactive emotional dyadic motion capture database (IEMOCAP), collected by the Speech Analysis and at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12 h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.},
	number = {4},
	journal = {Language Resources and Evaluation},
	author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
	year = {2008},
	note = {ISBN: 1057900890766},
	keywords = {Audio-visual database, Dyadic interaction, Emotion, Emotional assessment, Motion capture system},
	pages = {335--359},
}

@article{bicho_power_2012,
	title = {The power of prediction: {Robots} that read intentions},
	issn = {21530858},
	doi = {10.1109/IROS.2012.6386297},
	abstract = {Humans are experts in cooperating in a smooth and proactive manner. Action and intention understanding are critical components of efficient joint action. In the context of the EU Integrated Project JAST [16] we have developed an anthropomorphic robot endowed with these cognitive capacities. This project and respective robot (ARoS) is the focus of the video. More specifically, the results illustrate crucial cognitive capacities for efficient and successful human-robot collaboration such as goal inference, error detection and anticipatory action selection. Results were considered one of the ICT "success stories"[22].},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Bicho, E. and Erlhagen, W. and Sousa, E. and Louro, L. and Hipolito, N. and Silva, E. C. and Silva, R. and Ferreira, F. and MacHado, T. and Hulstijn, M. and Maas, Y. and De Bruijn, E. and Cuijpers, R. H. and Newman-Norlund, R. and Van Schie, H. and Meulenbroek, R. G J and Bekkering, H.},
	year = {2012},
	note = {ISBN: 9781467317375},
	pages = {5458--5459},
}

@article{gupta_observing_2009,
	title = {Observing human-object interactions: {Using} spatial and functional compatibility for recognition},
	issn = {01628828},
	doi = {10.1109/TPAMI.2009.83},
	abstract = {Interpretation of images and videos containing humans interacting with different objects is a daunting task. It involves understanding scene or event, analyzing human movements, recognizing manipulable objects, and observing the effect of the human movement on those objects. While each of these perceptual tasks can be conducted independently, recognition rate improves when interactions between them are considered. Motivated by psychological studies of human perception, we present a Bayesian approach which integrates various perceptual tasks involved in understanding human-object interactions. Previous approaches to object and action recognition rely on static shape or appearance feature matching and motion analysis, respectively. Our approach goes beyond these traditional approaches and applies spatial and functional constraints on each of the perceptual elements for coherent semantic interpretation. Such constraints allow us to recognize objects and actions when the appearances are not discriminative enough. We also demonstrate the use of such constraints in recognition of actions from static images without using any motion information},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gupta, Abhinav and Kembhavi, Aniruddha and Davis, Larry S.},
	year = {2009},
	pmid = {19696449},
	note = {ISBN: 0162-8828},
	keywords = {Action recognition, Bayesian Network, Functional recognition, Grasp, Human Grasp, Object, Object Recognition, Object recognition},
}

@article{umilta_i_2001,
	title = {I {Know} {What} {You} {Are} {DoingA} {Neurophysiological} {Study}},
	volume = {31},
	issn = {08966273},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627301003373},
	doi = {10.1016/S0896-6273(01)00337-3},
	abstract = {In the ventral premotor cortex of the macaque monkey, there are neurons that discharge both during the execution of hand actions and during the observation of the same actions made by others (mirror neurons). In the present study, we show that a subset of mirror neurons becomes active during action presentation and also when the final part of the action, crucial in triggering the response in full vision, is hidden and can therefore only be inferred. This implies that the motor representation of an action performed by others can be internally generated in the observer's premotor cortex, even when a visual description of the action is lacking. The present findings support the hypothesis that mirror neuron activation could be at the basis of action recognition.},
	number = {1},
	journal = {Neuron},
	author = {Umiltà, M.a. and Kohler, E. and Gallese, V. and Fogassi, L. and Fadiga, L. and Keysers, C. and Rizzolatti, G.},
	year = {2001},
	pmid = {11498058},
	note = {ISBN: 0896-6273 (Print)},
	keywords = {Handovers, Mirror neurons, Neurophysiological, Neuroscience, Object},
	pages = {155--165},
}

@article{koppula_learning_2013,
	title = {Learning {Spatio}-{Temporal} {Structure} from {RGB}-{D} {Videos} for {Human} {Activity} {Detection} and {Anticipation}},
	volume = {28},
	doi = {10.1.1.307.4781/???},
	abstract = {We consider the problem of detecting past activities as well as anticipating which activ- ity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional ran- dom field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, mul- tiple graph structures are possible. In this paper, we reason about these alternate pos- sibilities by reasoning over multiple possi- ble graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating fu- ture activities, on a dataset of 120 activity videos collected from four subjects.},
	journal = {International Conference on Machine Learning (ICML)},
	author = {Koppula, Hema S},
	year = {2013},
	keywords = {Activity Detection, Affordances, Anticipation, Object},
	pages = {792--800},
}

@article{ackzell_nonlinear_nodate,
	title = {Nonlinear {MPC} and grey-box identification using {PSAEM} of a quadruple-tank laboratory process},
	abstract = {This paper describes how to apply model predictive control to a quadruple-tank laboratory process, which is a nonlinear and non-minimum phase system. It shows how to use grey-box identification to identify a nonlinear process model using the recently proposed particle stochastic approximation expectation maximization al-gorithm. Non-minimum phase systems pose several challenges in the MPC frame-work which are discussed and a method that gives good results for the system in question is presented. The nonlinearity of the process is handled by linearizing around the predicted trajectory of the previous solution of the optimization prob-lem and the benefits of this approach is demonstrated by comparing it with using a fixed linearization point. The paper is concluded by demonstrating the validity of the approach presented by applying it to the real process.},
	author = {Ackzell, Erik and Duarte, Nuno and Nordh, Jerker},
	keywords = {Non-minimum phase systems, Nonlinear Model Predictive Control, Nonlinear System Identification, PGAS, PSAEM},
}

@article{tresilian_attention_1998,
	title = {Attention in action or obstruction of movement? {A} kinematic analysis of avoidance behavior in prehension},
	volume = {120},
	issn = {00144819},
	doi = {10.1007/s002210050409},
	abstract = {Obstacle avoidance strategies are of two basic but interrelated types: moving around an obstacle to that body parts do not come too close, and slowing down. In reaching-to-grasp, avoidance may involve the transport component, the grasp formation component, or both. There has been little research that has directly examined obstacle avoidance strategies during reaches-to-grasp. Several recent reports describe experiments in which reaches-to-grasp were made when nontarget objects were present in the workspace. The effects of these nontargets were interpreted as being due to their distracting effects rather than their obstructing effects. The results of these studies are reinterpreted as being due to the non-target's obstructing effects. The obstacle interpretation is more parsimonious and better predicts the pattern of results than the distractor interpretation. Predictions of the obstacle interpretation were examined in an experiment in which participants were required to reach to grasp a target in the presence of another object in various locations. The results were exactly in line with the interpretation of the object as an obstacle and the data show how grasp and transport movements are subtly adjusted so as to avoid potential obstacles. It is proposed that people move so as not to bring body parts within a minimum preferred distance from nontarget objects within the workspace. What constitutes the preferred distance in a particular context appears to depend upon the speed of movement and a variety of psychological factors related to the cost that a person attaches to a collision.},
	number = {3},
	journal = {Experimental Brain Research},
	author = {Tresilian, James R.},
	year = {1998},
	pmid = {9628422},
	note = {ISBN: 0014-4819 (Print){\textbackslash}n0014-4819 (Linking)},
	keywords = {Arm movement, Attention, Experiment, Human, Obstacle, Prehension},
	pages = {352--368},
}

@article{ferreira_duarte_optimal_nodate,
	title = {Optimal {Trajectory} {Generation} under {Environmental} {Uncertainties} using {Signal} {Temporal} {Logic} {Specifications}},
	abstract = {— Temporal Logic has successfully been used for specifying a wide range of behaviors for systems and envi-ronments. Previous works using Linear Temporal Logic (LTL) have synthesized controllers when specifying properties of discrete time signals. However, these controllers are inconve-nient to handle continuous signals with time dependency, more specifically, when optimizing trajectories under environmental uncertainties, such as static or dynamical obstacles, known and unknown obstacles. To address these issues, we use Signal Temporal Logic (STL) instead and our aim is to design a reactive strategy and trajectory planning for a surveillance mission in a partially known environment; the autonomous vehicle will satisfy tasks such as periodically monitoring areas, path planning, staying safe, and avoiding obstacles while the position of some of them is not known a priori. We also compare the performance of the obtained controller with the one obtained using the LTL language.},
	author = {Ferreira Duarte, Nuno R and Farahani, Samira S and Murray, Richard M},
}

@article{choi_robust_2011,
	title = {Robust {3D} {Visual} {Tracking} {Using} {Particle} {Filtering} on the {SE} ( 3 ) {Group}},
	abstract = {We present a 3D model-based visual tracking approach using edge and keypoint features in a particle filtering framework. Re-cently, particle filtering based approaches have been proposed to integrate multiple pose hypotheses and have shown good perfor-mance, but most of the work has made an assumption that an initial pose is given. To ameliorate this limitation, we employ keypoint features for initialization of the filter. Given 2D-3D key-point correspondences, we randomly choose a set of minimum correspondences to calculate a set of possible pose hypotheses. Based on the inlier ratio of correspondences, the set of poses are drawn to initialize particles. After the initialization, edge points are employed to estimate inter-frame motions. While we follow a standard edge-based tracking, we perform a refinement process to improve the edge correspondences between sampled model edge points and image edge points. For better tracking perfor-mance, we employ a first order autoregressive state dynamics, which propagates particles more effectively than Gaussian ran-dom walk models. The proposed system re-initializes particles by itself when the tracked object goes out of the field of view or is occluded. The robustness and accuracy of our approach is demonstrated using comparative experiments on synthetic and real image sequences.},
	number = {3},
	author = {Choi, Changhyun and Christensen, Henrik I},
	year = {2011},
}

@article{jiang_learning_2008,
	title = {Learning to {Place} {New} {Objects} in a {Scene}},
	abstract = {—Placing is a necessary skill for a personal robot to have in order to perform tasks such as arranging objects in a disorganized room. The object placements should not only be stable but also be in their semantically preferred placing areas and orientations. This is challenging because an environment can have a large variety of objects and placing areas that may not have been seen by the robot before. In this paper, we propose a learning approach for placing multiple objects in different placing areas in a scene. Given point-clouds of the objects and the scene, we design appropriate features and use a graphical model to encode various properties, such as the stacking of objects, stability, object-area relationship and common placing constraints. The inference in our model is an integer linear program, which we solve efficiently via an LP relaxation. We extensively evaluate our approach on 98 objects from 16 categories being placed into 40 areas. Our robotic experiments show a success rate of 98\% in placing known objects and 82\% in placing new objects stably. We use our method on our robots for performing tasks such as loading several dish-racks, a bookshelf and a fridge with multiple items.},
	author = {Jiang, Yun and Lim, Marcus and Zheng, Changxi and Saxena, Ashutosh},
	year = {2008},
	keywords = {Object, Object Recognition},
}

@article{natale_developmental_2002,
	title = {A developmental approach to grasping},
	issn = {{\textless}null{\textgreater}},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390195},
	doi = {10.1145/1390156.1390195},
	abstract = {In this paper we describe a developmental path which allows a humanoid robot to initiate interaction with the environment by grasping objects. Development begins with the exploration of the robot’s own body (control of the head and arm, identification of the hand) and moves afterward to the external world (reaching and grasping). A final experiment is reported to illustrate how these simple behaviors can be integrated to start autonomous exploration of the environment. In fact we believe that for an active system the capacity to act is not a mere arrival point but it is rather required in order for the system to further develop by acquiring and structuring information about its environment.},
	journal = {Artificial Intelligence},
	author = {Natale, Lorenzo and Metta, Giorgio and Sandini, Giulio},
	year = {2002},
	note = {ISBN: 9781605582054},
	keywords = {body-sch, development, grasping, humanoid robotics},
	pages = {304--311},
}

@article{saxena_robotic_2008,
	title = {Robotic {Grasping} of {Novel} {Objects} using {Vision}},
	volume = {27},
	issn = {0278-3649},
	doi = {10.1177/0278364907087172},
	abstract = {We consider the problemof grasping novel objects, specifically ones that are being seen for the first time through vision. Grasping a previously un- known object, one for which a 3-d model is not available, is a challenging problem. Further, even if given a model, one still has to decide where to grasp the object. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Given two (ormore) images of an ob- ject, our algorithmattempts to identify a few points in each image corresponding to good locations at which to grasp the object. This sparse set of points is then triangulated to obtain a 3-d location atwhich to attempt a grasp. This is in contrast to standard dense stereo, which tries to triangulate every single point in an image (and often fails to return a good 3-d model). Our algorithm for identifying grasp locations from an image is trained via supervised learning, using synthetic images for the training set. We demonstrate this approach on two robotic ma- nipulation platforms. Our algorithm successfully grasps a wide variety of objects, such as plates, tape-rolls, jugs, cellphones, keys, screwdrivers, sta- plers, a thick coil ofwire, a strangely shaped power horn, and others, none of which were seen in the training set. We also apply our method to the task of unloading items from dishwashers.},
	number = {2},
	journal = {The International Journal of Robotics Research},
	author = {Saxena, a. and Driemeyer, J. and Ng, a. Y.},
	year = {2008},
	pmid = {253318100002},
	note = {ISBN: 0278-3649},
	keywords = {Grasp, Object, Object Recognition, Vision, adaptive systems, grasping, learning and, perception, personal robots, robotics, vision of grasping},
	pages = {157--173},
}

@article{lopes_visual_2005,
	title = {Visual learning by imitation with motor representations},
	volume = {35},
	issn = {10834419},
	doi = {10.1109/TSMCB.2005.846654},
	abstract = {ge. Besides the holistic approach to the problem, our approach dif- fers from traditional work in i) the use of motor information for gesture recognition; ii) usage of context (e.g., object affordances) to focus the attention of the recognition system and reduce ambi- guities, and iii) use iconic image representations for the hand, as opposed to fitting kinematic models to the video sequence. This approach is motivated by the finding of visuomotor neu- rons in the F5 area of the macaque brain that suggest that ges- ture recognition/imitation is performed in motor terms (mirror) and rely on the use of object affordances (canonical) to handle am- biguous actions. Our results show that this approach can outperform more con- ventional (e.g., pure visual) methods.},
	number = {3},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
	author = {Lopes, Manuel and Santos-Victor, José},
	year = {2005},
	pmid = {15971913},
	note = {ISBN: 1083-4419},
	keywords = {Anthropomorphic robots, Imitation, Learning, Vision, Visuomotor coordination},
	pages = {438--449},
}

@inproceedings{kjellstrom_tracking_2010,
	title = {Tracking people interacting with objects},
	isbn = {978-1-4244-6984-0},
	doi = {10.1109/CVPR.2010.5540140},
	abstract = {While the problem of tracking 3D human motion has been widely studied, most approaches have assumed that the person is isolated and not interacting with the environment. Environmental constraints, however, can greatly constrain and simplify the tracking problem. The most studied constraints involve gravity and contact with the ground plane. We go further to consider interaction with objects in the environment. In many cases, tracking rigid environmental objects is simpler than tracking high-dimensional human motion. When a human is in contact with objects in the world, their poses constrain the pose of body, essentially removing degrees of freedom. Thus what would appear to be a harder problem, combining object and human tracking, is actually simpler. We use a standard formulation of the body tracking problem but add an explicit model of contact with objects. We find that constraints from the world make it possible to track complex articulated human motion in 3D from a monocular camera.},
	booktitle = {Proceedings of the {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kjellström, Hedvig and Kragić, Danica and Black, Michael J.},
	year = {2010},
	note = {ISSN: 10636919},
	keywords = {Object Recognition, Vision},
	pages = {747--754},
}

@article{johansson_eye-hand_2001,
	title = {Eye-hand coordination in object manipulation.},
	volume = {21},
	issn = {1529-2401},
	abstract = {We analyzed the coordination between gaze behavior, fingertip movements, and movements of the manipulated object when subjects reached for and grasped a bar and moved it to press a target-switch. Subjects almost exclusively fixated certain landmarks critical for the control of the task. Landmarks at which contact events took place were obligatory gaze targets. These included the grasp site on the bar, the target, and the support surface where the bar was returned after target contact. Any obstacle in the direct movement path and the tip of the bar were optional landmarks. Subjects never fixated the hand or the moving bar. Gaze and hand/bar movements were linked concerning landmarks, with gaze leading. The instant that gaze exited a given landmark coincided with a kinematic event at that landmark in a manner suggesting that subjects monitored critical kinematic events for phasic verification of task progress and subgoal completion. For both the obstacle and target, subjects directed saccades and fixations to sites that were offset from the physical extension of the objects. Fixations related to an obstacle appeared to specify a location around which the extending tip of the bar should travel. We conclude that gaze supports hand movement planning by marking key positions to which the fingertips or grasped object are subsequently directed. The salience of gaze targets arises from the functional sensorimotor requirements of the task. We further suggest that gaze control contributes to the development and maintenance of sensorimotor correlation matrices that support predictive motor control in manipulation.},
	number = {17},
	journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
	author = {Johansson, Roland S and Westling, G. and Bäckström, A. and Flanagan, J Randall},
	year = {2001},
	pmid = {11517279},
	note = {ISBN: 1529-2401 (Electronic){\textbackslash}n0270-6474 (Linking)},
	keywords = {eye, grasping, hand coordination, hand movement, object manipulation, obstacle avoidance, saccadic eye},
	pages = {6917--6932},
}

@article{dragan_effects_2015,
	title = {Effects of {Robot} {Motion} on {Human}-{Robot} {Collaboration}},
	volume = {1},
	issn = {21672148},
	url = {http://dl.acm.org/citation.cfm?doid=2696454.2696473},
	doi = {10.1145/2696454.2696473},
	abstract = {Most motion in robotics is purely functional, planned to achieve the goal and avoid collisions. Such motion is great in isolation, but collaboration affords a human who is watching the motion and making inferences about it, trying to coordinate with the robot to achieve the task. This paper analyzes the benefit of planning motion that explicitly enables the collaborator's inferences on the success of physical collaboration, as measured by both objective and subjective metrics. Results suggest that legible motion, planned to clearly express the robot's intent, leads to more fluent collaborations than predictable motion, planned to match the collaborator's expectations. Furthermore, purely functional motion can harm coordination, which negatively affects both task efficiency, as well as the participants' perception of the collaboration.},
	journal = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction - HRI '15},
	author = {Dragan, Anca D and Bauman, Shira and Forlizzi, Jodi and Srinivasa, Siddhartha S},
	year = {2015},
	note = {ISBN: 9781450328838},
	keywords = {Action understanding, Human-Robot Interaction(HRI), Motion, Robotics, coordination, human-robot collaboration, intent, motion},
	pages = {51--58},
}

@article{noris_wearable_2011,
	title = {A wearable gaze tracking system for children in unconstrained environments},
	volume = {115},
	issn = {10773142},
	doi = {10.1016/j.cviu.2010.11.013},
	abstract = {We present here a head-mounted gaze tracking system for the study of visual behavior in unconstrained environments. The system is designed both for adults and for infants as young as 1 year of age. The system uses two CCD cameras to record a very wide field of view (96?? ?? 96??) that allows to study both central and peripheral vision. A small motor-driven mirror allows to obtain the direction of the wearer's gaze with no need for active lighting and with little intrusiveness. The calibration of the system is done offline allowing experiments to be conducted with subjects who cannot cooperate in a calibration phase (e.g. very young children, animals). We use illumination normalization to increase the robustness of the system, and eye blinking detection to avoid tracking errors. We use Support Vector Regression to estimate a mapping between the appearance of the eyes and the corresponding gaze direction. The system can be used successfully indoors as well as outdoors and reaches an accuracy of 1.59?? with adults and 2.42?? with children. ?? 2011 Published by Elsevier Inc.},
	number = {4},
	journal = {Computer Vision and Image Understanding},
	author = {Noris, Basilio and Keller, Jean Baptiste and Billard, Aude},
	year = {2011},
	note = {ISBN: 1077-3142},
	keywords = {Appearance based, Children, Eye blinking, Gaze tracking, Lay users, Support Vector Regression, Unconstrained environments},
	pages = {476--486},
}

@article{wang_our_2014,
	title = {In our own image? {Emotional} and neural processing differences when observing human-human vs human-robot interactions},
	volume = {10},
	issn = {17495024},
	doi = {10.1093/scan/nsv043},
	abstract = {Notwithstanding the significant role that human–robot interactions (HRI) will play in the near future, limited research has explored the neural correlates of feeling eerie in response to social robots. To address this empirical lacuna, the current investigation examined brain activity using functional magnetic resonance imaging while a group of participants (n ¼ 26) viewed a series of human–human interactions (HHI) and HRI. Although brain sites constituting the mentalizing network were found to respond to both types of interactions, systematic neural variation across sites signaled diverging social-cognitive strategies during HHI and HRI processing. Specifically, HHI elicited increased activity in the left temporal–parietal junction indicative of situation-specific mental state attributions, whereas HRI recruited the precuneus and the ventromedial prefrontal cortex (VMPFC) suggestive of script-based social reasoning. Activity in the VMPFC also tracked feelings of eeriness towards HRI in a parametric manner, revealing a potential neural correlate for a phenomenon known as the uncanny valley. By demonstrating how understanding social interactions depends on the kind of agents involved, this study highlights pivotal sub-routes of impression formation and identifies prominent challenges in the use of humanoid robots.},
	number = {11},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Wang, Yin and Quadflieg, Susanne},
	year = {2014},
	pmid = {25911418},
	note = {ISBN: 1749-5016},
	keywords = {Impression formation, Mind attributions, Person construal, Person dyads, Social robotics},
	pages = {1515--1524},
}

@article{henry_rgb-d_2010,
	title = {{RGB}-{D} {Mapping} : {Using} {Depth} {Cameras} for {Dense} {3D} {Modeling} of {Indoor} {Environments}},
	volume = {1},
	issn = {0278-3649},
	doi = {10.1177/0278364911434148},
	abstract = {RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. In this paper we investigate how such cameras can be used in the context of robotics, speci cally for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. We present RGB-D Mapping, a full 3D mapping system that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment. Visual and depth information are also combined for view-based loop closure detection, followed by pose optimization to achieve globally consistent maps.We evaluate RGB-DMapping on two large indoor environments, and show that it effectively combines the visual and shape information available from RGB-D cameras.},
	number = {c},
	journal = {RGBD Advanced Reasoning with Depth Cameras Workshop in conjunction with RSS},
	author = {Henry, Peter and Krainin, Michael and Herbst, Evan and Ren, Xiaofeng and Fox, Dieter},
	year = {2010},
	note = {ISBN: 0278364911434},
	keywords = {Kinect, Modelling, RGB-D, rgb-d camera, slam},
	pages = {9--10},
}

@article{mortl_role_2012,
	title = {The role of roles: {Physical} cooperation between humans and robots},
	volume = {31},
	issn = {0278-3649},
	url = {http://ijr.sagepub.com/content/31/13/1656.short},
	doi = {10.1177/0278364912455366},
	abstract = {Since the strict separation of working spaces of humans and robots has experienced a softening due to recent robotics research achievements, close interaction of humans and robots comes rapidly into reach. In this context, physical human-robot interaction raises a number of questions regarding a desired intuitive robot behavior. The continuous bilateral information and energy exchange requires an appropriate continuous robot feedback. Investigating a cooperative manipulation task, the desired behavior is a combination of an urge to fulfill the task, a smooth instant reactive behavior to human force inputs and an assignment of the task effort to the cooperating agents. In this paper, a formal analysis of human-robot cooperative load transport is presented. Three different possibilities for the assignment of task effort are proposed. Two proposed dynamic role exchange mechanisms adjust the robot's urge to complete the task based on the human feedback. For comparison, a static role allocation strategy not relying on the human agreement feedback is investigated as well. All three role allocation mechanisms are evaluated in a user study that involves large-scale kinesthetic interaction and full-body human motion. Results show tradeoffs between subjective and objective performance measures stating a clear objective advantage of the proposed dynamic role allocation scheme.},
	number = {13},
	journal = {The International Journal of Robotics Research},
	author = {Mörtl, Alexander and Lawitzky, Martin and Kucukyilmaz, Ayse and Sezgin, Metin and Basdogan, Cagatay and Hirche, Sandra},
	year = {2012},
	note = {ISBN: 0278364912455},
	keywords = {cooperative manipulation, human feedback, input decomposition, kinesthetic interaction, load sharing},
	pages = {1656--1674},
}

@article{koppula_semantic_2011,
	title = {Semantic {Labeling} of {3D} {Point} {Clouds} for {Indoor} {Scenes}},
	abstract = {RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic la- beling of these 3D point clouds. We propose a graphical model that captures var- ious features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the models parsimony becomes im- portant and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06\% in labeling 17 object classes for offices, and 73.38\% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms},
	journal = {Neural Information Processing Systems},
	author = {Koppula, Hema Swetha and Anand, Abhishek and Joachims, Thorsten and Saxena, Ashutosh},
	year = {2011},
	note = {ISBN: 9781618395993},
	keywords = {Labeling, RGB-D},
	pages = {1--9},
}

@inproceedings{grizou_interactive_2014,
	title = {Interactive learning from unlabeled instructions},
	isbn = {978-0-9749039-1-0},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84923308524&partnerID=tZOtx3y1},
	abstract = {Interactive learning deals with the problem of learning and solving tasks using human instructions. It is common in human-robot interaction, tutoring systems, and in human-computer interfaces such as brain-computer ones. In most cases, learning these tasks is possible because the signals are predefined or an ad-hoc calibration procedure allows to map signals to specific meanings. In this paper, we address the problem of simultaneously solving a task under human feedback and learning the associated meanings of the feedback signals. This has important practical application since the user can start controlling a device from scratch, without the need of an expert to define the meaning of signals or carrying out a calibration phase. The paper proposes an algorithm that simultaneously assign meanings to signals while solving a sequential task under the assumption that both, human and machine, share the same a priori on the possible instruction meanings and the possible tasks. Furthermore, we show using synthetic and real EEG data from a brain-computer interface that taking into account the uncertainty of the task and the signal is necessary for the machine to actively plan how to solve the task efficiently.},
	booktitle = {Uncertainty in {Artificial} {Intelligence} - {Proceedings} of the 30th {Conference}, {UAI} 2014},
	author = {Grizou, Jonathan and Iturrate, Iñaki and Montesano, Luis and Oudeyer, Pierre Yves and Lopes, Manuel},
	year = {2014},
	keywords = {EEG, Human Activity, Human-Robot Interaction(HRI)},
	pages = {290--299},
}

@article{sung_unstructured_2012,
	title = {Unstructured human activity detection from {RGBD} images},
	issn = {10504729},
	doi = {10.1109/ICRA.2012.6224591},
	abstract = {Being able to detect and recognize human activities is important for making personal assistant robots useful in performing assistive tasks. The challenge is to develop a system that is low-cost, reliable in unstructured home settings, and also straightforward to use. In this paper, we use a RGBD sensor (Microsoft Kinect) as the input sensor, and present learning algorithms to infer the activities. Our algorithm is based on a hierarchical maximum entropy Markov model (MEMM). It considers a person's activity as composed of a set of sub-activities, and infers the two-layered graph structure using a dynamic programming approach. We test our algorithm on detecting and recognizing twelve different activities performed by four people in different environments, such as a kitchen, a living room, an office, etc., and achieve an average performance of 84.3\% when the person was seen before in the training set (and 64.2\% when the person was not seen before).},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	author = {Sung, Jaeyong and Ponce, Colin and Selman, Bart and Saxena, Ashutosh},
	year = {2012},
	note = {arXiv: 1107.0169
ISBN: 9781467314039},
	keywords = {Activity Detection},
	pages = {842--849},
}

@article{krupenye_great_2016,
	title = {Great apes anticipate that other individuals will act according to false beliefs},
	volume = {354},
	issn = {0036-8075},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaf8110},
	doi = {10.1126/science.aaf8110},
	abstract = {Humans operate with a “theory of mind” with which they are able to understand that others’ actions are driven not by reality but by beliefs about reality, even when those beliefs are false. Although great apes sharewith humansmany social-cognitive skills, they have repeatedly failed experimental tests of such false-belief understanding.We use an anticipatory looking test (originallydeveloped for human infants) to showthat three species of great apes reliably look in anticipation of an agent acting on a location where he falsely believes an object to be, even though the apes themselves know that the object is no longer there. Our results suggest that great apes also operate, at least on an implicit level,with an understanding of false beliefs.},
	number = {6308},
	journal = {Science},
	author = {Krupenye, Christopher and Kano, Fumihiro and Hirata, Satoshi and Call, Josep and Tomasello, Michael},
	year = {2016},
	pmid = {27846501},
	keywords = {Anticipation, Apes or monkeys, False beliefs},
	pages = {110--114},
}

@article{collet_moped_2011,
	title = {The {MOPED} framework: {Object} recognition and pose estimation for manipulation},
	volume = {30},
	issn = {0278-3649},
	doi = {10.1177/0278364911401765},
	abstract = {We present MOPED, a framework for Multiple Object Pose Estimation and Detection that seamlessly integrates single-image and multi-image object recognition and pose estimation in one optimized, robust, and scalable framework. We address two main challenges in computer vision for robotics: robust performance in complex scenes, and low latency for real-time operation. We achieve robust performance with Iterative Clustering Estimation (ICE), a novel algorithm that iteratively combines feature clustering with robust pose estimation. Feature clustering quickly partitions the scene and produces object hypotheses. The hypotheses are used to further refine the feature clusters, and the two steps iterate until convergence. ICE is easy to parallelize, and easily integrates single- and multi-camera object recognition and pose estimation. We also introduce a novel object hypothesis scoring function based on M-estimator theory, and a novel pose clustering algorithm that robustly handles recognition outliers. We achieve scalability and low latency with an improved feature matching algorithm for large databases, a GPU/CPU hybrid architecture that exploits parallelism at all levels, and an optimized resource scheduler. We provide extensive experimental results demonstrating state-of-the-art performance in terms of recognition, scalability, and latency in real-world robotic applications.},
	number = {10},
	journal = {The International Journal of Robotics Research},
	author = {Collet, Alvaro and Martinez, Manuel and Srinivasa, Siddhartha S},
	year = {2011},
	keywords = {Object, Object Recognition, Pose, Pose Estimation, architecture optimization, efficiency analysis, object recognition, personal robotics, pose estimation, robotic manipulation, scalability analysis, scene complexity},
	pages = {1284--1306},
}

@article{ansuini_grasping_2016,
	title = {Grasping {Others}’ {Movements}: {Rapid} {Discrimination} of {Object} {Size} {From} {Observed} {Hand} {Movements}.},
	volume = {42},
	issn = {1939-1277},
	url = {http://dx.doi.org/10.1037/xhp0000169.supp},
	doi = {10.1037/xhp0000169},
	abstract = {During reach-to-grasp movements, the hand is gradually molded to conform to the size and shape of the object to be grasped. Yet the ability to glean information about object properties by observing grasping movements is poorly understood. In this study, we capitalized on the effect of object size to investigate the ability to discriminate the size of an invisible object from movement kinematics. The study consisted of 2 phases. In the first action execution phase, to assess grip scaling, we recorded and analyzed reach-to-grasp movements performed toward differently sized objects. In the second action observation phase, video clips of the corresponding movements were presented to participants in a two-alternative forced-choice task. To probe discrimination performance over time, videos were edited to provide selective vision of different periods from 2 viewpoints. Separate analyses were conducted to determine how the participants' ability to discriminate between stimulus alternatives (Type I sensitivity) and their metacognitive ability to discriminate between correct and incorrect responses (Type II sensitivity) varied over time and viewpoint. We found that as early as 80 ms after movement onset, participants were able to discriminate object size from the observation of grasping movements delivered from the lateral viewpoint. For both viewpoints, information pickup closely matched the evolution of the hand's kinematics, reaching an almost perfect performance well before the fingers made contact with the object (60\% of movement duration). These findings suggest that observers are able to decode object size from kinematic sources specified early on in the movement. (PsycINFO Database Record},
	number = {7},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Ansuini, Caterina and Cavallo, Andrea and Koul, Atesh and D’Ausilio, Alessandro and Taverna, Laura and Becchio, Cristina},
	year = {2016},
	pmid = {27078036},
	keywords = {10, 1037, action prediction, and shape, constrained by the intrinsic, doi, dx, grasping movements are necessarily, http, interestingly, kinematics, object size, org, properties of the object, reach-to-grasp, such as its size, supp, supplemental materials, time course, xhp0000169},
	pages = {918--929},
}

@article{singer_survey_2011,
	title = {A {Survey} of {Quantitative} {Team} {Performance} {Metrics} for {Human}-{Robot} {Collaboration}},
	doi = {10.2514/6.2011-5248},
	abstract = {Humans and robots have been increasingly used not only in the same workspace, but as team members that interact to accomplish overall mission goals. With a multitude of options developing for how humans and robots can simultaneously participate on a team, it has become necessary to quantitatively analyze the performance of the heterogeneous teams to enable comparison between different team configurations. This paper contains a survey of the field of collaborative human and robot team performance metric models, and examines existing overall team quantitative performance models to determine which are more applicable to future human and robotic space exploration missions. I.},
	number = {July},
	journal = {41st International Conference on Environmental Systems},
	author = {Singer, SM and Akin, DL},
	year = {2011},
	note = {ISBN: 978-1-60086-948-8},
	pages = {1--19},
}

@article{van_der_wel_let_2011,
	title = {Let the force be with us: {Dyads} exploit haptic coupling for coordination},
	volume = {37},
	issn = {1939-1277},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0022337},
	doi = {10.1037/a0022337},
	abstract = {People often perform actions that involve a direct physical coupling with another person, such as when moving furniture together. Here, we examined how people successfully coordinate such actions with others. We tested the hypothesis that dyads amplify their forces to create haptic information to coordinate. Participants moved a pole (resembling a pendulum) back and forth between two targets at different amplitudes and frequencies. They did so by pulling on cords attached to the base of the pole, one on each side. In the individual condition, one participant performed this task bimanually, and in the joint condition two participants each controlled one cord. We measured the moment-to-moment pulling forces on each cord and the pole kinematics to determine how well individuals and dyads performed. Results indicated that dyads produced much more overlapping forces than individuals, especially for tasks with higher coordination requirements. Thus, the results suggest that dyads amplify their forces to generate a haptic information channel. This likely reflects a general coordination principle in haptic joint action, where force amplification allows dyads to perform at the same level as individuals.},
	number = {5},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {van der Wel, Robrecht P. R. D. and Knoblich, Guenther and Sebanz, Natalie},
	year = {2011},
	pmid = {21417545},
	keywords = {haptics, interpersonal coordination, joint action},
	pages = {1420--1431},
}

@article{hecht_differences_2013,
	title = {Differences in {Neural} {Activation} for {Object}-{Directed} {Grasping} in {Chimpanzees} and {Humans}},
	volume = {33},
	issn = {1529-2401},
	doi = {10.1523/jneurosci.2172-13.2013},
	abstract = {The human faculty for object-mediated action, including tool use and imitation, exceeds that of even our closest primate relatives and is a key foundation of human cognitive and cultural uniqueness. In humans and macaques, observing object-directed grasping actions activates a network of frontal, parietal, and occipitotemporal brain regions, but differences in human and macaque activation suggest that this system has been a focus of selection in the primate lineage. To study the evolution of this system, we performed functional neuroimaging in humans' closest living relatives, chimpanzees. We compare activations during performance of an object-directed manual grasping action, observation of the same action, and observation of a mimed version of the action that consisted of only movements without results. Performance and observation of the same action activated a distributed frontoparietal network similar to that reported in macaques and humans. Like humans and unlike macaques, these regions were also activated by observing movements without results. However, in a direct chimpanzee/human comparison, we also identified unique aspects of human neural responses to observed grasping. Chimpanzee activation showed a prefrontal bias, including significantly more activity in ventrolateral prefrontal cortex, whereas human activation was more evenly distributed across more posterior regions, including significantly more activation in ventral premotor cortex, inferior parietal cortex, and inferotemporal cortex. This indicates a more "bottom-up" representation of observed action in the human brain and suggests that the evolution of tool use, social learning, and cumulative culture may have involved modifications of frontoparietal interactions.},
	number = {35},
	journal = {Journal of Neuroscience},
	author = {Hecht, Erin E and Murphy, Lauren E and Gutman, David A and Votaw, John R and Schuster, David M and Preuss, Todd M and Orban, Guy A and Stout, Dietrich and Parr, Lisa A},
	year = {2013},
	pmid = {23986247},
	note = {ISBN: 0270-6474},
	keywords = {Brain, Human, Human Experiment, Human Grasp, Macaque, Monkey Experiment},
	pages = {14117--14134},
}

@article{javdani_near_2014,
	title = {Near {Optimal} {Bayesian} {Active} {Learning} for {Decision} {Making}},
	volume = {33},
	abstract = {How should we gather information to make ef-fective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hy-potheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient condi-tions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the in-tractable optimal policy. Our efficient imple-mentation of the algorithm relies on comput-ing subsets of the complete homogeneous sym-metric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator.},
	author = {Javdani, Shervin and Krause, Andreas and Chen, Yuxin and Bagnell, J Andrew},
	year = {2014},
	note = {arXiv: 1402.5886v1},
}

@article{pfeiffer_predicting_2016,
	title = {Predicting {Actions} to {Act} {Predictably} : {Cooperative} {Partial} {Motion} {Planning} with {Maximum} {Entropy} {Models}},
	issn = {21530866},
	doi = {10.1109/IROS.2016.7759329},
	abstract = {— This paper reports on a data-driven motion plan-ning approach for interaction-aware, socially-compliant robot navigation among human agents. Autonomous mobile robots navigating in workspaces shared with human agents require motion planning techniques providing seamless integration and smooth navigation in such. Smooth integration in mixed scenarios calls for two abilities of the robot: predicting actions of others and acting predictably for them. The former requirement requests trainable models of agent behaviors in order to accurately forecast their actions in the future, taking into account their reaction on the robot's decisions. A human-like navigation style of the robot facilitates other agents—most likely not aware of the underlying planning technique applied—to predict the robot motion vice versa, resulting in smoother joint navigation. The approach presented in this paper is based on a feature-based maximum entropy model and is able to guide a robot in an unstructured, real-world environment. The model is trained to predict joint behavior of heterogeneous groups of agents from onboard data of a mobile platform. We evaluate the benefit of interaction-aware motion planning in a realistic public setting with a total distance traveled of over 4 km. Interestingly the motion models learned from human-human interaction did not hold for robot-human interaction, due to the high attention and interest of pedestrians in testing basic braking functionality of the robot.},
	journal = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Pfeiffer, Mark and Schwesinger, Ulrich and Sommer, Hannes and Galceran, Enric and Siegwart, Roland},
	year = {2016},
	note = {ISBN: 9781509037612},
	keywords = {Motion, Prediction},
	pages = {2096--2101},
}

@article{finley_training_2008,
	title = {Training structural {SVMs} when exact inference is intractable},
	issn = {1605582050},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390195},
	doi = {10.1145/1390156.1390195},
	abstract = {While discriminative training (e.g., CRF, structural SVM) holds much promise for ma- chine translation, image segmentation, and clustering, the complex inference these ap- plications require make exact training in- tractable. This leads to a need for ap- proximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Fo- cusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and over- generating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained struc- tural SVMs, focusing on fully connected pair- wise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergener- ating methods, are empirically robust rela- tive to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors. 1. Introduction Discriminative training methods like cond},
	journal = {Icml},
	author = {Finley, Thomas and Joachims, Thorsten},
	year = {2008},
	note = {ISBN: 9781605582054},
	pages = {304--311},
}

@article{jiang_learning_2012,
	title = {Learning {Object} {Arrangements} in {3D} {Scenes} using {Human} {Context}},
	abstract = {We consider the problem of learning object arrangements in a 3D scene. The key idea here is to learn how objects relate to human poses based on their affordances, ease of use and reachability. In contrast to modeling object-object relationships, modeling human-object relationships scales linearly in the number of objects. We design appropriate density functions based on 3D spatial features to capture this. We learn the distribution of human poses in a scene using a variant of the Dirichlet process mixture model that allows sharing of the density function parameters across the same object types. Then we can reason about arrangements of the objects in the room based on these meaningful human poses. In our extensive experiments on 20 different rooms with a total of 47 objects, our algorithm predicted correct placements with an average error of 1.6 meters from ground truth. In arranging five real scenes, it received a score of 4.3/5 compared to 3.7 for the best baseline method.},
	journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
	author = {Jiang, Yun and Lim, Marcus and Saxena, Ashutosh},
	year = {2012},
	note = {arXiv: 1206.6462
ISBN: 978-1-4503-1285-1},
	keywords = {Human, Object, Object Pose},
	pages = {1543--1550},
}

@article{thelen_development_1996,
	title = {Development of {Reaching} {During} the {First} {Year}: {Role} of {Movement} {Speed}},
	volume = {00},
	abstract = {When infants first learn to reach at about 4 months, their hand paths are jerky and tortuous, but their reaches become smoother and straighter over the first year. Here the authors consider the role of the underlying limb dynamics, which scale with movement speed, on the development of trajectory control. The authors observed 4 infants weekly and then biweekly from reach onset to 1 year. Improvements in trajectories were not linear, but showed plateaus and regressions in straightness and smoothness. When infants' nonreaching movements were fast, their reaches were also fast, and faster reaches were also less straight. This is consistent with an equilibrium trajectory form of control, where development involves the increasing ability to stabilize the trajectory against self-generated movement perturbations. Recently, there has been increasing interest in the devel-opment of infant reaching as a means of understanding the more general issue of how the brain controls the upper limbs. Hundreds of studies have addressed this question using adult participants, and we know a great deal about arm trajectory formation when the participants are well practiced and the tasks are simple and repeated from trial to trial (see Georgopoulos, 1986; Jeannerod, 1988, for reviews). Only a few studies, however, have asked how humans actually learn to control their limbs during infancy. Whereas adults are highly accomplished in integrating the cognitive, visual, proprioceptive, and biomechanical demands of reaching and grasping, this is not true of infants. Thus, the gradual emergence of reaching skill offers an opportunity (a) to identify the multiple problems infants face in getting their hands to objects and (b) to examine the ways in which they},
	number = {5},
	journal = {Journal of Experimental Psychology},
	author = {Thelen, Esther and Corbetta, Daniela and Spencer, John P and Adolph, Karen E and Berkoben, Deanna and Gormley, Dexter and Hagen, Charles W and Jensen, Jody L and Kamm, Kathi and Konczak, Jtirgen and Neal, Ronald S and Schoeny, Michael and Smith Ronald Zemicke, Gregory A and Schneider, Klaus and Kay, Bruce and Goldfield, Eugene and Smith, Gregory A and Shepherd, Roberta and Bingham, Geoffrey and Viken, Richard},
	year = {1996},
	keywords = {Human-Human Interaction (HHI), Joint-action},
	pages = {1059--1076},
}

@article{shotton_real-time_2013,
	title = {Real-time human pose recognition in parts from single depth images},
	volume = {411},
	issn = {1860949X},
	doi = {10.1007/978-3-642-28661-2-5},
	abstract = {We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
	journal = {Studies in Computational Intelligence},
	author = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
	year = {2013},
	pmid = {23109523},
	note = {arXiv: 1111.6189v1
ISBN: 9783642286605},
	keywords = {Human, Human Pose, Kinect, Pose, Pose Recognition, RGB-D},
	pages = {119--135},
}

@article{ganesh_two_2014,
	title = {Two is better than one: physical interactions improve motor performance in humans.},
	volume = {4},
	issn = {2045-2322},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3899645&tool=pmcentrez&rendertype=abstract},
	doi = {10.1038/srep03824},
	abstract = {How do physical interactions with others change our own motor behavior? Utilizing a novel motor learning paradigm in which the hands of two - individuals are physically connected without their conscious awareness, we investigated how the interaction forces from a partner adapt the motor behavior in physically interacting humans. We observed the motor adaptations during physical interactions to be mutually beneficial such that both the worse and better of the interacting partners improve motor performance during and after interactive practice. We show that these benefits cannot be explained by multi-sensory integration by an individual, but require physical interaction with a reactive partner. Furthermore, the benefits are determined by both the interacting partner's performance and similarity of the partner's behavior to one's own. Our results demonstrate the fundamental neural processes underlying human physical interactions and suggest advantages of interactive paradigms for sport-training and physical rehabilitation.},
	journal = {Scientific reports},
	author = {Ganesh, G and Takagi, A and Osu, R and Yoshioka, T and Kawato, M and Burdet, E},
	year = {2014},
	pmid = {24452767},
	note = {ISBN: 2045-2322 (Electronic){\textbackslash}r2045-2322 (Linking)},
	pages = {3824},
}

@article{erlhagen_development_2007,
	title = {On the development of intention understanding for joint action tasks},
	volume = {2007},
	doi = {10.1109/DEVLRN.2007.4354022},
	abstract = {Our everyday, common sense ability to discern the intentions of others' from their motions is fundamental for a successful cooperation in joint action tasks. In this paper we address in a modeling study the question of how the ability to understand complex goal-directed action sequences may develop during learning and practice. The model architecture reflects recent neurophysiological findings that suggest the existence of chains of mirror neurons associated with specific goals. These chains may be activated by external events to simulate the consequences of observed actions. Using the mathematical framework of dynamical neural fields to model the dynamics of different neural populations representing goals, action means and contextual cues, we show that such chains may develop based on a local, Hebbian learning rule. We validate the functionality of the learned model in a joint action task in which an observer robot infers the intention of a partner to chose a complementary action sequence.},
	number = {July},
	journal = {2007 IEEE 6th International Conference on Development and Learning, ICDL},
	author = {Erlhagen, Wolfram and Mukovskiy, Albert and Chersi, Fabian and Bicho, Estela},
	year = {2007},
	note = {ISBN: 1424411165},
	keywords = {Dynamic field model, Grasp, Human-Robot Interaction(HRI), Intention understanding, Joint action in autonomous robots, Joint-action, Mirror neurons, Neuroscience, Social development},
	pages = {140--145},
}

@article{kanda_practical_nodate,
	title = {A practical experiment with interactive humanoid robots in a human society},
	journal = {English},
	author = {Kanda, Takayuki and Hirano, Takayuki and Eaton, Daniel and Ishiguro, Hiroshi},
	keywords = {Experiment, Human-Robot Interaction(HRI)},
}

@article{eastough_movement_2007,
	title = {Movement kinematics in prehension are affected by grasping objects of different mass},
	volume = {176},
	issn = {00144819},
	doi = {10.1007/s00221-006-0749-3},
	abstract = {Previous research has identified that prehension is composed of a reach and a grasp component and that the position and size of the object can independently affect each. However, no effects on prior to contact prehension kinematics have been reported for manipulations in object mass. We felt that this 'lack of a finding' was surprising, as a more accurate grip position on heavier objects would prevent slippage and rotation when lifting the object. Therefore, we hypothesized that increased object mass would effect the grasp component movement kinematics prior to contact in preparation for a stable final grip placement on the object. We report two experiments in which participants reached, grasped and lifted objects of various size and mass, and their movement kinematics were recorded using a motion tracking system. The results showed that the mass of the object significantly influenced prior-to-contact grasp kinematics. Both studies showed that the heavy compared to light objects caused increased peak grasp aperture, a final finger and thumb placement on the object that more closely passed through the object centre of mass, increased lift delay and reduced peak lift velocity. The data are discussed in terms of how object mass influences the reach, grasp and lift components of prehensile movement.},
	number = {1},
	journal = {Experimental Brain Research},
	author = {Eastough, Daniel and Edwards, Martin G.},
	year = {2007},
	pmid = {17072606},
	note = {ISBN: 0014-4819 (Print) 0014-4819 (Linking)},
	keywords = {Grasp, Human Experiment, Mass, Object, Objects Weights, Prehension, Vision, Weight},
	pages = {193--198},
}

@article{fadiga_motor_2013,
	title = {Motor facilitation during action observation : a magnetic stimulation study},
	volume = {73},
	issn = {0022-3077},
	doi = {10.1.1.299.4524},
	number = {6},
	journal = {Journal of Neurophysiology},
	author = {Fadiga, L and Fogassi, L and Pavesi, G and Rizzolatti, G},
	year = {2013},
	pmid = {7666169},
	note = {ISBN: 0022-3077 (Print)},
	pages = {2608--2611},
}

@article{breazeal_robots_2002,
	title = {Robots that imitate humans},
	volume = {6},
	issn = {13646613},
	doi = {10.1016/S1364-6613(02)02016-8},
	abstract = {The study of social learning in robotics has been motivated by both scientific interest in the learning process and practical desires to produce machines that are useful, flexible, and easy to use. In this review, we introduce the social and task-oriented aspects of robot imitation. We focus on methodologies for addressing two fundamental problems. First, how does the robot know what to imitate? And second, how does the robot map that perception onto its own action repertoire to replicate it? In the future, programming humanoid robots to perform new tasks might be as simple as showing them.},
	number = {11},
	journal = {Trends in Cognitive Sciences},
	author = {Breazeal, Cynthia and Scassellati, Brian},
	year = {2002},
	pmid = {12457900},
	note = {ISBN: 1364-6613},
	keywords = {Human-Robot Interaction(HRI), Imitation},
	pages = {481--487},
}

@article{huang_using_2015,
	title = {Using gaze patterns to predict task intent in collaboration.},
	volume = {6},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01049/abstract},
	doi = {10.3389/fpsyg.2015.01049},
	abstract = {In everyday interactions, humans naturally exhibit behavioral cues, such as gaze and head movements, that signal their intentions while interpreting the behavioral cues of others to predict their intentions. Such intention prediction enables each partner to adapt their behaviors to the intent of others, serving a critical role in joint action where parties work together to achieve a common goal. Among behavioral cues, eye gaze is particularly important in understanding a person's attention and intention. In this work, we seek to quantify how gaze patterns may indicate a person's intention. Our investigation was contextualized in a dyadic sandwich-making scenario in which a "worker" prepared a sandwich by adding ingredients requested by a "customer." In this context, we investigated the extent to which the customers' gaze cues serve as predictors of which ingredients they intend to request. Predictive features were derived to represent characteristics of the customers' gaze patterns. We developed a support vector machine-based (SVM-based) model that achieved 76\% accuracy in predicting the customers' intended requests based solely on gaze features. Moreover, the predictor made correct predictions approximately 1.8 s before the spoken request from the customer. We further analyzed several episodes of interactions from our data to develop a deeper understanding of the scenarios where our predictor succeeded and failed in making correct predictions. These analyses revealed additional gaze patterns that may be leveraged to improve intention prediction. This work highlights gaze cues as a significant resource for understanding human intentions and informs the design of real-time recognizers of user intention for intelligent systems, such as assistive robots and ubiquitous devices, that may enable more complex capabilities and improved user experience.},
	number = {July},
	journal = {Frontiers in psychology},
	author = {Huang, Chien-Ming and Andrist, Sean and Sauppé, Allison and Mutlu, Bilge},
	year = {2015},
	pmid = {26257694},
	keywords = {Action understanding, Experiment, Gaze Patterns, Human Activity, Human intentions, Human-Robot Interaction(HRI), Prediction, Support vector machine, eye gaze, intention predictor, task intent},
	pages = {1049},
}

@article{song_predicting_nodate,
	title = {Predicting {Human} {Intention} in {Visual} {Observations} of {Hand}/{Object} {Interactions}},
	abstract = {— The main contribution of this paper is a prob-abilistic method for predicting human manipulation intention from image sequences of human-object interaction. Predicting intention amounts to inferring the imminent manipulation task when human hand is observed to have stably grasped the object. Inference is performed by means of a probabilistic graphical model that encodes object grasping tasks over the 3D state of the observed scene. The 3D state is extracted from RGB-D image sequences by a novel vision-based, markerless hand-object 3D tracking framework. To deal with the high-dimensional state-space and mixed data types (discrete and continuous) involved in grasping tasks, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Experimentation showed that the model trained on simulated data can provide a potent basis for accurate goal-inference with partial and noisy observations of actual real-world demonstrations. We also show a grasp selection process, guided by the inferred human intention, to illustrate the use of the system for goal-directed grasp imitation.},
	author = {Song, Dan and Kyriazis, Nikolaos and Oikonomidis, Iason and Papazov, Chavdar and Argyros, Antonis and Burschka, Darius and Kragic, Danica},
	keywords = {Experiment},
}

@article{mason_robot_2011,
	title = {Robot self-initiative and personalization by learning through repeated interactions},
	volume = {69},
	issn = {07383991},
	doi = {10.1145/1957656.1957814},
	abstract = {OBJECTIVE: To determine whether an advance directive redesigned to meet most adults' literacy needs (fifth grade reading level with graphics) was more useful for advance care planning than a standard form ({\textgreater}12th grade level). METHODS: We enrolled 205 English and Spanish-speaking patients, aged {\textgreater}/=50 years from an urban, general medicine clinic. We randomized participants to review either form. Main outcomes included acceptability and usefulness in advance care planning. Participants then reviewed the alternate form; we assessed form preference and six-month completion rates. RESULTS: Forty percent of enrolled participants had limited literacy. Compared to the standard form, the redesigned form was rated higher for acceptability and usefulness in care planning, P{\textless}/=0.03, particularly for limited literacy participants (P for interaction {\textless}/=0.07). The redesigned form was preferred by 73\% of participants. More participants randomized to the redesigned form completed an advance directive at six months (19\% vs. 8\%, P=0.03); of these, 95\% completed the redesigned form. CONCLUSIONS: The redesigned advance directive was rated more acceptable and useful for advance care planning and was preferred over a standard form. It also resulted in higher six-month completion rates. PRACTICE IMPLICATIONS: An advance directive redesigned to meet most adults' literacy needs may better enable patients to engage in advance care planning.},
	number = {1-3},
	journal = {Proceedings of the 6th international conference on Humanrobot interaction HRI 11},
	author = {Mason, Martin and Lopes, Manuel C},
	year = {2011},
	pmid = {17942272},
	note = {ISBN: 9781450305617},
	keywords = {Grasp, human robot interaction, learning demonstration},
	pages = {433},
}

@article{hayhoe_eye_2005,
	title = {Eye movements in natural behavior},
	volume = {9},
	issn = {13646613},
	doi = {10.1016/j.tics.2005.02.009},
	abstract = {The classic experiments of Yarbus over 50 years ago revealed that saccadic eye movements reflect cognitive processes. But it is only recently that three separate advances have greatly expanded our understanding of the intricate role of eye movements in cognitive function. The first is the demonstration of the pervasive role of the task in guiding where and when to fixate. The second has been the recognition of the role of internal reward in guiding eye and body movements, revealed especially in neurophysiological studies. The third important advance has been the theoretical developments in the fields of reinforcement learning and graphic simulation. All of these advances are proving crucial for understanding how behavioral programs control the selection of visual information. ?? 2005 Elsevier Ltd. All rights reserved.},
	number = {4},
	journal = {Trends in Cognitive Sciences},
	author = {Hayhoe, Mary and Ballard, Dana},
	year = {2005},
	pmid = {15808501},
	note = {ISBN: 1364-6613},
	pages = {188--194},
}

@article{rizzolatti_parietal_1997,
	title = {Parietal cortex: {From} sight to action},
	volume = {7},
	issn = {09594388},
	doi = {10.1016/S0959-4388(97)80037-2},
	abstract = {Recent findings have altered radically our thinking about the functional role of the parietal cortex. According to this view, the parietal lobe consists of a multiplicity of areas with specific connections to the frontal lobe. These areas, together with the frontal areas to which they are connected, mediate distinct sensorimotor transformations related to the control of hand, arm, eye or head movements. Space perception is not unitary, but derives from the joint activity of the fronto-parietal circuits that control actions requiring space computation.},
	number = {4},
	journal = {Current Opinion in Neurobiology},
	author = {Rizzolatti, Giacomo and Fogassi, Leonardo and Gallese, Vittorio},
	year = {1997},
	pmid = {9287198},
	note = {ISBN: 0959-4388},
	pages = {562--567},
}

@article{jiang_hallucinating_2012,
	title = {Hallucinating {Humans} for {Learning} {Robotic} {Placement} of {Objects}},
	doi = {10.1007/978-3-319-00065-7_61},
	abstract = {While a significant body of work has been done on grasping objects, there is little prior work on placing and arranging objects in the environment. In this work, we consider placing multiple objects in complex placing areas, where neither the object nor the placing area may have been seen by the robot before. Specifically, the placements should not only be stable, but should also follow hu- man usage preferences. We present learning and inference algorithms that con- sider these aspects in placing. In detail, given a set of 3D scenes containing ob- jects, our method, based on Dirichlet process mixture models, samples human poses in each scene and learns how objects relate to those human poses. Then given a new room, our algorithm is able to select meaningful human poses and use them to determine where to place new objects.We evaluate our approach on a variety of scenes in simulation, as well as on robotic experiments.},
	journal = {International Symposium on Experimental Robotics (ISER)},
	author = {Jiang, Yun and Saxena, Ashutosh},
	year = {2012},
	keywords = {Human, Object, Object Placement},
	pages = {15},
}

@article{corbetta_motor_2000,
	title = {Motor constraints on the development of perception-action matching in infant reaching},
	volume = {23},
	issn = {01636383},
	doi = {10.1016/S0163-6383(01)00049-2},
	abstract = {Previous studies on reaching and grasping have suggested that infants need considerable experience at both seeing and touching in order to develop responses adapted to the environment. Such an account, however, does not reveal how appropriate perception-action matching emerges from these repeated experiences at seeing and touching. The present research addresses this issue by investigating the dynamics of perceiving and acting in 5- to 9-month-old infants as they saw, reached for, touched, and grasped objects of different sizes and texture. To gain insights into the mechanisms of change that underlie pattern formation, we observed infants’ responses as a function of time, as infants reached for and manipulated objects successively. We found that the developmental process by which appropriate perception-action matching emerges is tied to important changes in the motor system. Before 8 months, infants’ reaching responses are constrained by systemic motor tendencies that conflict with the process of perceptual-motor mapping. When these motor tendencies disappear, infants are able to use and integrate visual and haptic information to scale their actions to objects. These results are consistent with a dynamic systems approach, which views behavioral changes and their underlying psychological processes as the product of continuous tensions and interactions between the organism’s own constraints and the characteristics of the task at hand. © 2000 Elsevier Science Inc. All rights reserved.},
	number = {3-4},
	journal = {Infant Behavior and Development},
	author = {Corbetta, Daniela and Thelen, Esther and Johnson, Kimberly},
	year = {2000},
	note = {ISBN: 0163-6383},
	pages = {351--374},
}

@inproceedings{feth_performance_2009,
	title = {Performance related energy exchange in haptic human-human interaction in a shared virtual object manipulation task},
	isbn = {978-1-4244-3858-7},
	doi = {10.1109/WHC.2009.4810854},
	abstract = {In order to enable intuitive physical interaction with autonomous robots as well as in collaborative multi-user virtual reality and tele-operation systems a deep understanding of human-human haptic interaction is required. In this paper the effect of haptic interaction in single and dyadic conditions is investigated. Furthermore, an energy-based framework suitable for the analysis of the underlying processes is introduced. A pursuit tracking task experiment is performed where a virtual object is manipulated, jointly by two humans and alone. The performance in terms of the root-mean-square tracking error is improved in dyadic compared to individual conditions, even though the virtual object mass is reduced to one half in the latter. Our results indicate that the interacting partners benefit from role distributions which can be associated with different energy flows.},
	booktitle = {Proceedings - 3rd {Joint} {EuroHaptics} {Conference} and {Symposium} on {Haptic} {Interfaces} for {Virtual} {Environment} and {Teleoperator} {Systems}, {World} {Haptics} 2009},
	author = {Feth, Daniela and Groten, Raphaela and Peer, Angelika and Hirche, Sandra and Buss, Martin},
	year = {2009},
	pages = {338--343},
}

@inproceedings{zhang_sliding_2014,
	title = {Sliding mode prediction control for {3D} path following of an underactuated {AUV}},
	volume = {19},
	isbn = {978-3-902823-62-5},
	doi = {10.0/Linux-x86_64},
	abstract = {Compared to the traditional 2D path following case, 3D path following is more complicated since the coupling effect between horizontal and vertical plane has to be taken into account. In this paper, a sliding mode prediction controller is presented, which drives an underactuated AUV to follow a desired 3D path under time-varying current disturbances. A sliding mode technique combined with the predictive control strategy is developed to compensate for the impact of the hydrodynamic damping coupling via the feedback correction and receding horizon optimization techniques. In addition, Different from the traditional reference orientation method for path following control, here a methodology named 3D virtual guidance orientation (VGO) is built to deal with the vehicle dynamics. The geometric characteristic of tracking curve is contained in the control design as an extra degree of freedom to drive the vehicle to track a virtual target along desired path. Hence, the proposed method can avoid saturation problem of driving devices (stern plane and rudder) of AUV. Numerical simulations illustrate the excellent path following performance of the AUV in 3D under the proposed control scheme.},
	booktitle = {{IFAC} {Proceedings} {Volumes} ({IFAC}-{PapersOnline})},
	author = {Zhang, Li Jun and Jia, He Ming and Jiang, Da Peng},
	year = {2014},
	pmid = {22255825},
	note = {arXiv: 1204.3968
ISSN: 14746670},
	keywords = {3D path following, Sliding mode predictive control, Underactuated AUV, Visual guidance},
	pages = {8799--8804},
}

@article{schuch_observing_2007,
	title = {On observing another person's actions: influences of observed inhibition and errors.},
	volume = {69},
	issn = {0031-5117},
	doi = {10.3758/BF03193782},
	abstract = {It was investigated whether an observer would simulate another person's inhibitory and error processes. Two participants sitting next to each other performed a stop signal task in which they occasionally had to try and inhibit their response when indicated to do so by a stop signal. They could either successfully stop the response or fail to stop and, thereby, make an error. An aftereffect of the other person's successful action inhibition and error was obtained: The participants became slower and more accurate when they observed the other person make an error on the previous trial and when they observed a successful stop. The results suggest that observing another person successfully inhibit an action or make an error evokes processes similar to those that occur when these behaviors are produced.},
	number = {5},
	journal = {Perception \& psychophysics},
	author = {Schuch, Stefanie and Tipper, Steven P},
	year = {2007},
	pmid = {17929703},
	note = {ISBN: 1943-3921},
	keywords = {Human, Human-Human Interaction (HHI), Observation},
	pages = {828--837},
}

@article{craighero_temporal_2008,
	title = {Temporal prediction of touch instant during observation of human and robot grasping},
	volume = {75},
	issn = {03619230},
	doi = {10.1016/j.brainresbull.2008.01.014},
	abstract = {The aim of the present work was to test the ability to predict the instant at which a grasping hand touches an object. Our hypothesis was that, because of the activation of the mirror-neuron system, the same predictive process necessary for action execution should be active during observation. Experimental evidence indicates, however, that not only observed actions but also observed objects automatically activate observer's motor repertoire. What happens, therefore, if the observed action is different from the one automatically evoked by the vision of the object? To answer this question we presented subjects with two different grasping actions: the one most suitable for the presented object and a less appropriate one. Subjects were required to detect the instant at which the demonstrator's hand touched the object. In a further condition, subjects were required to detect the outcome of an action performed by a robotic arm moving with constant kinematics. Results showed that while in the case of robot grasping subjects responded before the touch instant, in the case of human grasping the response followed the touch instant, but occurred much earlier than simple reaction times. This demonstrates that subjects were able to predict the outcome of the seen action. The predictive capability was specifically enhanced during observation of the "suitable" grasping. We interpret these results as an indication of the synergic contribution of both object-related (canonical) and action-related (mirror) neurons during observation of actions directed towards graspable objects. ?? 2008 Elsevier Inc. All rights reserved.},
	number = {6},
	journal = {Brain Research Bulletin},
	author = {Craighero, Laila and Bonetti, Francesco and Massarenti, Luca and Canto, Rosario and Fabbri Destro, Maddalena and Fadiga, Luciano and Fogassi, L and Pavesi, G and Rizzolatti, G and Craighero, Laila and Bonetti, Francesco and Massarenti, Luca and Canto, Rosario and Fabbri Destro, Maddalena and Fadiga, Luciano},
	year = {2008},
	pmid = {18394523},
	note = {ISBN: 0361-9230 (Print){\textbackslash}r0361-9230 (Linking)},
	keywords = {Action observation, Action prediction, Canonical neurons, Mirror system, Monocular and binocular vision, Motor resonance},
	pages = {770--774},
}

@article{land_roles_1999,
	title = {The roles of vision and eye movements in the control of activities of daily living},
	volume = {28},
	issn = {03010066},
	doi = {10.1068/p2935},
	abstract = {The aim of this study was to determine the pattern of fixations during the performance of a well-learned task in a natural setting (making tea), and to classify the types of monitoring action that the eyes perform. We used a head-mounted eye-movement video camera, which provided a continuous view of the scene ahead, with a dot indicating foveal direction with an accuracy of about 1 deg. A second video camera recorded the subject's activities from across the room. The videos were linked and analysed frame by frame. Foveal direction was always close to the object being manipulated, and very few fixations were irrelevant to the task. The first object-related fixation typically led the first indication of manipulation by 0.56 s, and vision moved to the next object about 0.61 s before manipulation of the previous object was complete. Each object-related act that did not involve a waiting period lasted an average of 3.3 s and involved about 7 fixations. Roughly a third of all fixations on objects could be definitely identified with one of four monitoring functions: locating objects used later in the process, directing the hand or object in the hand to a new location, guiding the approach of one object to another (e.g. kettle and lid), and checking the state of some variable (e.g. water level). We conclude that although the actions of tea-making are 'automated' and proceed with little conscious involvement, the eyes closely monitor every step of the process. This type of unconscious attention must be a common phenomenon in everyday life.},
	number = {11},
	journal = {Perception},
	author = {Land, Michael and Mennie, Neil and Rusted, Jennifer},
	year = {1999},
	pmid = {10755142},
	note = {ISBN: 0301-0066},
	pages = {1311--1328},
}

@article{tucker_potentiation_2001,
	title = {The potentiation of grasp types during visual object categorization},
	volume = {8},
	issn = {1350-6285},
	url = {wos:000172392900004%5Cnhttp://www.tandfonline.com/doi/abs/10.1080/13506280042000144},
	doi = {10.1080/13506280042000144},
	abstract = {The close integration between visual and motor processes suggests that some visuomotor transformations may proceed automatically and to an extent that permits observable effects on subsequent actions. A series of experiments investigated the effects of visual objects on motor responses during a categorisation task. In Experiment 1 participants responded according to an object's natural or manufactured category. The responses consisted in uni-manual precision or power grasps that could be compatible or incompatible with the viewed object. The data indicate that object grasp compatibility significantly affected participant response times and that this did not depend upon the object being viewed within the reaching space. The time course of this effect was investigated in Experiments 2-4b by using a go-nogo paradigm with responses cued by tones and go-nogo trials cued by object category. The compatibility effect was not present under advance response cueing and rapidly diminished following object extinction. A final experiment established that the compatibility effect did not depend on a within-hand response choice, but was at least as great with bi-manual responses where a full power grasp could be used. Distributional analyses suggest that the effect is not subject to rapid decay but increases linearly with RT whilst the object remains visible. The data are consistent with the view that components of the actions an object affords are integral to its representation.},
	number = {6},
	journal = {Visual Cognition},
	author = {Tucker, Mike and Ellis, Rob},
	year = {2001},
	pmid = {1153},
	note = {arXiv: 1011.1669v3
ISBN: 1350-6285},
	keywords = {EXTINCTION, OBJECTS, REPRESENTATION, SPACE},
	pages = {769--800},
}

@article{grafton_localization_1996,
	title = {Localization of grasp representations in humans by positron emission tomography},
	volume = {112},
	issn = {0014-4819, 1432-1106},
	url = {http://link.springer.com/article/10.1007/BF00227183},
	doi = {10.1007/BF00227183},
	abstract = {Positron emission tomography imaging of cerebral blood flow was used to localize brain areas involved in the representation of hand grasping movements. Seven normal subjects were scanned under three conditions. In the first, they observed precision grasping of common objects performed by the examiner. In the second, they imagined themselves grasping the objects without actually moving the hand. These two tasks were compared with a control task of object viewing. Grasp observation activated the left rostral superior temporal sulcus, left inferior frontal cortex (area 45), left rostral inferior parietal cortex (area 40), the rostral part of left supplementary motor area (SMA-proper), and the right dorsal premotor cortex. Imagined grasping activated the left inferior frontal (area 44) and middle frontal cortex, left caudal inferior parietal cortex (area 40), a more extensive response in left rostral SMA-proper, and left dorsal premotor cortex. The two conditions activated different areas of the right posterior cerebellar cortex. We propose that the areas active during grasping observation may form a circuit for recognition of hand-object interactions, whereas the areas active during imagined grasping may be a putative human homologue of a circuit for hand grasping movements recently defined in nonhuman primates. The location of responses in SMA-proper confirms the rostrocaudal segregation of this area for imagined and real movement. A similar segregation is also present in the cerebellum, with imagined and observed grasping movements activating different parts of the posterior lobe and real movements activating the anterior lobe.},
	number = {1},
	journal = {Experimental Brain Research},
	author = {Grafton, Scott T. and Arbib, Michael A. and Fadiga, Luciano and Rizzolatti, Giacomo},
	year = {1996},
	pmid = {8951412},
	note = {Publisher: Elsevier Ltd
ISBN: 4047273732},
	keywords = {cerebral blood flow 9, emission, grasp 9 imagined, motor control 9 positron, tomography},
	pages = {103--111},
}

@article{fadiga_human_2005,
	title = {Human motor cortex excitability during the perception of others' action},
	volume = {15},
	issn = {09594388},
	doi = {10.1016/j.conb.2005.03.013},
	abstract = {Neuroscience research during the past ten years has fundamentally changed the traditional view of the motor system. In monkeys, the finding that premotor neurons also discharge during visual stimulation (visuomotor neurons) raises new hypotheses about the putative role played by motor representations in perceptual functions. Among visuomotor neurons, mirror neurons might be involved in understanding the actions of others and might, therefore, be crucial in interindividual communication. Functional brain imaging studies enabled us to localize the human mirror system, but the demonstration that the motor cortex dynamically replicates the observed actions, as if they were executed by the observer, can only be given by fast and focal measurements of cortical activity. Transcranial magnetic stimulation enables us to instantaneously estimate corticospinal excitability, and has been used to study the human mirror system at work during the perception of actions performed by other individuals. In the past ten years several TMS experiments have been performed investigating the involvement of motor system during others' action observation. Results suggest that when we observe another individual acting we strongly 'resonate' with his or her action. In other words, our motor system simulates underthreshold the observed action in a strictly congruent fashion. The involved muscles are the same as those used in the observed action and their activation is temporally strictly coupled with the dynamics of the observed action. ?? 2005 Elsevier Ltd. All rights reserved.},
	number = {2},
	journal = {Current Opinion in Neurobiology},
	author = {Fadiga, Luciano and Craighero, Laila and Olivier, Etienne},
	year = {2005},
	pmid = {15831405},
	note = {ISBN: 0959-4388},
	pages = {213--218},
}

@article{staudte_investigating_2011,
	title = {Investigating joint attention mechanisms through spoken human-robot interaction},
	volume = {120},
	issn = {00100277},
	url = {http://dx.doi.org/10.1016/j.cognition.2011.05.005},
	doi = {10.1016/j.cognition.2011.05.005},
	abstract = {Referential gaze during situated language production and comprehension is tightly coupled with the unfolding speech stream (Griffin, 2001; Meyer, Sleiderink, \& Levelt, 1998; Tanenhaus, Spivey-Knowlton, Eberhard, \& Sedivy, 1995). In a shared environment, utterance comprehension may further be facilitated when the listener can exploit the speaker's focus of (visual) attention to anticipate, ground, and disambiguate spoken references. To investigate the dynamics of such gaze-following and its influence on utterance comprehension in a controlled manner, we use a human-robot interaction setting. Specifically, we hypothesize that referential gaze is interpreted as a cue to the speaker's referential intentions which facilitates or disrupts reference resolution. Moreover, the use of a dynamic and yet extremely controlled gaze cue enables us to shed light on the simultaneous and incremental integration of the unfolding speech and gaze movement.We report evidence from two eye-tracking experiments in which participants saw videos of a robot looking at and describing objects in a scene. The results reveal a quantified benefit-disruption spectrum of gaze on utterance comprehension and, further, show that gaze is used, even during the initial movement phase, to restrict the spatial domain of potential referents. These findings more broadly suggest that people treat artificial agents similar to human agents and, thus, validate such a setting for further explorations of joint attention mechanisms. © 2011 Elsevier B.V.},
	number = {2},
	journal = {Cognition},
	author = {Staudte, Maria and Crocker, Matthew W.},
	year = {2011},
	pmid = {21665198},
	note = {Publisher: Elsevier B.V.
ISBN: 0010-0277},
	keywords = {Gaze, Human-robot interaction, Joint attention, Reference resolution, Referential gaze, Referential intention, Situated language processing, Utterance comprehension},
	pages = {268--291},
}

@article{aglioti_action_2008,
	title = {Action anticipation and motor resonance in elite basketball players.},
	volume = {11},
	issn = {1097-6256},
	doi = {10.1038/nn.2182},
	abstract = {We combined psychophysical and transcranial magnetic stimulation studies to investigate the dynamics of action anticipation and its underlying neural correlates in professional basketball players. Athletes predicted the success of free shots at a basket earlier and more accurately than did individuals with comparable visual experience (coaches or sports journalists) and novices. Moreover, performance between athletes and the other groups differed before the ball was seen to leave the model's hands, suggesting that athletes predicted the basket shot's fate by reading the body kinematics. Both visuo-motor and visual experts showed a selective increase of motor-evoked potentials during observation of basket shots. However, only athletes showed a time-specific motor activation during observation of erroneous basket throws. Results suggest that achieving excellence in sports may be related to the fine-tuning of specific anticipatory 'resonance' mechanisms that endow elite athletes' brains with the ability to predict others' actions ahead of their realization.},
	number = {9},
	journal = {Nature neuroscience},
	author = {Aglioti, Salvatore M and Cesari, Paola and Romani, Michela and Urgesi, Cosimo},
	year = {2008},
	pmid = {19160510},
	note = {ISBN: 1097-6256, 1097-6256},
	pages = {1109--1116},
}

@article{metta_understanding_2006,
	title = {Understanding mirror neurons: {A} bio-robotic approach},
	volume = {7},
	issn = {15720373},
	doi = {10.1075/is.7.2.06met},
	abstract = {This paper reports about our investigation on action understanding in the brain. We review recent results of the neurophysiology of the mirror system in the monkey. Based on these observations we propose a model of this brain system which is responsible for action recognition. The link between object affordances and action understanding is considered. To support our hypothesis we describe two experiments where some aspects of the model have been implemented. In the first experiment an action recognition system is trained by using data recorded from human movements. In the second experiment, the model is partially implemented on a humanoid robot which learns to mimic simple actions performed by a human subject on different objects. These experiments show that motor information can have a significant role in action interpretation and that a mirror-like representation can be developed autonomously as a result of the interaction between an individual and the environment.},
	number = {2},
	journal = {Interaction Studies},
	author = {Metta, Giorgio and Sandini, Giulio and Natale, Lorenzo and Craighero, Laila and Fadiga, Luciano},
	year = {2006},
	pmid = {-2275093745733071400},
	note = {ISBN: 1572-0373},
	keywords = {action recognition, mirror neurons, neurophysiology, robotics},
	pages = {197--232},
}

@article{castiello_neuroscience_2005,
	title = {The neuroscience of grasping.},
	volume = {6},
	issn = {1471-003X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16100518},
	doi = {10.1038/nrn1744},
	abstract = {People have always been fascinated by the exquisite precision and flexibility of the human hand. When hand meets object, we confront the overlapping worlds of sensorimotor and cognitive functions. We reach for objects, grasp and lift them, manipulate them and use them to act on other objects. This review examines one of these actions--grasping. Recent research in behavioural neuroscience, neuroimaging and electrophysiology has the potential to reveal where in the brain the process of grasping is organized, but has yet to address several questions about the sensorimotor transformations that relate to the control of the hands.},
	number = {9},
	journal = {Nature reviews. Neuroscience},
	author = {Castiello, Umberto},
	year = {2005},
	pmid = {16100518},
	note = {ISBN: 1471-003X (Print){\textbackslash}r1471-003X (Linking)},
	keywords = {Animals, Biomechanical Phenomena, Brain, Brain: physiology, Brain: radionuclide imaging, Hand, Hand Strength, Hand Strength: physiology, Hand: physiology, Humans, Magnetic Resonance Imaging, Neuropsychology, Neuropsychology: methods, Neurosciences, Neurosciences: methods, Positron-Emission Tomography},
	pages = {726--36},
}

@article{wang_probabilistic_2013,
	title = {Probabilistic movement modeling for intention inference in human–robot interaction},
	volume = {32},
	issn = {0278-3649, 1741-3176},
	doi = {10.1177/0278364913478447},
	abstract = {Intention inference can be an essential step toward efficient human–robot interaction. For this purpose, we propose the Intention-Driven Dynamics Model (IDDM) to probabilistically model the generative process of movements that are directed by the intention. The IDDM allows the intention to be inferred from observed movements using Bayes’ theorem. The IDDM simultaneously finds a latent state representation of noisy and high-dimensional observations, and models the intention-driven dynamics in the latent states. As most robotics applications are subject to real-time constraints, we develop an efficient online algorithm that allows for real-time intention inference. Two human–robot interaction scenarios, i.e. target prediction for robot table tennis and action recognition for interactive humanoid robots, are used to evaluate the performance of our inference algorithm. In both intention inference tasks, the proposed algorithm achieves substantial improvements over support vector machines and Gaussian processes.},
	number = {7},
	journal = {The International Journal of Robotics Research},
	author = {Wang, Zhikun and Mülling, Katharina and Deisenroth, Marc Peter and Amor, Heni Ben and Vogt, David and Schölkopf, Bernhard and Peters, Jan},
	year = {2013},
	note = {ISBN: 978-0-262-19475-4},
	keywords = {a pproximate inference, gaussian process, intention inference},
	pages = {841--858},
}

@article{jung_sharing_2011,
	title = {Sharing a bimanual task between two: {Evidence} of temporal alignment in interpersonal coordination},
	volume = {211},
	issn = {00144819},
	doi = {10.1007/s00221-011-2665-4},
	abstract = {In recent years, researchers have made many new discoveries in the field of social interaction and have attempted to understand the mechanisms of interpersonal coordination. This research is marked by two streams: On the one hand, there are attempts to explain spontaneous, incidental interpersonal coordination in terms of the behavioral dynamics perspective, and on the other, to explain instructed, intentional interpersonal coordination in terms of joint action. Other paradigms fall somewhere between incidental and intentional coordination, e.g. task sharing paradigms. The present study has two major objectives. First, we wanted to explore to what extent a dyadic scenario for bimanual coordination mimics typical signatures of bimanual coordination performance as obtained in the classical individual scenario. Second, if such mimicking is obtained, we wanted to investigate the kind of information on which the coordination between the two individuals may be grounded. To do so, we used a bimanual aiming task, which enabled us to assess measurements of two levels of coordination: global (operating over longer periods of time) and local (operating on each particular trial). In Experiment 1, this task was performed in an individual and in a dyadic setting. In the dyadic scenario, we observed strong global coordination and weak local coordination. In Experiment 2, we replicated this pattern and showed that different kinds of feedback had no impact on interpersonal coordination. Based on these findings, we propose that interpersonal coordination in a non-rhythmic choice response task is based on weak interpersonal coordination.},
	number = {3-4},
	journal = {Experimental Brain Research},
	author = {Jung, Christina and Holländer, Antje and Müller, Karsten and Prinz, Wolfgang},
	year = {2011},
	pmid = {21503651},
	note = {ISBN: 00144819},
	keywords = {Anticipation, Bimanual coordination, Interpersonal coordination, ★},
	pages = {471--482},
}

@article{Moreau2016,
	title = {Dynamics of {Social} {Interaction}: {Kinematic} {Analysis} of a {Joint} {Action}},
	volume = {7},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.02016/full},
	doi = {10.3389/fpsyg.2016.02016},
	number = {December},
	journal = {Frontiers in Psychology},
	author = {Moreau, Quentin and Galvan, Lucie and Nazir, Tatjana A. and Paulignan, Yves},
	year = {2016},
	keywords = {joint action, joint action, kinematics and dynamics, social inte, kinematics and dynamics, motor system, reach-to-grasp, social interactions},
	pages = {1--8},
}

@article{chinellato_visual_2008,
	title = {Visual neuroscience of robotic grasping},
	issn = {{\textless}null{\textgreater}},
	number = {June},
	author = {Chinellato, E},
	year = {2008},
}

@article{bisio_motor_2014,
	title = {Motor contagion during human-human and human-robot interaction},
	volume = {9},
	issn = {19326203},
	doi = {10.1371/journal.pone.0106172},
	abstract = {Motor resonance mechanisms are known to affect humans' ability to interact with others, yielding the kind of "mutual understanding" that is the basis of social interaction. However, it remains unclear how the partner's action features combine or compete to promote or prevent motor resonance during interaction. To clarify this point, the present study tested whether and how the nature of the visual stimulus and the properties of the observed actions influence observer's motor response, being motor contagion one of the behavioral manifestations of motor resonance. Participants observed a humanoid robot and a human agent move their hands into a pre-specified final position or put an object into a container at various velocities. Their movements, both in the object- and non-object- directed conditions, were characterized by either a smooth/curvilinear or a jerky/segmented trajectory. These trajectories were covered with biological or non-biological kinematics (the latter only by the humanoid robot). After action observation, participants were requested to either reach the indicated final position or to transport a similar object into another container. Results showed that motor contagion appeared for both the interactive partner except when the humanoid robot violated the biological laws of motion. These findings suggest that the observer may transiently match his/her own motor repertoire to that of the observed agent. This matching might mediate the activation of motor resonance, and modulate the spontaneity and the pleasantness of the interaction, whatever the nature of the communication partner.},
	number = {8},
	journal = {PLoS ONE},
	author = {Bisio, Ambra and Sciutti, Alessandra and Nori, Francesco and Metta, Giorgio and Fadiga, Luciano and Sandini, Giulio and Pozzo, Thierry},
	year = {2014},
	pmid = {25153990},
}

@article{pezzulo_human_2013,
	title = {Human sensorimotor communication: {A} theory of signaling in online social interactions},
	volume = {8},
	issn = {19326203},
	doi = {10.1371/journal.pone.0079876},
	abstract = {Although the importance of communication is recognized in several disciplines, it is rarely studied in the context of online social interactions and joint actions. During online joint actions, language and gesture are often insufficient and humans typically use non-verbal, sensorimotor forms of communication to send coordination signals. For example, when playing volleyball, an athlete can exaggerate her movements to signal her intentions to her teammates (say, a pass to the right) or to feint an adversary. Similarly, a person who is transporting a table together with a co-actor can push the table in a certain direction to signal where and when he intends to place it. Other examples of "signaling" are over-articulating in noisy environments and over-emphasizing vowels in child-directed speech. In all these examples, humans intentionally modify their action kinematics to make their goals easier to disambiguate. At the moment no formal theory exists of these forms of sensorimotor communication and signaling. We present one such theory that describes signaling as a combination of a pragmatic and a communicative action, and explains how it simplifies coordination in online social interactions. We cast signaling within a "joint action optimization" framework in which co-actors optimize the success of their interaction and joint goals rather than only their part of the joint action. The decision of whether and how much to signal requires solving a trade-off between the costs of modifying one's behavior and the benefits in terms of interaction success. Signaling is thus an intentional strategy that supports social interactions; it acts in concert with automatic mechanisms of resonance, prediction, and imitation, especially when the context makes actions and intentions ambiguous and difficult to read. Our theory suggests that communication dynamics should be studied within theories of coordination and interaction rather than only in terms of the maximization of information transmission.},
	number = {11},
	journal = {PLoS ONE},
	author = {Pezzulo, Giovanni and Donnarumma, Francesco and Dindo, Haris},
	year = {2013},
	pmid = {24278201},
	note = {ISBN: 10.1371/journal.pone.0079876},
	keywords = {Bayesian Network, Human Activity, Motion},
}

@article{lewis_long_2013,
	title = {The long reach of the gene},
	volume = {26},
	issn = {09528229},
	doi = {10.1162/jocn},
	abstract = {The negotiation of social order is intimately connected to the capacity to infer and track status relationships. Despite the foundational role of status in social cognition, we know little about how the brain constructs status from social interactions that display it. Although emerging cognitive neuroscience reveals that status judgments depend on the intraparietal sulcus, a brain region that supports the comparison of targets along a quantitative continuum, we present evidence that status judgments do not necessarily reduce to ranking targets along a quantitative continuum. The process of judging status also fits a social interdependence analysis. Consistent with third-party perceivers judging status by inferring whose goals are dictating the terms of the interaction and who is subordinating their desires to whom, status judgments were associated with increased recruitment of medial pFC and STS, brain regions implicated in mental state inference},
	number = {3},
	journal = {Psychologist},
	author = {Lewis, Gary J. and Bates, Timothy C.},
	year = {2013},
	pmid = {23647519},
	note = {arXiv: 1511.04103
ISBN: 9780192880512},
	pages = {194--198},
}

@article{huber_spatiotemporal_2013,
	title = {Spatiotemporal {Movement} {Planning} and {Rapid} {Adaptation} for {Manual} {Interaction}},
	volume = {8},
	issn = {19326203},
	doi = {10.1371/journal.pone.0064982},
	abstract = {Many everyday tasks require the ability of two or more individuals to coordinate their actions with others to increase efficiency. Such an increase in efficiency can often be observed even after only very few trials. Previous work suggests that such behavioral adaptation can be explained within a probabilistic framework that integrates sensory input and prior experience. Even though higher cognitive abilities such as intention recognition have been described as probabilistic estimation depending on an internal model of the other agent, it is not clear whether much simpler daily interaction is consistent with a probabilistic framework. Here, we investigate whether the mechanisms underlying efficient coordination during manual interactions can be understood as probabilistic optimization. For this purpose we studied in several experiments a simple manual handover task concentrating on the action of the receiver. We found that the duration until the receiver reacts to the handover decreases over trials, but strongly depends on the position of the handover. We then replaced the human deliverer by different types of robots to further investigate the influence of the delivering movement on the reaction of the receiver. Durations were found to depend on movement kinematics and the robot's joint configuration. Modeling the task was based on the assumption that the receiver's decision to act is based on the accumulated evidence for a specific handover position. The evidence for this handover position is collected from observing the hand movement of the deliverer over time and, if appropriate, by integrating this sensory likelihood with prior expectation that is updated over trials. The close match of model simulations and experimental results shows that the efficiency of handover coordination can be explained by an adaptive probabilistic fusion of a-priori expectation and online estimation.},
	number = {5},
	journal = {PLoS ONE},
	author = {Huber, Markus and Kupferberg, Aleksandra and Lenz, Claus and Knoll, Alois and Brandt, Thomas and Glasauer, Stefan},
	year = {2013},
	pmid = {23724112},
	keywords = {Handovers, Human Activity, Planning},
}

@article{oztop_chapter_2015,
	title = {Chapter 2 {Humanoid} {Brain} {Science}},
	author = {Oztop, Erhan and Ugur, Emre and Shimizu, Yu and Imamizu, Hiroshi},
	year = {2015},
	pages = {1--19},
}

@article{paulignan_influence_1997,
	title = {Influence of object position and size on human prehension movements.},
	volume = {114},
	issn = {0014-4819},
	doi = {10.1007/PL00005631},
	abstract = {Prehension movements of the right hand were recorded in normal subjects using a computerized motion analyzer. The kinematics and the spatial paths of markers placed at the wrist and at the tips of the index finger and thumb were measured. Cylindrical objects of different diameters (3, 6, 9 cm) were used as targets. They were placed at six different positions in the workspace along a circle centered on subject's head axis. The positions were spaced by 10 degrees starting from 10 degrees on the left of the sagittal axis, up to 40 degrees on the right. Both the transport and the grasp components of prehension were influenced by the distance between the resting hand position and the object position. Movement time, time to peak velocity of the wrist and time to maximum grip aperture varied as a function of distance from the object, irrespective of its size. The variability of the spatial paths of wrist and fingers sharply decreased during the phase of the movement prior to contact with the object. This indicates that the final position of the thumb and the index finger is a controlled parameter of visuomotor transformation during prehension. The orientation of the opposition axis (defined as the line connecting the tips of the thumb and the index finger at the end of the movement) was measured. Several different frames of reference were used. When an object-centered frame was used, the orientation of the opposition axis was found to change by about 10 degrees from one object position to the next. By contrast, when a body-centered frame was used (with the head or the forearm as a reference), this orientation was found to remain relatively invariant for different object positions and sizes. The degree of wrist flexion was little affected by the position of the object. This result, together with the invariant orientation of the opposition axis, shows that prehension movements aimed at cylindrical objects are organized so as to minimize changes in posture of the lower arm.},
	number = {2},
	journal = {Experimental brain research},
	author = {Paulignan, Y and Frak, V G and Toni, I and Jeannerod, M},
	year = {1997},
	pmid = {9166912},
	note = {ISBN: 0014-4819 (Print){\textbackslash}r0014-4819 (Linking)},
	keywords = {Arm, Arm: physiology, Female, Fingers, Grasp, Hand Strength, Hand Strength: physiology, Human Activity, Human Experiment, Human Grasp, Humans, Male, Motion, Motor Activity, Movement, Movement: physiology, Posture, Psychomotor Performance, Space Perception, Thumb, Wrist Joint},
	pages = {226--34},
}

@article{oztop_mental_2005,
	title = {Mental state inference using visual control parameters},
	volume = {22},
	issn = {09266410},
	doi = {10.1016/j.cogbrainres.2004.08.004},
	abstract = {Although we can often infer the mental states of others by observing their actions, there are currently no computational models of this remarkable ability. Here we develop a computational model of mental state inference that builds upon a generic visuomanual feedback controller, and implements mental simulation and mental state inference functions using circuitry that subserves sensorimotor control. Our goal is (1) to show that control mechanisms developed for manual manipulation are readily endowed with visual and predictive processing capabilities and thus allows a natural extension to the understanding of movements performed by others; and (2) to give an explanation on how cortical regions, in particular the parietal and premotor cortices, may be involved in such dual mechanism. To analyze the model, we simulate tasks in which an observer watches an actor performing either a reaching or a grasping movement. The observer's goal is to estimate the 'mental state' of the actor: the goal of the reaching movement or the intention of the agent performing the grasping movement. We show that the motor modules of the observer can be used in a 'simulation mode' to infer the mental state of the actor. The simulations with different grasping and non-straight line reaching strategies show that the mental state inference model is applicable to complex movements. Moreover, we simulate deceptive reaching, where an actor imposes false beliefs about his own mental state on an observer. The simulations show that computational elements developed for sensorimotor control are effective in inferring the mental states of others. The parallels between the model and cortical organization of movement suggest that primates might have developed a similar resource utilization strategy for action understanding, and thus lead to testable predictions about the brain mechanisms of mental state inference. © 2004 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Cognitive Brain Research},
	author = {Oztop, Erhan and Wolpert, Daniel and Kawato, Mitsuo},
	year = {2005},
	pmid = {15653289},
	note = {ISBN: 09266410},
	keywords = {Cortex, Grasping, Mental simulation, Reaching, Theory of mind},
	pages = {129--151},
}

@article{lenz_deep_2015,
	title = {Deep learning for detecting robotic grasps},
	volume = {34},
	issn = {0278-3649},
	url = {http://ijr.sagepub.com/content/34/4-5/705.short},
	doi = {10.1177/0278364914549607},
	abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
	number = {4-5},
	journal = {The International Journal of Robotics Research},
	author = {Lenz, I. and Lee, H. and Saxena, A.},
	year = {2015},
	pmid = {19352402},
	note = {arXiv: 1301.3592v5
ISBN: 9789810739379},
	keywords = {3d feature learning, baxter, deep learning, pr2, rgb-d multi-modal data, robotic grasping},
	pages = {705--724},
}

@article{aggarwal_human_2011,
	title = {Human activity analysis: {A} review},
	volume = {43},
	issn = {03600300},
	url = {http://dl.acm.org/citation.cfm?id=1922653},
	doi = {10.1145/1922649.1922653},
	abstract = {Human activity recognition is an important area of computer vision research. Its applications include surveillance systems, patient monitoring systems, and a variety of systems that involve interactions between persons and electronic devices such as human-computer interfaces. Most of these applications require an automated recognition of high-level activities, composed of multiple simple (or atomic) actions of persons. This article provides a detailed overview of various state-of-the-art research papers on human activity recognition. We discuss both the methodologies developed for simple human actions and those for high-level activities. An approach-based taxonomy is chosen that compares the advantages and limitations of each approach. Recognition methodologies for an analysis of the simple actions of a single person are first presented in the article. Space-time volume approaches and sequential approaches that represent and recognize activities directly from input images are discussed. Next, hierarchical recognition methodologies for high-level activities are presented and compared. Statistical approaches, syntactic approaches, and description-based approaches for hierarchical recognition are discussed in the article. In addition, we further discuss the papers on the recognition of human-object interactions and group activities. Public datasets designed for the evaluation of the recognition methodologies are illustrated in our article as well, comparing the methodologies' performances. This review will provide the impetus for future research in more productive areas.},
	number = {3},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Aggarwal, Jk and Ryoo, Ms},
	year = {2011},
	pmid = {14979917},
	note = {ISBN: 2222222222222},
	keywords = {2, a review, acm computing surveys, aggarwal 1 and m, appear, human activity analysis, j, k, ryoo 1, s},
	pages = {16:1--16:43},
}

@article{urbaniak_nonverbal_nodate,
	title = {Nonverbal {Communication} in},
	author = {Urbaniak, Anthony},
	pages = {13--16},
}

@article{sciutti_investigating_2015,
	title = {Investigating the ability to read others' intentions using humanoid robots},
	volume = {6},
	issn = {16641078},
	doi = {10.3389/fpsyg.2015.01362},
	abstract = {The ability to interact with other people hinges crucially on the possibility to  anticipate how their actions would unfold. Recent evidence suggests that a similar skill may be grounded on the fact that we perform an action differently if different intentions lead it. Human observers can detect these differences and use them to predict the purpose leading the action. Although intention reading from movement observation is receiving a growing interest in research, the currently applied experimental paradigms have important limitations. Here, we describe a new approach to study intention understanding that takes advantage of robots, and especially of humanoid robots. We posit that this choice may overcome the drawbacks of previous methods, by guaranteeing the ideal trade-off between controllability and naturalness of the interactive scenario. Robots indeed can establish an interaction in a controlled manner, while sharing the same action space and exhibiting contingent behaviors. To conclude, we discuss the advantages of this research strategy and the aspects to be taken in consideration when attempting to define which human (and robot) motion features allow for intention reading during social interactive tasks.},
	journal = {Frontiers in Psychology},
	author = {Sciutti, Alessandra and Ansuini, Caterina and Becchio, Cristina and Sandini, Giulio and Loth, Sebastian and Forlizzi, Jodi and Sciutti, Alessandra and Ansuini, Caterina and Becchio, Cristina and Sandini, Giulio},
	year = {2015},
	pmid = {26441738},
	keywords = {Contingency, Human-robot interaction, Intention reading, Kinematics, Motor cognition, Second-person interaction, contingency, human–robot interaction, intention reading, kinematics, motor cognition, second-person interaction, ★},
}

@article{koppula_anticipating_2016,
	title = {Anticipating {Human} {Activities} {Using} {Object} {Affordances} for {Reactive} {Robotic} {Response}},
	issn = {01628828},
	doi = {10.1109/TPAMI.2015.2430335},
	abstract = {We represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Koppula, Hema S. and Saxena, Ashutosh},
	year = {2016},
	pmid = {26656575},
	note = {ISBN: 9781467363587},
	keywords = {3D Activity Understanding, Human Activity Anticipation, Human Experiment, Human-Robot Interaction(HRI), Machine Learning, RGBD Data, Robotics Perception, ★},
}

@article{dragan_generating_2013,
	title = {Generating {Legible} {Motion}},
	doi = {10.15607/RSS.2013.IX.024},
	abstract = {Legible motion — motion that communicates its intent to a human observer—is crucial for enabling seamless human-robot collaboration. In this paper, we propose a functional gradient optimization technique for autonomously generating legible motion. Our algorithm optimizes a legibility metric inspired by the psychology of action interpretation in humans, resulting in motion trajectories that purposefully deviate from what an observer would expect in order to better convey intent. A trust region constraint on the optimization ensures that the motion does not become too surprising or unpredictable to the observer. Our studies with novice users that evaluate the resulting trajectories support the applicability of our method and of such a trust region. They show that within the region, legibility as measured in practice does significantly increase. Outside of it, however, the trajectory becomes confusing and the users’ confidence in knowing the robot’s intent significantly decreases.},
	journal = {Proceedings of Robotics: Science and Systems Conference (RSS 2013)},
	author = {Dragan, Anca D. and Srinivasa, Siddhartha S.},
	year = {2013},
	note = {ISBN: 9789810739379},
	pages = {NP},
}

@article{dragan_legibility_2013,
	title = {Legibility and predictability of robot motion},
	volume = {1},
	issn = {21672148},
	doi = {10.1109/HRI.2013.6483603},
	abstract = {A key requirement for seamless human-robot collaboration is for the robot to make its intentions clear to its human collaborator. A collaborative robot's motion must be legible, or intent-expressive. Legibility is often described in the literature as and effect of predictable, unsurprising, or expected motion. Our central insight is that predictability and legibility are fundamentally different and often contradictory properties of motion. We develop a formalism to mathematically define and distinguish predictability and legibility of motion. We formalize the two based on inferences between trajectories and goals in opposing directions, drawing the analogy to action interpretation in psychology. We then propose mathematical models for these inferences based on optimizing cost, drawing the analogy to the principle of rational action. Our experiments validate our formalism's prediction that predictability and legibility can contradict, and provide support for our models. Our findings indicate that for robots to seamlessly collaborate with humans, they must change the way they plan their motion.},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	author = {Dragan, Anca D. and Lee, Kenton C T and Srinivasa, Siddhartha S.},
	year = {2013},
	note = {ISBN: 9781467330558},
	keywords = {action interpretation, formalism, human-robot collaboration, manipulation, motion planning, trajectory optimization},
	pages = {301--308},
}

@article{erlhagen_dynamic_2006,
	title = {The dynamic neural field approach to cognitive robotics.},
	volume = {3},
	issn = {1741-2560},
	doi = {10.1088/1741-2560/3/3/R02},
	abstract = {This tutorial presents an architecture for autonomous robots to generate behavior in joint action tasks. To efficiently interact with another agent in solving a mutual task, a robot should be endowed with cognitive skills such as memory, decision making, action understanding and prediction. The proposed architecture is strongly inspired by our current understanding of the processing principles and the neuronal circuitry underlying these functionalities in the primate brain. As a mathematical framework, we use a coupled system of dynamic neural fields, each representing the basic functionality of neuronal populations in different brain areas. It implements goal-directed behavior in joint action as a continuous process that builds on the interpretation of observed movements in terms of the partner's action goal. We validate the architecture in two experimental paradigms: (1) a joint search task; (2) a reproduction of an observed or inferred end state of a grasping-placing sequence. We also review some of the mathematical results about dynamic neural fields that are important for the implementation work.},
	journal = {Journal of neural engineering},
	author = {Erlhagen, Wolfram and Bicho, Estela},
	year = {2006},
	pmid = {16921201},
	note = {ISBN: 1741-2560 (Print){\textbackslash}n1741-2552 (Linking)},
	keywords = {Cognition, Dynamical Neural Field, ★},
	pages = {R36--R54},
}

@article{koppula_learning_2013-1,
	title = {Learning human activities and object affordances from {RGB}-{D} videos},
	volume = {32},
	issn = {0278-3649},
	url = {http://arxiv.org/abs/1210.1207%5Cnhttp://ijr.sagepub.com/cgi/content/long/32/8/951},
	doi = {10.1177/0278364913478446},
	abstract = {Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4\% for affordance, 63.4\% for sub-activity and 75.0\% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot.},
	journal = {The International Journal of Robotics Research},
	author = {Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
	year = {2013},
	pmid = {89023590},
	note = {arXiv: 1210.1207v2
ISBN: 0278-3649},
	keywords = {3d perception, Action understanding, Affordances, Human, Human Activity, Object, human activity detection, object affordance, personal robots, spatio-temporal context, supervised learning},
	pages = {951--970},
}

@inproceedings{dragan_learning_2017,
	title = {Learning from experience in manipulation planning: {Setting} the right goals},
	isbn = {978-3-319-29362-2},
	doi = {10.1007/978-3-319-29363-9_18},
	booktitle = {Springer {Tracts} in {Advanced} {Robotics}},
	author = {Dragan, Anca D. and Gordon, Geoffrey J. and Srinivasa, Siddhartha S.},
	year = {2017},
	note = {ISSN: 1610742X},
}

@article{Admoni,
	title = {Deliberate {Delays} {During} {Robot}-to-{Human} {Handovers} {Improve} {Compliance} {With} {Gaze} {Communication}},
	doi = {10.1145/2559636.2559682},
	abstract = {As assistive robots become popular in factories and homes, there is greater need for natural, multi-channel communi-cation during collaborative manipulation tasks. Non-verbal communication such as eye gaze can provide information without overloading more taxing channels like speech. How-ever, certain collaborative tasks may draw attention away from these subtle communication modalities. For instance, robot-to-human handovers are primarily manual tasks, and human attention is therefore drawn to robot hands rather than to robot faces during handovers. In this paper, we show that a simple manipulation of a robot's handover behav-ior can significantly increase both awareness of the robot's eye gaze and compliance with that gaze. When eye gaze communication occurs during the robot's release of an ob-ject, delaying object release until the gaze is finished draws attention back to the robot's head, which increases con-scious perception of the robot's communication. Further-more, the handover delay increases peoples' compliance with the robot's communication over a non-delayed handover, even when compliance results in counterintuitive behavior.},
	author = {Admoni, Henny and Dragan, Anca and Srinivasa, Siddhartha S and Scassellati, Brian},
}

@article{dragan_integrating_2014,
	title = {Integrating human observer inferences into robot motion planning},
	issn = {09295593},
	doi = {10.1007/s10514-014-9408-x},
	abstract = {Our goal is to enable robots to produce mo- tion that is suitable for human-robot collaboration and co- existence. Most motion in robotics is purely functional, ideal when the robot is performing a task in isolation. In collaboration, however, the robot’s motion has an observer, watching and interpreting the motion. In this work, we move beyond functional motion, and introduce the notion of an observer into motion planning, so that robots can generate motion that is mindful of how it will be interpreted by a human collaborator. We formalize predictability and legibility as properties of motion that naturally arise from the inferences in opposing directions that the observer makes, drawing on action in- terpretation theory in psychology. We propose models for these inferences based on the principle of rational action, and derive constrained functional trajectory optimization techniques for planning motion that is predictable or legible. Finally, we present experiments that test our work on novice users, and discuss the remaining challenges in en- abling robots to generate such motion online in complex situations.},
	journal = {Autonomous Robots},
	author = {Dragan, Anca and Srinivasa, Siddhartha},
	year = {2014},
	keywords = {Action interpretation, Human???robot collaboration, Legibility, Predictability, Trajectory optimization},
}

@article{stulp_facilitating_2015,
	title = {Facilitating intention prediction for humans by optimizing robot motions},
	volume = {2015-Decem},
	issn = {21530866},
	doi = {10.1109/IROS.2015.7353529},
	abstract = {Members of a team are able to coordinate their actions by anticipating the intentions of others. Achieving such implicit coordination between humans and robots requires humans to be able to quickly and robustly predict the robot’s intentions, i.e. the robot should demonstrate a behavior that is legible. Whereas previous work has sought to explicitly optimize the legibility of behavior, we investigate legibility as a property that arises automatically from general requirements on the efficiency and robustness of joint human-robot task completion. We do so by optimizing fast and successful completion of joint human-robot tasks through policy improvement with stochastic optimization. Two experiments with human subjects show that robots are able to adapt their behavior so that humans become better at predicting the robot’s intentions early on, which leads to faster and more robust overall task completion.},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	author = {Stulp, Freek and Grizou, Jonathan and Busch, Baptiste and Lopes, Manuel},
	year = {2015},
	note = {ISBN: 9781479999941},
	keywords = {Action understanding, Grasp, Human, Human-Robot Interaction(HRI), Prediction},
	pages = {1249--1255},
}

@article{newman-norlund_exploring_2007,
	title = {Exploring the brain basis of joint action: co-ordination of actions, goals and intentions.},
	volume = {2},
	issn = {1747-0919},
	url = {http://www.tandfonline.com/loi/psns20},
	doi = {10.1080/17470910701224623},
	abstract = {Humans are frequently confronted with goal-directed tasks that can not be accomplished alone, or that benefit from co-operation with other agents. The relatively new field of social cognitive neuroscience seeks to characterize functional neuroanatomical systems either specifically or preferentially engaged during such joint-action tasks. Based on neuroimaging experiments conducted on critical components of joint action, the current paper outlines the functional network upon which joint action is hypothesized to be dependant. This network includes brain areas likely to be involved in interpersonal co-ordination at the action, goal, and intentional levels. Experiments focusing specifically on joint-action situations similar to those encountered in real life are required to further specify this model.},
	number = {768418105},
	journal = {Social neuroscience},
	author = {Newman-Norlund, Roger D and Noordzij, Matthijs L and Meulenbroek, Ruud G J and Bekkering, Harold and Donders, F C},
	year = {2007},
	pmid = {18633806},
	note = {ISBN: 1747-0927 (Electronic){\textbackslash}n1747-0919 (Linking)},
	keywords = {Brain, Human-Human Interaction (HHI), Joint-action, ★},
	pages = {48--65},
}

@article{wenke_what_2011,
	title = {What is {Shared} in {Joint} {Action}? {Issues} of {Co}-representation, {Response} {Conflict}, and {Agent} {Identification}},
	issn = {18785158},
	doi = {10.1007/s13164-011-0057-0},
	abstract = {When sharing a task with another person that requires turn taking, as in doubles games of table tennis, performance on the shared task is similar to performing the whole task alone. This has been taken to indicate that humans co-represent their partner's task share, as if it were their own. Task co-representation allows prediction of the other's responses when it is the other's turn, and leads to response conflict in joint interference tasks. However, data from our lab cast doubt on the view that task co-representation and resulting response conflict are the only or even primary source of effects observed in task sharing.},
	journal = {Review of Philosophy and Psychology},
	author = {Wenke, Dorit and Atmaca, Silke and Holländer, Antje and Liepelt, Roman and Baess, Pamela and Prinz, Wolfgang},
	year = {2011},
	note = {ISBN: 1878-5158},
}

@article{strabala_towards_2012,
	title = {Towards {Seamless} {Human}-{Robot} {Handovers}},
	volume = {1},
	issn = {2163-0364},
	doi = {10.5898/jhri.v2i1.114},
	abstract = {A handover is a complex collaboration, where actors coordinate in time and space to transfer control of an object. This coordination comprises two processes: the physical process of moving to get close enough to transfer the object, and the cognitive process of exchanging information to guide the transfer. Despite this complexity, we humans are capable of performing handovers seamlessly in a wide variety of situations, even when unexpected. This suggests a common procedure that guides all handover interactions. Our goal is to codify that procedure. To that end, we first study how people hand over objects to each other in order to understand their coordination process and the signals and cues that they use and observe with their partners. Based on these studies, we propose a coordination structure for human--robot handovers that considers the physical and social-cognitive aspects of the interaction separately. This handover structure describes how people approach, reach out their hands, and transfer objects while simultaneously coordinating the what, when, and where of handovers: to agree that the handover will happen (and with what object), to establish the timing of the handover, and to decide the configuration at which the handover will occur. We experimentally evaluate human-robot handover behaviors that exploit this structure and offer design implications for seamless human-robot handover interactions.},
	number = {1},
	journal = {Journal of Human-Robot Interaction (2013)},
	author = {Strabala, Kyle and Lee, Min Kyung and Dragan, Anca and Forlizzi, Jodi and Srinivasa, Siddhartha S and Cakmak, Maya and Garage, Willow and Universi, Vincenzo Micelli and Studi, Degli and Parma, Di and Srinavasa, Siddhartha S. and Cakmak, Maya and Micelli, Vincenzo},
	year = {2012},
	keywords = {Handover, Handovers, Human, Human-Robot Interaction(HRI), Joint Activity, Physical HRI, Physical human-robot interaction, Signaling, handover, joint activity, signaling},
	pages = {112--132},
}

@article{friston_action_2011,
	title = {Action understanding and active inference},
	volume = {104},
	issn = {03401200},
	doi = {10.1007/s00422-011-0424-z},
	abstract = {We have suggested that the mirror-neuron system might be usefully understood as implementing Bayes-optimal perception of actions emitted by oneself or others. To substantiate this claim, we present neuronal simulations that show the same representations can prescribe motor behavior and encode motor intentions during action-observation. These simulations are based on the free-energy formulation of active inference, which is formally related to predictive coding. In this scheme, (generalised) states of the world are represented as trajectories. When these states include motor trajectories they implicitly entail intentions (future motor states). Optimizing the representation of these intentions enables predictive coding in a prospective sense. Crucially, the same generative models used to make predictions can be deployed to predict the actions of self or others by simply changing the bias or precision (i.e. attention) afforded to proprioceptive signals. We illustrate these points using simulations of handwriting to illustrate neuronally plausible generation and recognition of itinerant (wandering) motor trajectories. We then use the same simulations to produce synthetic electrophysiological responses to violations of intentional expectations. Our results affirm that a Bayes-optimal approach provides a principled framework, which accommodates current thinking about the mirror-neuron system. Furthermore, it endorses the general formulation of action as active inference.},
	number = {1-2},
	journal = {Biological Cybernetics},
	author = {Friston, Karl and Mattout, Jérémie J??r??mie and Kilner, James},
	year = {2011},
	pmid = {21327826},
	note = {ISBN: 1432-0770},
	keywords = {Action understanding, Action-observation, Activity Detection, Free-energy, Generative models, Inference, Mirror-neuron system, Perception, Precision, Predictive coding, ★},
	pages = {137--160},
}
